{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/18Ewg5KEFcfVSIWEOs303K1FLoPrVE9RD?usp=sharing)"
      ],
      "metadata": {
        "id": "qIzynv82-wND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tradictional frecuency approach for text classification\n",
        "\n",
        "Traditional approaches come to improve text models that worked only with bag-of-words. TFIDF was developed as an alternative that looks at the frequency of words in the vocabulary allowing us to run a traditional ML algorithm on top of these features to obtain text classification. It works very well when we have texts of different classes with a differentiable vocabulary. However, it does not take into account the order of the words in the text nor their semantic or contextual meaning.\n",
        "\n",
        "<br>\n",
        "\n",
        "For this notebook we are going to use the TFIDF processing of the sklearn library that we can consult in the following link:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
      ],
      "metadata": {
        "id": "Le5NZMJl4TyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from decimal import Decimal, ROUND_HALF_DOWN\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import classification_report, recall_score"
      ],
      "metadata": {
        "id": "plbbIFD8QE0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_round(number, decimal_places=1):\n",
        "    rounded_number = Decimal(number).quantize(Decimal('0.0' + '1' * decimal_places), rounding=ROUND_HALF_DOWN)\n",
        "    return float(rounded_number)\n",
        "\n",
        "def viz_top_terms(tfidf_values, feature_names):\n",
        "  # Calculate the mean TF-IDF scores across all documents\n",
        "  mean_tfidf_scores = np.mean(tfidf_values, axis=0)\n",
        "\n",
        "  # Create a dictionary mapping feature names (terms) to their mean TF-IDF scores\n",
        "  term_tfidf_dict = dict(zip(feature_names, mean_tfidf_scores))\n",
        "\n",
        "  # Sort the terms by their TF-IDF scores in descending order\n",
        "  sorted_terms = sorted(term_tfidf_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  # Select the top N terms to visualize (e.g., top 20)\n",
        "  top_n = 20\n",
        "  top_terms, top_scores = zip(*sorted_terms[:top_n])\n",
        "\n",
        "  # Create a bar chart to visualize the top N terms and their TF-IDF scores\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.barh(top_terms, top_scores, color='skyblue')\n",
        "  plt.xlabel('Mean TF-IDF Score')\n",
        "  plt.title(f'Top {top_n} Terms by TF-IDF')\n",
        "  plt.gca().invert_yaxis()  # Invert y-axis to display highest TF-IDF at the top\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "HjHZvz8hSJ4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_effects(tfidf_matrix, categories):\n",
        "    \"\"\"\n",
        "        https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html\n",
        "    \"\"\"\n",
        "    # learned coefficients weighted by frequency of appearance\n",
        "    average_feature_effects = clf.coef_ * np.asarray(tfidf_matrix.mean(axis=0)).ravel()\n",
        "\n",
        "    for i, label in enumerate(categories):\n",
        "        top5 = np.argsort(average_feature_effects[i])[-5:][::-1]\n",
        "        if i == 0:\n",
        "            top = pd.DataFrame(feature_names[top5], columns=[label])\n",
        "            top_indices = top5\n",
        "        else:\n",
        "            top[label] = feature_names[top5]\n",
        "            top_indices = np.concatenate((top_indices, top5), axis=None)\n",
        "    top_indices = np.unique(top_indices)\n",
        "    predictive_words = feature_names[top_indices]\n",
        "\n",
        "    # plot feature effects\n",
        "    bar_size = 0.25\n",
        "    padding = 0.75\n",
        "    y_locs = np.arange(len(top_indices)) * (4 * bar_size + padding)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    for i, label in enumerate(categories):\n",
        "        ax.barh(\n",
        "            y_locs + (i - 2) * bar_size,\n",
        "            average_feature_effects[i, top_indices],\n",
        "            height=bar_size,\n",
        "            label=label,\n",
        "        )\n",
        "    ax.set(\n",
        "        yticks=y_locs,\n",
        "        yticklabels=predictive_words,\n",
        "        ylim=[\n",
        "            0 - 4 * bar_size,\n",
        "            len(top_indices) * (4 * bar_size + padding) - 4 * bar_size,\n",
        "        ],\n",
        "    )\n",
        "    ax.legend(loc=\"lower right\")\n",
        "\n",
        "    print(\"top 5 keywords per class:\")\n",
        "    print(top)\n",
        "    print()\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "kFRlC1EJdVpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifier model using TDIDF\n",
        "\n",
        "\n",
        "The objective of this section is to apply TFIDF on the 20newsgroups dataset, which consists of news on 20 topics from which we will select the following 5 to reduce the complexity of the exercise:\n",
        "- *talk.politics.guns*\n",
        "- *sci.space*\n",
        "- *sci.med*\n",
        "- *comp.windows.x*\n",
        "- *alt.atheism*\n",
        "\n",
        "The idea is to start from a raw text and a TIDf with the default parameters and explore all the alternatives we have to improve the models with this approach.\n",
        "\n",
        "Doc: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n"
      ],
      "metadata": {
        "id": "ebFmMz92dclU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c05iHL7QL4d_"
      },
      "outputs": [],
      "source": [
        "# Load the 20 Newsgroups dataset (only 5 categroies)\n",
        "categories = ['talk.politics.guns',\n",
        "              'sci.space',\n",
        "              'sci.med',\n",
        "              'comp.windows.x',\n",
        "              'alt.atheism']\n",
        "\n",
        "data_train = fetch_20newsgroups(\n",
        "    subset=\"train\",\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "data_test = fetch_20newsgroups(\n",
        "    subset=\"test\",\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "pd.Series(data_train.target).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the text data (newsgroup posts) and target labels\n",
        "X_train, y_train = data_train.data, data_train.target\n",
        "X_test, y_test = data_test.data, data_test.target\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the text data to calculate TF-IDF values\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Get the feature names (terms) and TF-IDF values as a dense array\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_values = tfidf_matrix.toarray()"
      ],
      "metadata": {
        "id": "oVRjl7euQGDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clf = RandomForestClassifier(max_depth=5, random_state=0)\n",
        "# clf.fit(tfidf_values, y_train)\n",
        "\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
        "clf.fit(tfidf_matrix, y_train)"
      ],
      "metadata": {
        "id": "z-DPquqYSyOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_test = clf.predict(tfidf_vectorizer.transform(X_test))\n",
        "\n",
        "print('Evaluate test')\n",
        "print(classification_report(y_test, y_hat_test, target_names=categories))"
      ],
      "metadata": {
        "id": "OgFbsbmkTGiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plot_feature_effects(tfidf_matrix, categories).set_title(\"Average feature effect on the original data\")"
      ],
      "metadata": {
        "id": "kBtq6efSbI0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improve TFIDF with text preprocessing"
      ],
      "metadata": {
        "id": "PL_lNM5DWLOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Stopwords\n",
        "\n",
        "Set of commonly used words in any language. We have several libraries where we can load predefined sets for various languages or apply frecuency analysis in order to detect them."
      ],
      "metadata": {
        "id": "aWoKhSqUWWWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can check that are several words that are not contribute to our model\n",
        "viz_top_terms(tfidf_values, feature_names)"
      ],
      "metadata": {
        "id": "ObrsHgTcOBdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### List of predifined stopwords"
      ],
      "metadata": {
        "id": "di0tieyX4ENV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loca english stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Pyo1TB3JdGsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_values = tfidf_matrix.toarray()\n",
        "\n",
        "# Model\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
        "clf.fit(tfidf_matrix, y_train)\n",
        "\n",
        "# Results\n",
        "y_hat_test = clf.predict(tfidf_vectorizer.transform(X_test))\n",
        "print('Evaluate test')\n",
        "print(classification_report(y_test, y_hat_test, target_names=categories))"
      ],
      "metadata": {
        "id": "vLF6_xYNQXzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detect stopwords with term frecuency\n",
        "\n",
        "Sometimes the vocabulary can be very large and we want to reduce it to have the necessary features without adding noise. We can do this in two ways:\n",
        "\n",
        "- Removing the less frequent words `min_df`: if a word only appears 1 or 2 times in our dataset for our frequency analysis probably they will be a misspelled word or irrelevant term.\n",
        "- Removing the most used words `max_df`: If there are certain words that are repeated in almost all documents, they are probably stopwords in our dataset that can be eliminated."
      ],
      "metadata": {
        "id": "I2PT0gTH2Soo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_values = tfidf_matrix.toarray()\n",
        "\n",
        "# Model\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
        "clf.fit(tfidf_matrix, y_train)\n",
        "\n",
        "# Results\n",
        "y_hat_test = clf.predict(tfidf_vectorizer.transform(X_test))\n",
        "print('Evaluate test')\n",
        "print(classification_report(y_test, y_hat_test, target_names=categories))"
      ],
      "metadata": {
        "id": "w6A5vcIC2Rzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove accents and punctuation\n",
        "\n",
        "To get a better processing and reach a standardized text, another common step is to delete accents and punctuation symbols."
      ],
      "metadata": {
        "id": "N3oLMCRshd0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. remove punctuation\n",
        "import string\n",
        "\n",
        "def preprocess_and_remove_punctuation(text):\n",
        "    # Remove punctuation using Python's string.punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    return text\n",
        "\n",
        "# Preprocess train and test\n",
        "X_train_norm = [preprocess_and_remove_punctuation(doc) for doc in X_train]\n",
        "X_test_norm = [preprocess_and_remove_punctuation(doc) for doc in X_test]"
      ],
      "metadata": {
        "id": "HWQd8_rtj8N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. remove accents\n",
        "\n",
        "# TFIDF\n",
        "tfidf_vectorizer = TfidfVectorizer(strip_accents='unicode')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train_norm)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_values = tfidf_matrix.toarray()\n",
        "\n",
        "# Model\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
        "clf.fit(tfidf_matrix, y_train)\n",
        "\n",
        "# Results\n",
        "y_hat_test = clf.predict(tfidf_vectorizer.transform(X_test_norm))\n",
        "print('Evaluate test')\n",
        "print(classification_report(y_test, y_hat_test, target_names=categories))"
      ],
      "metadata": {
        "id": "a8lLJOmbhdMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams\n",
        "\n",
        "An n-gram is a collection of n successive items in a text document. Can be useful when some word are complemented to each other. E.g \"*transport line*\", \"*super star*\", \"*power supply*\".\n",
        "\n"
      ],
      "metadata": {
        "id": "eBj-OGoAdtJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))      # we include n-grams from 1 to 2 words (could slow donw our process)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_values = tfidf_matrix.toarray()\n",
        "\n",
        "# Model\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
        "clf.fit(tfidf_matrix, y_train)\n",
        "\n",
        "# Results\n",
        "y_hat_test = clf.predict(tfidf_vectorizer.transform(X_test))\n",
        "print('Evaluate test')\n",
        "print(classification_report(y_test, y_hat_test, target_names=categories))"
      ],
      "metadata": {
        "id": "r3iHAdFHWb32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. It usually relies on dictionaries to map words to their standardized version."
      ],
      "metadata": {
        "id": "7vvGuSO6fc0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "YmXOP-j2WcUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_and_stem(text):\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    # Stem the words\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "\n",
        "# Preprocess and stem the documents\n",
        "X_train_st = [preprocess_and_stem(doc) for doc in X_train]\n",
        "X_test_st = [preprocess_and_stem(doc) for doc in X_test]"
      ],
      "metadata": {
        "id": "yiqXKvwYgMn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train_st)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_values = tfidf_matrix.toarray()\n",
        "\n",
        "# Model\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
        "clf.fit(tfidf_matrix, y_train)\n",
        "\n",
        "# Results\n",
        "y_hat_test = clf.predict(tfidf_vectorizer.transform(X_test_st))\n",
        "print('Evaluate test')\n",
        "print(classification_report(y_test, y_hat_test, target_names=categories))"
      ],
      "metadata": {
        "id": "ztCCqUoQgx1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "Takes a word and breaks it down to its lemma. It usually relies on models pretrained in different languages."
      ],
      "metadata": {
        "id": "eKC-Y6Vffdoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "sB5fm27EWcbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def preprocess_and_lemmatize(text):\n",
        "    # Parse the text using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Lemmatize and remove punctuation\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    # Join the lemmatized tokens back into a string\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Preprocess and lemmatize the documents\n",
        "X_train_lm = [preprocess_and_lemmatize(doc) for doc in X_train]\n",
        "X_test_lm = [preprocess_and_lemmatize(doc) for doc in X_test]"
      ],
      "metadata": {
        "id": "V4B1wZ4qhTKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train_lm)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_values = tfidf_matrix.toarray()\n",
        "\n",
        "# Model\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
        "clf.fit(tfidf_matrix, y_train)\n",
        "\n",
        "# Results\n",
        "y_hat_test = clf.predict(tfidf_vectorizer.transform(X_test_lm))\n",
        "print('Evaluate test')\n",
        "print(classification_report(y_test, y_hat_test, target_names=categories))"
      ],
      "metadata": {
        "id": "ga6NLrBsikvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS-tagging\n",
        "\n",
        "Part-of-speech (POS) tagging is a process in natural language processing (NLP) where each word in a text is labeled with its corresponding part of speech. This can include nouns, verbs, adjectives, and other grammatical categories. It usually relies on models pretrained in different languages.\n",
        "\n",
        "<br>\n",
        "\n",
        "https://spacy.io/usage/linguistic-features\n"
      ],
      "metadata": {
        "id": "7ks8m6D5jKVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def preprocess_and_extract_nouns(text):\n",
        "    # Parse the text using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Extract nouns (NOUNs) and convert them to lowercase\n",
        "    nouns = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
        "    # Join the nouns back into a string\n",
        "    return ' '.join(nouns)\n",
        "\n",
        "\n",
        "# Preprocess and lemmatize the documents\n",
        "X_train_pos = [preprocess_and_extract_nouns(doc) for doc in X_train]\n",
        "X_test_pos = [preprocess_and_extract_nouns(doc) for doc in X_test]"
      ],
      "metadata": {
        "id": "PBGv6nAwjI45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train_pos)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_values = tfidf_matrix.toarray()\n",
        "\n",
        "# Model\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
        "clf.fit(tfidf_matrix, y_train)\n",
        "\n",
        "# Results\n",
        "y_hat_test = clf.predict(tfidf_vectorizer.transform(X_test_pos))\n",
        "print('Evaluate test')\n",
        "print(classification_report(y_test, y_hat_test, target_names=categories))"
      ],
      "metadata": {
        "id": "prLfvbqajlMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise span dataset\n",
        "\n",
        "Let's apply the spam detection dataset to sms texts.\n",
        "\n",
        "https://archive.ics.uci.edu/dataset/228/sms+spam+collection\n",
        "\n",
        "\n",
        "\n",
        "We must perform the following steps:\n",
        "- Apply at least 2 of the above word processing techniques. It is important to apply them in the correct order and give reasons why.\n",
        "- Generate TFIDF\n",
        "- Use a custom classfier without changing hyperparams (we want to improve our model based on our data preprocessing)\n",
        "- Obtain a `test SPAM recall >= 0.9`\n",
        "\n",
        "<br>\n",
        "\n",
        "**IMPORTANT**: if you want to reuse preprocess functions, you can do it but now we are dealing with pandas so we have to call them a litle bit different:\n",
        "```\n",
        "X_train = X_train.apply(preprocess_and_stem)\n",
        "```"
      ],
      "metadata": {
        "id": "DYUS60-mQZC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the URL of the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "\n",
        "# Define the directory where you want to save and extract the dataset\n",
        "download_dir = \"./sms_spam_collection\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(download_dir):\n",
        "    os.makedirs(download_dir)\n",
        "\n",
        "# Download the dataset file\n",
        "print(\"Downloading SMS Spam Collection dataset...\")\n",
        "urllib.request.urlretrieve(url, os.path.join(download_dir, \"sms_spam_collection.zip\"))\n",
        "\n",
        "# Extract the dataset from the ZIP file\n",
        "print(\"Extracting dataset...\")\n",
        "with zipfile.ZipFile(os.path.join(download_dir, \"sms_spam_collection.zip\"), \"r\") as zip_ref:\n",
        "    zip_ref.extractall(download_dir)\n",
        "\n",
        "# Read the dataset into a pandas DataFrame\n",
        "sms_df = pd.read_csv(os.path.join(download_dir, \"SMSSpamCollection\"), sep='\\t', names=[\"label\", \"message\"])\n",
        "\n",
        "print(sms_df.label.value_counts())\n",
        "sms_df.head()"
      ],
      "metadata": {
        "id": "R0-US8BYQYov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and labels (y)\n",
        "X = sms_df['message']\n",
        "y = sms_df['label']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test) with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42, stratify=y)\n",
        "\n",
        "# Display the shapes of the resulting datasets\n",
        "print(\"Training set shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
        "print(\"Testing set shape (X_test, y_test):\", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "Zs-NYCU6rDz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Preprocessing (your code here)\n",
        "...\n",
        "\n",
        "# Initialize the TF-IDF vectorizer (your code here)\n",
        "tfidf_vectorizer = TfidfVectorizer(...)"
      ],
      "metadata": {
        "id": "P8vZ1lRHrHnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_values = tfidf_matrix.toarray()\n",
        "viz_top_terms(tfidf_values, feature_names)"
      ],
      "metadata": {
        "id": "JuEqsVOIrSlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\", random_state=0)\n",
        "clf.fit(tfidf_matrix, y_train)\n",
        "\n",
        "y_hat_test = clf.predict(tfidf_vectorizer.transform(X_test))\n",
        "print('Evaluate test')\n",
        "print(classification_report(y_test, y_hat_test))\n",
        "\n",
        "assert custom_round(recall_score(y_test, y_hat_test, average='binary', pos_label='spam'), 1) >= 0.9"
      ],
      "metadata": {
        "id": "NoYBKPcfrV_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}