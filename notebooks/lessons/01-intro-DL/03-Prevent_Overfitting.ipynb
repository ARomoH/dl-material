{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prevent Overfitting: Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), consisting of a collection of 28x28 pixel images corresponding in digits from 0 to 9 manuscripts. The purpose of this data set is to train models that recognize handwritten numbers.\n",
    "\n",
    "We will train, therefore, an [OCR (Optical Character Recognition)](https://en.wikipedia.org/wiki/Optical_character_recognition) model for multiclass classification (numbers from 0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As they are 8-bit images, the colors (in this case only one channel, the gray channel) of each pixel are encoded with a value between 0 and 255, with 0 being black and 255 being white. It is usual to normalize the values to work with a range between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize a random image of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "i = np.random.randint(0, len(x_train))\n",
    "plt.imshow(x_train[i], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the dataset for  inducing more overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split into train test sets\n",
    "_, x, _, y = train_test_split(\n",
    "    x_train, y_train, test_size=0.02, random_state=1, stratify=y_train)\n",
    "\n",
    "num_classes = 10\n",
    "from collections import Counter \n",
    "x.shape, y.shape, Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need to use regularization?\n",
    "\n",
    "We are going to assign the classes randomly to each image with random shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shuffle = np.copy(y)\n",
    "np.random.shuffle(y_shuffle)\n",
    "y[:5], y_shuffle[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, 9):\n",
    "    ind = np.random.randint(len(y_shuffle))\n",
    "    axes[i].imshow(x[ind].reshape(28, 28))\n",
    "    axes[i].set_title(\n",
    "        f\"Digit = {y_shuffle[ind]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a model for learning the new random classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "# Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = layers.Dense(64, activation='relu')(flat)\n",
    "# hidden layer 2\n",
    "l_2 = layers.Dense(64, activation='relu')(l_1)\n",
    "# hidden layer 3\n",
    "l_3 = layers.Dense(64, activation='relu')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = layers.Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    x,\n",
    "    y_shuffle,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x, y_shuffle, verbose=0)\n",
    "print('Train Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, 9):\n",
    "    ind = np.random.randint(len(y_shuffle))\n",
    "    pred_pobs = model.predict(np.expand_dims(x[ind], 0)).flatten()\n",
    "    pred_class = np.argmax(pred_pobs)\n",
    "    prob = np.max(pred_pobs)\n",
    "    axes[i].imshow(x[ind].reshape(28, 28))\n",
    "    axes[i].set_title(\n",
    "        f\"Digit = {y_shuffle[ind]} \\n Prediction={pred_class} , Prob={prob:.4f}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has fully fitted the training set. It practically does not matter what data you introduce into the neural network, it will almost always fit. The objective is not to fit the training set, it is to **GENERALIZE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization for preventing overfitting\n",
    "\n",
    "To get the model to **generalize**, we can use one of the numerous regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a model consisting of:\n",
    "\n",
    "- Input with dimension (28,28)\n",
    "- Flatten layer\n",
    "- Dense hidden  layer with 256 neurons and ReLU as activation function\n",
    "- Second dense hidden layer with 128 neurons and ReLU as activation function\n",
    "- Third dense  hidden layer with 128 neurons and ReLU as activation function\n",
    "- Output layer with a single neuron that implements the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = tf.keras.Input(shape=(28,28), name='input_layer')  \n",
    "# Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = layers.Dense(256, activation='relu', name='layer_1')(flat)  \n",
    "# hidden layer 2\n",
    "l_2 = layers.Dense(128, activation='relu', name='layer_2')(l_1)  \n",
    "# hidden layer 3\n",
    "l_3 = layers.Dense(128, activation='relu', name='layer_3')(l_2)  \n",
    "\n",
    "# Outputs\n",
    "outputs = layers.Dense(num_classes, activation='softmax', name='output_layer')(l_3)  \n",
    "\n",
    "# Model definition\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='dont_overfit_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following configuration for training:\n",
    "- **optimizer**: adam\n",
    "- **loss function**: binary crossentropy\n",
    "- **metrics**: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the evolution of the values in each epoch of the loss function and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overfitting_results'></a>\n",
    "When interpreting both graphs, it is evident that the model over-fits the training values and is not able to generalize well for the validation values. **It is a clear example of overfitting**.\n",
    "\n",
    "The following function will be useful throughout the different practical examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_loss_accuracy_evolution(history):\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Categorical Crossentropy')\n",
    "    ax1.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    ax1.plot(hist['epoch'], hist['val_loss'], label = 'Val Error')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n",
    "    ax2.grid()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def show_samples(x_test, y_test, model):\n",
    "    predictions = model.predict(x_test)\n",
    "    predicted_classes = np.argmax(predictions, -1)\n",
    "    \n",
    "    L = 5\n",
    "    W = 5\n",
    "    fig, axes = plt.subplots(L, W, figsize=(14, 14))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in np.arange(0, L * W):\n",
    "        ind = np.random.randint(len(y_test))\n",
    "        axes[i].imshow(x_test[ind].reshape(28, 28))\n",
    "        prob_pred = np.max(predictions[ind, :])\n",
    "        class_pred = int(predicted_classes[ind])\n",
    "        original_class = int(y_test[ind])\n",
    "        if class_pred == original_class:\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        title = \"Pred: {0} \\n Target: {1} \\n Prob: {2:.3f}\".format(\n",
    "        class_pred, original_class, prob_pred)\n",
    "        axes[i].set_title(title, color=color)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simpler model\n",
    "\n",
    "The simplest way to prevent overfitting is to create  a simple model: A model with less number of  parameters (which is determined by the number of layers and the number of units per layer). \n",
    "\n",
    "A more complex model with more parameters will have much more capacity and will be able to learn almost any training set. Deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = tf.keras.Input(shape=(28, 28), name='input_layer')\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "# First hidden layer\n",
    "l_1 = layers.Dense(8, activation='relu', name='layer_1')(flat)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(num_classes, activation='softmax',\n",
    "                       name='output_layer')(l_1)\n",
    "model_simpler = keras.Model(\n",
    "    inputs=inputs, outputs=outputs, name='simpler_model')\n",
    "\n",
    "model_simpler.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simpler.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "history_simpler = model_simpler.fit(x,\n",
    "                                    y,\n",
    "                                    batch_size=64,\n",
    "                                    epochs=100,\n",
    "                                    validation_split=0.25,\n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_simpler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_simpler.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce batch_size\n",
    "\n",
    "Other method is decreasing the `batch_size`during the gradient descend to add more uncertainty to the estimation of the parameter gradients, thus reducing the capacity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = tf.keras.Input(shape=(28,28), name='input_layer')  \n",
    "# Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = layers.Dense(256, activation='relu', name='layer_1')(flat)  \n",
    "# hidden layer 2\n",
    "l_2 = layers.Dense(128, activation='relu', name='layer_2')(l_1)  \n",
    "# hidden layer 3\n",
    "l_3 = layers.Dense(128, activation='relu', name='layer_3')(l_2)  \n",
    "\n",
    "# Outputs\n",
    "outputs = layers.Dense(num_classes, activation='softmax', name='output_layer')(l_3)  \n",
    "\n",
    "# Model definition\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='dont_overfit_model')\n",
    "##\u00a0same model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history_2 = model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=16,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "####\u00a0What differences do you see if you change the batch_size to a lower or higher value? For example to 1 and to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout'></a>\n",
    "## Dropout\n",
    "\n",
    "Dropout, applied to a layer, consists of randomly \"dropping out\" (set to zero) a number of output features of the layer during training with a probability **p** (`dropout rate`).\n",
    "\n",
    "In `keras` you can introduce dropout in a network via the [Dropout layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), which gets applied to the output of layer right before.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Dropout(\n",
    "    rate, noise_shape=None, seed=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "next_layer = layers.Dropout(0.4)(prev_layer)\n",
    "```\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(layers.Dropout(0.4))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = tf.keras.Input(shape=(28, 28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "flat = layers.Dropout(0.5, name='dropout_flat')(flat)\n",
    "# hidden layer 1\n",
    "l_1 = layers.Dense(256, activation='relu', name='layer_1')(flat)\n",
    "l_1 = layers.Dropout(0.5, name='dropout_l1')(l_1)\n",
    "# hidden layer 2\n",
    "l_2 = layers.Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "l_2 = layers.Dropout(0.5, name='dropout_l2')(l_2)\n",
    "# hidden layer 3\n",
    "l_3 = layers.Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "l_3 = layers.Dropout(0.5, name='dropout_l3')(l_3)\n",
    "\n",
    "# Outputs\n",
    "outputs = layers.Dense(num_classes, activation='softmax',\n",
    "                       name='output_layer')(l_3)\n",
    "\n",
    "\n",
    "model_dropout = keras.Model(\n",
    "    inputs=inputs, outputs=outputs, name='dont_overfit_model_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dropout = model_dropout.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_dropout.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "#### What happens if you change the dropout to 0.1 or 0.9? And what if you put it only on one layer?\n",
    "#### What happens if we delete the flatten first dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "[Batch normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n",
    "\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "next_layer = layers.BatchNormalization()(prev_layer)\n",
    "```\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(layers.BatchNormalization())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = tf.keras.Input(shape=(28, 28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = layers.Dense(256, activation='relu', name='layer_1')(flat)\n",
    "l_1 = layers.BatchNormalization()(l_1)\n",
    "# hidden layer 2\n",
    "l_2 = layers.Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "l_2 = layers.BatchNormalization()(l_2)\n",
    "# hidden layer 3\n",
    "l_3 = layers.Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = layers.Dense(num_classes, activation='softmax',\n",
    "                       name='output_layer')(l_3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_batch_norm = keras.Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_batch_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_batch_norm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch_norm.compile(\n",
    "    optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_batch_norm = model_batch_norm.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_batch_norm.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='l1_l2_elasticnet'></a>\n",
    "## L1, L2 and ElasticNet\n",
    "\n",
    "Recordemos que durante el entrenamiento de la red se aprende la configuraci\u00f3n de pesos y biases que permita mejorar los resultados para una determinada funci\u00f3n de p\u00e9rdida.\n",
    "\n",
    "* [L1 regularization](https://developers.google.com/machine-learning/glossary/#L1_regularization), where the cost added is proportional to the absolute value of the weights coefficients (\"L1 norm\" of the weights).\n",
    "\n",
    "```python\n",
    "tf.keras.regularizers.l1(l1=0.01)\n",
    "\n",
    "layers.Dense(3, kernel_regularizer='l1')\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "* [L2 regularization](https://developers.google.com/machine-learning/glossary/#L2_regularization), where the cost added is proportional to the square of the value of the weights coefficients ( \"L2 norm\" of the weights). \n",
    "\n",
    "```python\n",
    "tf.keras.regularizers.l2(l2=0.01)\n",
    "\n",
    "layers.Dense(3, kernel_regularizer='l2')\n",
    "\n",
    "```\n",
    "* L1_L2 or Elastic Net.\n",
    "\n",
    "```python\n",
    "tf.keras.regularizers.l1_l2(\n",
    "    l1=0.01, l2=0.01\n",
    ")\n",
    "\n",
    "layers.Dense(3, kernel_regularizer='l1_l2')\n",
    "\n",
    "```\n",
    "\n",
    "Furthermore, it is possible to choose whether to include the penalty in the cost function on the weights, the biases or on the activation, with the following arguments:\n",
    "- `kernel_regularizer`: only on weights.\n",
    "- `bias_regularizer`: only on biases.\n",
    "- `activity_regularizer`: on full output.\n",
    "\n",
    "[link to documentation](https://keras.io/api/layers/regularizers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerner_regularizer_l1 = regularizers.l1_l2(l1=1e-5, l2=5e-4)\n",
    "kerner_regularizer_l2 = regularizers.l2(5e-4)\n",
    "kerner_regularizer_l3 = regularizers.l1(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input\n",
    "inputs = tf.keras.Input(shape=(28, 28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = layers.Dense(256, activation='relu',\n",
    "                   kernel_regularizer=kerner_regularizer_l1, name='layer_1')(flat)\n",
    "# hidden layer 2\n",
    "l_2 = layers.Dense(128, activation='relu',\n",
    "                   kernel_regularizer=kerner_regularizer_l2, name='layer_2')(l_1)\n",
    "# hidden layer 3\n",
    "l_3 = layers.Dense(128, activation='relu',\n",
    "                   kernel_regularizer=kerner_regularizer_l3, name='layer_3')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = layers.Dense(num_classes, activation='softmax',\n",
    "                       name='output_layer')(l_3)\n",
    "\n",
    "\n",
    "model_regularizers = keras.Model(inputs=inputs,\n",
    "                                 outputs=outputs,\n",
    "                                 name='dont_overfit_model_regularizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regularizers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regularizers.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_regularizers = model_regularizers.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the weights norms of the model without regularization and with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ly in model.layers:\n",
    "    if 'layer_' in ly.name:\n",
    "        W, b = ly.get_weights()\n",
    "        l2_norm = np.sum(W**2)**0.5\n",
    "        zero_elems = len(W[np.abs(W) < 1e-14])\n",
    "        sparsity = zero_elems / np.prod(W.shape)\n",
    "        sparsity = np.round(100 * sparsity, 2)\n",
    "        print('L2 norm {0} weights: {1}, shape:{2}, sparsity:{3}'.format(\n",
    "            ly.name, l2_norm, W.shape,sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ly in model_regularizers.layers:\n",
    "    if 'layer_' in ly.name:\n",
    "        W, b = ly.get_weights()\n",
    "        l2_norm = np.sum(W**2)**0.5\n",
    "        zero_elems = len(W[np.abs(W) < 1e-14])\n",
    "        sparsity = zero_elems / np.prod(W.shape)\n",
    "        sparsity = np.round(100 * sparsity, 2)\n",
    "        print('L2 norm {0} weights: {1}, shape:{2}, sparsity:{3}'.format(\n",
    "            ly.name, l2_norm, W.shape,sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ly in model_batch_norm.layers:\n",
    "    if 'layer_' in ly.name:\n",
    "        W, b = ly.get_weights()\n",
    "        l2_norm = np.sum(W**2)**0.5\n",
    "        zero_elems = len(W[np.abs(W) < 1e-14])\n",
    "        sparsity = zero_elems / np.prod(W.shape)\n",
    "        sparsity = np.round(100 * sparsity, 2)\n",
    "        print('L2 norm {0} weights: {1}, shape:{2}, sparsity:{3}'.format(\n",
    "            ly.name, l2_norm, W.shape,sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_regularizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_regularizers.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "#### What happens if you change the regularizers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine L2 Regularization and Dropout\n",
    "\n",
    "Use L2: \n",
    "```python\n",
    "kerner_regularizer_l2 = regularizers.l2(5e-4)\n",
    "```\n",
    "And Dropout:\n",
    "\n",
    "```python\n",
    "next_layer = layers.Dropout(0.4)(prev_layer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerner_regularizer_l2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(..., ), name='input_layer')  \n",
    "\n",
    "# Convert the 2D image to a vector\n",
    "flat = layers....()(inputs)\n",
    "\n",
    "\n",
    "# Add L2-normalization\n",
    "l_1 = layers.Dense(128, activation='relu',\n",
    "                   kernel_regularizer=...,\n",
    "                   name='layer_1')(flat)\n",
    "\n",
    "# Add dropout\n",
    "l_1 = ...(l_1)\n",
    "\n",
    "\n",
    "# Add L2-normalization\n",
    "l_2 = layers.Dense(64,\n",
    "                   activation='relu',\n",
    "                   kernel_regularizer=...,\n",
    "                   name='layer_2')(l_1)\n",
    "# Add dropout\n",
    "l_2 = ...(l_2)\n",
    "\n",
    "\n",
    "# Add L2-normalization\n",
    "l_3 = layers.Dense(32,\n",
    "                   activation='relu',\n",
    "                   kernel_regularizer=...,\n",
    "                   name='layer_3')(l_2)\n",
    "\n",
    "# Add dropout\n",
    "l_3 = ...(l_3)\n",
    "\n",
    "\n",
    "outputs = layers.Dense(..., activation=...,\n",
    "                       name='output_layer')(l_3)  \n",
    "\n",
    "model_combination = keras.Model(inputs=inputs,\n",
    "                                 outputs=outputs,\n",
    "                                 name='dont_overfit_model_regularizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_combination.compile(\n",
    "    optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_combination = model_combination.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_combination.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise injection \n",
    "\n",
    "Apply additive zero-mean Gaussian noise.\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "next_layer = layers.GaussianNoise(stddev)(prev_layer)\n",
    "```\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(layers.GaussianNoise(stddev)())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = 2\n",
    "\n",
    "# Input\n",
    "inputs = tf.keras.Input(shape=(28,28), name='input_layer')  \n",
    "# Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = layers.Dense(256, activation='relu', name='layer_1')(flat)  \n",
    "l_1 = layers.GaussianNoise(stddev, name='noise_l1')(l_1) \n",
    "# hidden layer 2\n",
    "l_2 = layers.Dense(128, activation='relu', name='layer_2')(l_1) \n",
    "l_2 = layers.GaussianNoise(stddev, name='noise_l2')(l_2)  \n",
    "# hidden layer 3\n",
    "l_3 = layers.Dense(128, activation='relu', name='layer_3')(l_2)  \n",
    "l_3 = layers.GaussianNoise(stddev, name='noise_l3')(l_3)  \n",
    "\n",
    "# Outputs\n",
    "outputs = layers.Dense(num_classes, activation='softmax', name='output_layer')(l_3)  \n",
    "\n",
    "model_noise = keras.Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_noise.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_noise.compile(\n",
    "    optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_noise = model_noise.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_noise.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='early_stopping'></a>\n",
    "## Early Stopping\n",
    "\n",
    "Another of the most used techniques in neural network training is **early stopping**. It basically consists of stopping the training process in an Epoch prior to the one defined in the compilation.\n",
    "\n",
    "This, in addition to resulting in a model with less overfitting, saves unnecessary computation time.\n",
    "\n",
    "To do this, we can use the [Keras callbacks](https://keras.io/api/callbacks/), objects that perform actions at different times during training. Specifically, we will make use of [`EarlyStopping`](https://keras.io/api/callbacks/early_stopping/).\n",
    "\n",
    "```python\n",
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    ")\n",
    "``` \n",
    "\n",
    "- **monitor**: Quantity to be monitored. Depending on the evolution, it will be decided to stop training.\n",
    "\n",
    "- **min_delta**: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "\n",
    "- **patience**: Number of epochs with no improvement after which training will be stopped.\n",
    "\n",
    "- **verbose**: verbosity mode.\n",
    "\n",
    "There are other callbacks that can be very useful such as:\n",
    "- [`ModelCheckpoint`](https://keras.io/api/callbacks/model_checkpoint/), which allows storing the state of a model at different times of training. This is very useful in trainings that can take hours or days.\n",
    "- [`TensorBoard`](https://keras.io/api/callbacks/tensorboard/), which allows the use of [TensorBoard](https://www.tensorflow.org/tensorboard?hl=es-419), framework TensorFlow for visualizing metrics and evaluating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy'\n",
    "    patience=5,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the same model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = tf.keras.Input(shape=(28,28), name='input_layer')  \n",
    "# Convert the 2D image to a vector\n",
    "flat = layers.Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = layers.Dense(256, activation='relu', name='layer_1')(flat)  \n",
    "# hidden layer 2\n",
    "l_2 = layers.Dense(128, activation='relu', name='layer_2')(l_1)  \n",
    "# hidden layer 3\n",
    "l_3 = layers.Dense(128, activation='relu', name='layer_3')(l_2)  \n",
    "\n",
    "# Outputs\n",
    "outputs = layers.Dense(num_classes, activation='softmax', name='output_layer')(l_3)  \n",
    "\n",
    "model_early_stopping = keras.Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_early_stopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_early_stopping.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_early_stopping.compile(optimizer='adam',\n",
    "                             loss='sparse_categorical_crossentropy',\n",
    "                             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_early_stopping = model_early_stopping.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_early_stopping.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tensorboard'></a>\n",
    "# TensorBoard\n",
    "\n",
    "\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard) is a set of TensorFlow tools that allows us to represent the evolution and results of the training.\n",
    "\n",
    "In order to introduce this tool, the following is a notebook from the [TensorFlow tutorial on TensorBoard](https://www.tensorflow.org/tensorboard/get_started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load TensorBoard notebook extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We build the model. The input layer will have a dimension of `28x28`. The output layer, in a multiclass classification problem with 10 possible classes, will be made up of 10 neurons with [softmax activation function](https://keras.io/api/layers/activations/#softmax-function ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_regularizer_l2 = keras.regularizers.l2(5e-4)\n",
    "\n",
    "model = tf.keras.models.Sequential(name=\"tensorboard_model\")\n",
    "model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Hidden Layer + regularization\n",
    "model.add(layers.Dense(64, activation='relu', name='layer_1'))\n",
    "model.add(layers.Dense(64, activation='relu',\n",
    "                       kernel_regularizer=kernel_regularizer_l2, name='layer_2'))\n",
    "# output layer\n",
    "model.add(layers.Dense(10, activation='softmax', name='output_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [example with early stopping](#early_stopping) the Keras callbacks were presented. To use TensorBoard we will need to include the [`TensorBoard` callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) to the training.\n",
    "\n",
    "This callback will create and store the logs in the directory that we indicate.\n",
    "\n",
    "By default it will create a directory called \"logs\" in the directory where this notebook is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy'\n",
    "    patience=10,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=x_train, \n",
    "    y=y_train, \n",
    "    epochs=20, \n",
    "    validation_split=0.2, \n",
    "    batch_size=32,\n",
    "    callbacks=[tensorboard_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ** start TensorBoard ** we can execute the following cell or, from the command line, execute:\n",
    "\n",
    "    tensorboard --logdir logs / fit\n",
    "\n",
    "An application will be launched that will be listening by default on port 6006. It is possible to use TensorBoard by opening [localhost: 6006 /](http://localhost:6006/) in a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief overview of the dashboards shown (tabs in top navigation bar):\n",
    "\n",
    "* The **Scalars** dashboard shows how the loss and metrics change with every epoch. You can use it to also track training speed, learning rate, and other scalar values.\n",
    "* The **Graphs** dashboard helps you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. \n",
    "* The **Distributions** and **Histograms** dashboards show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the last model with some regularization techniques to obtain at least `0.98` of test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "## Hidden Layer + regularization\n",
    "model.add(...)\n",
    "### output layer\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=x_train, \n",
    "    y=y_train, \n",
    "    epochs=5, \n",
    "    validation_split=0.2, \n",
    "    batch_size=64\n",
    ")\n",
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=x_train, \n",
    "    y=y_train, \n",
    "    epochs=..., \n",
    "    validation_split=0.2, \n",
    "    batch_size=...,\n",
    "    callbacks=...\n",
    ")\n",
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predicted_classes = np.argmax(predictions, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 5\n",
    "W = 5\n",
    "fig, axes = plt.subplots(L, W, figsize=(14, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, L * W):\n",
    "    ind = np.random.randint(len(y_test))\n",
    "    axes[i].imshow(x_test[ind].reshape(28, 28))\n",
    "    prob_pred = np.max(predictions[ind, :])\n",
    "    class_pred = int(predicted_classes[ind])\n",
    "    original_class = int(y_test[ind])\n",
    "    if class_pred == original_class:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    title = \"Pred: {0} \\n Target: {1} \\n Prob: {2:.3f}\".format(\n",
    "    class_pred, original_class, prob_pred)\n",
    "    axes[i].set_title(title, color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQHcaG++PPLYUXDcKliAVL",
   "name": "Prevent_Overfitting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}