{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/RNN/Introduction_to_RNN_Time_Series.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/RNN/Introduction_to_RNN_Time_Series.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6873211b02d4"
   },
   "source": [
    "#### Import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T16:35:22.863599Z",
     "iopub.status.busy": "2020-10-02T16:35:22.863030Z",
     "iopub.status.idle": "2020-10-02T16:35:28.261974Z",
     "shell.execute_reply": "2020-10-02T16:35:28.261435Z"
    },
    "id": "71c626bbac35"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4041a2e9b310"
   },
   "source": [
    "# Simple Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98e0c38cf95d"
   },
   "source": [
    "There are three built-in RNN layers in Keras:\n",
    "\n",
    "1. [`keras.layers.SimpleRNN`](https://keras.io/api/layers/recurrent_layers/simple_rnn/), a fully-connected RNN where the output from previous\n",
    "timestep is to be fed to next timestep.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.SimpleRNN(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    ")\n",
    "````\n",
    "\n",
    "2. [`keras.layers.GRU`](https://keras.io/api/layers/recurrent_layers/gru/), first proposed in\n",
    "[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n",
    "```python\n",
    "tf.keras.layers.GRU(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    ")\n",
    "```\n",
    "\n",
    "3. [`keras.layers.LSTM`](https://keras.io/api/layers/recurrent_layers/lstm/), first proposed in\n",
    "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "```python\n",
    "tf.keras.layers.LSTM(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    ")\n",
    "````\n",
    "For more information, see the\n",
    "[RNN API documentation](https://keras.io/api/layers/recurrent_layers/).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In time series forecasting we are going to use the **many-to-one** architecture with default parameter `return_sequences=False`.\n",
    "\n",
    "The shape of the output  for this architecture  is `(batch_size, units)`.\n",
    "where `units` corresponds to the `units` argument passed to the layer's constructor.\n",
    "\n",
    "Lets see one some examples for understanding the input/output dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T16:35:28.271135Z",
     "iopub.status.busy": "2020-10-02T16:35:28.270504Z",
     "iopub.status.idle": "2020-10-02T16:35:30.101802Z",
     "shell.execute_reply": "2020-10-02T16:35:30.102221Z"
    },
    "id": "a5617759e54e"
   },
   "outputs": [],
   "source": [
    "# dims of input: [batch, timesteps, features]\n",
    "inputs = tf.random.normal([32, 10, 4])\n",
    "print('input dim (batch, timesteps, feature): ', inputs.shape)\n",
    "# return_sequences=False, return_state=False\n",
    "lstm = tf.keras.layers.LSTM(units= 2)\n",
    "output = lstm(inputs)\n",
    "print('return_state=False output shape: ',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNN\n",
    "We can stack multiple layers of RNNs on top of each other. Each hidden state is continuously passed to both the next time step of the current layer and the current time step of the next layer.\n",
    "\n",
    "For stack another RNN layer to an existing one, we need to use the states with `return_sequences=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can modify the input vector before the rnn cell with TimeDistributed\n",
    "timesteps = 10\n",
    "features = 8 # dimension of the innput of every cell\n",
    "\n",
    "#Shape [batch, timesteps, features] \n",
    "inputs = tf.keras.Input(shape=(timesteps, features), name='input')\n",
    "lstm_1 = layers.LSTM(64, return_sequences=True, name='lstm_1')(inputs)\n",
    "lstm_2 = layers.LSTM(64, return_sequences=True, name='lstm_2')(lstm_1)\n",
    "# last lstm layer depends in [one to many or  many to many]\n",
    "lstm_3 = layers.LSTM(64, return_sequences=False, name='lstm_3')(lstm_2)\n",
    "model = keras.Model(inputs=inputs, outputs=lstm_3, name='rnn_example')\n",
    "#print(model.summary())\n",
    "inputs = tf.random.normal([32, timesteps, features])\n",
    "print(model(inputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs\n",
    "\n",
    "For sequences other than time series (e.g. text), it is often the case that a RNN model\n",
    "can perform better if it not only processes sequence from start to end, but also\n",
    "backwards. For example, to predict the next word in a sentence, it is often useful to\n",
    "have the context around the word, not only just the words that come before it.\n",
    "\n",
    "Keras provides an easy API for you to build such bidirectional RNNs: the\n",
    "`keras.layers.Bidirectional` wrapper.\n",
    "\n",
    "[link to documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# If you crete a second layer you must set return_sequences=True\n",
    "model.add(\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(timesteps, features))\n",
    ")\n",
    "# Second Bidirectional layer\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "# Output\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Data Processing\n",
    "\n",
    "## Sliding Windows\n",
    "\n",
    "A sliding window is a technique used to create input-output pairs from time series data. It moves a fixed-size window over the time series data to extract subsequences as input features and the corresponding next value(s) as the target(s). The window size determines the number of past time steps to use as input features, and the horizon determines the number of future time steps to predict.\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/5nvJQB4/split-window.png\" alt=\"cnn\" border=\"0\">\n",
    "\n",
    "We will define two `create_windows` functions, one for `tf.data.Dataset` and another for NumPy, to create input-output pairs from the time series data using sliding windows.\n",
    "\n",
    "\n",
    "For example if  `data = [0,1,2,3,4,5,6]` :\n",
    "\n",
    "If we have `window_size=3`, and `horizon=2`, we use **windows of size 3** for predicting **2 steps ahead**.\n",
    "\n",
    "we are going to use `[0,1,2] (length=window_size=3)` to predict `4`, \n",
    "\n",
    "We need too create a training data like\n",
    "```python\n",
    "[0,1,2], 4 \n",
    "[1,2,3], 5 \n",
    "[2,3,4], 6\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows_np(data, window_size, horizon, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given time series data using NumPy.\n",
    "    \n",
    "    Parameters:\n",
    "    data (np.ndarray): Time series data with one dimension.\n",
    "    window_size (int): The number of past time steps to use as input features.\n",
    "    horizon (int): The number of future time steps to predict.\n",
    "    shuffle (bool): Shuffle the windows or not.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the input-output pairs (windows, targets) as NumPy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size+horizon-1])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "        X, y = X[indices], y[indices]\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trial, y_trial = create_windows_np(np.array([0, 1, 2, 3, 4, 5, 6]),\n",
    "                                     window_size=3,\n",
    "                                     horizon=2,\n",
    "                                     shuffle=False)\n",
    "for ind in range(len(y_trial)):\n",
    "    print(X_trial[ind, :], y_trial[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows_tf(data, window_size, horizon, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given time series data using tf.data.Dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data (np.ndarray): Time series data with with one dimension.\n",
    "    window_size (int): The number of past time steps to use as input features.\n",
    "    horizon (int): The number of future time steps to predict.\n",
    "    shuffle (bool): Whether to shuffle the data or not.\n",
    "    \n",
    "    Returns:\n",
    "    tf.data.Dataset: The resulting dataset.\n",
    "    \"\"\"  \n",
    "    ts_data = tf.data.Dataset.from_tensor_slices(data)\n",
    "    ts_data = ts_data.window(window_size + horizon, shift=1, drop_remainder=True)\n",
    "    ts_data = ts_data.flat_map(lambda window: window.batch(window_size + horizon))\n",
    "    ts_data = ts_data.map(lambda window: (window[:window_size], window[-1]))\n",
    "    if shuffle:\n",
    "        ts_data = ts_data.shuffle(buffer_size=data.shape[0])\n",
    "    return ts_data\n",
    "\n",
    "ts_dataset = create_windows_tf(np.array([0, 1, 2, 3, 4, 5, 6]),\n",
    "                                     window_size=3,\n",
    "                                     horizon=2,\n",
    "                                     shuffle=False)\n",
    "\n",
    "for window, target in ts_dataset:\n",
    "    print(window.numpy(), target.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple univariate time series forecasting with RNN\n",
    "\n",
    "We will use a sine wave as our example time series data. We will generate 1000 data points with a step of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sine wave data\n",
    "t = np.arange(0, 24, 0.02)\n",
    "y = np.sin(t)\n",
    "\n",
    "# Plot the sine wave\n",
    "plt.plot(t, y)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Sine Wave')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_size = int(len(y) * 0.8)\n",
    "ts_train, ts_test = y[:train_size], y[train_size:]\n",
    "print(ts_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "horizon = 10\n",
    "X_train, y_train = create_windows_np(ts_train, window_size, horizon=2, shuffle=True)\n",
    "X_test, y_test = create_windows_np(ts_test, window_size, horizon=2, shuffle=False)\n",
    "\n",
    "print(f'Train shape: {X_train.shape}')\n",
    "print(f'Test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_shape = (window_size, 1)\n",
    "\n",
    "# Define input shape and number of time steps\n",
    "inputs = keras.layers.Input(shape=(window_size, 1))\n",
    "\n",
    "# Define LSTM layer with a certain number of units\n",
    "lstm_out_1 = keras.layers.LSTM(8, return_sequences=False)(inputs)\n",
    "\n",
    "# Define output layer with a single unit\n",
    "outputs = keras.layers.Dense(1)(lstm_out_1)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=7,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained RNN model to make predictions on the test data.\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the actual vs. predicted values.\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple multivariate time series forecasting with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_windows_multivariate_np(data, window_size, horizon, target_col_idx, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given time series data using NumPy.\n",
    "\n",
    "    Parameters:\n",
    "    data (np.ndarray or pd.DataFrame): Time series data with multiple features.\n",
    "    window_size (int): The number of past time steps to use as input features.\n",
    "    horizon (int): The number of future time steps to predict.\n",
    "    target_col_idx (int): The index of the target column in the input data.\n",
    "    shuffle (bool): Whether to shuffle the data or not.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the input-output pairs (X, y) as NumPy arrays.\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.values\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i+window_size, :])\n",
    "        y.append(data[i+window_size+horizon-1, target_col_idx])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    if shuffle:\n",
    "        indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X, y = X[indices], y[indices]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_windows_multivariate_tf(data, window_size, horizon, target_col_idx, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates a dataset from the given time series data using tf.data.Dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Time series data with multiple features.\n",
    "    window_size (int): The number of past time steps to use as input features.\n",
    "    horizon (int): The number of future time steps to predict.\n",
    "    target_col_idx (int): The index of the target column in the input data.\n",
    "    shuffle (bool): Whether to shuffle the data or not.\n",
    "\n",
    "    Returns:\n",
    "    tf.data.Dataset: The resulting dataset.\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.values\n",
    "    \n",
    "    ts_data = tf.data.Dataset.from_tensor_slices(data)\n",
    "    ts_data = ts_data.window(window_size + horizon, shift=1, drop_remainder=True)\n",
    "    ts_data = ts_data.flat_map(lambda window: window.batch(window_size + horizon))\n",
    "    ts_data = ts_data.map(lambda window: (\n",
    "        window[:window_size], window[-1, target_col_idx]))\n",
    "    if shuffle:\n",
    "        ts_data = ts_data.shuffle(buffer_size=data.shape[0])\n",
    "    return ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic multivariate time series data\n",
    "t = np.arange(0, 24, 0.01)\n",
    "y1 = np.sin(0.1 * t)\n",
    "y2 = 0.5 * np.sin(0.2 * t)\n",
    "data = np.vstack((y1, y2)).T\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_size = int(data.shape[0] * 0.8)\n",
    "ts_train, ts_test = data[:train_size], data[train_size:]\n",
    "print(ts_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "horizon = 10\n",
    "\n",
    "\n",
    "train_ds = create_windows_multivariate_tf(\n",
    "    ts_train, window_size, horizon, target_col_idx=0, shuffle=True)\n",
    "\n",
    "test_ds = create_windows_multivariate_tf(\n",
    "    ts_test, window_size, horizon, target_col_idx=0, shuffle=False)\n",
    "\n",
    "train_ds = train_ds.batch(64)\n",
    "test_ds = test_ds.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window, target in train_ds:\n",
    "    print(window.numpy().shape, target.numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 2 # (y1, y2)\n",
    "\n",
    "# Define input shape and number of time steps\n",
    "inputs = keras.layers.Input(shape=(window_size, num_features))\n",
    "\n",
    "# Define LSTM layer with a certain number of units\n",
    "lstm_out_1 = keras.layers.LSTM(8, return_sequences=False)(inputs)\n",
    "\n",
    "# Define output layer with a single unit\n",
    "outputs = keras.layers.Dense(1)(lstm_out_1)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=12,\n",
    "    validation_data=test_ds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for x, y in train_ds.take(5):\n",
    "    x_in = x.numpy()\n",
    "    y_in = y.numpy().flatten()\n",
    "    pred = model.predict(x)\n",
    "    show_plot(\n",
    "        [x_in[0,:, 0], y_in[0], pred.flatten()[0]],\n",
    "        horizon,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(test_ds):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    y_pred = model.predict(x)\n",
    "    plt.plot(y.numpy(), label='Actual')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Data Time-Series\n",
    "\n",
    "We will be using Jena Climate dataset recorded by the\n",
    "[Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/).\n",
    "The dataset consists of 14 features such as temperature, pressure, humidity etc, recorded once per\n",
    "10 minutes.\n",
    "\n",
    "**Location**: Weather Station, Max Planck Institute for Biogeochemistry\n",
    "in Jena, Germany\n",
    "\n",
    "**Time-frame Considered**: Jan 10, 2009 - December 31, 2016\n",
    "\n",
    "\n",
    "The table below shows the column names, their value formats, and their description.\n",
    "\n",
    "Index| Features      |Format             |Description\n",
    "-----|---------------|-------------------|-----------------------\n",
    "1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n",
    "2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n",
    "3    |T (degC)       |-8.02              |Temperature in Celsius\n",
    "4    |Tpot (K)       |265.4              |Temperature in Kelvin\n",
    "5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n",
    "6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n",
    "7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n",
    "8    |VPact (mbar)   |3.11               |Vapor pressure\n",
    "9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n",
    "10   |sh (g/kg)      |1.94               |Specific humidity\n",
    "11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n",
    "12   |rho (g/m ** 3) |1307.75            |Airtight\n",
    "13   |wv (m/s)       |1.03               |Wind speed\n",
    "14   |max. wv (m/s)  |1.75               |Maximum wind speed\n",
    "15   |wd (deg)       |152.3              |Wind direction in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "\n",
    "uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "zip_path = keras.utils.get_file(origin=uri, fname=\"jena_climate_2009_2016.csv.zip\")\n",
    "zip_file = ZipFile(zip_path)\n",
    "zip_file.extractall()\n",
    "csv_path = \"jena_climate_2009_2016.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path, parse_dates =['Date Time'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Visualization\n",
    "\n",
    "To give us a sense of the data we are working with, each feature has been plotted below.\n",
    "This shows the distinct pattern of each feature over the time period from 2009 to 2016.\n",
    "It also shows where anomalies are present, which will be addressed during normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"Pressure\",\n",
    "    \"Temperature\",\n",
    "    \"Temperature in Kelvin\",\n",
    "    \"Temperature (dew point)\",\n",
    "    \"Relative Humidity\",\n",
    "    \"Saturation vapor pressure\",\n",
    "    \"Vapor pressure\",\n",
    "    \"Vapor pressure deficit\",\n",
    "    \"Specific humidity\",\n",
    "    \"Water vapor concentration\",\n",
    "    \"Airtight\",\n",
    "    \"Wind speed\",\n",
    "    \"Maximum wind speed\",\n",
    "    \"Wind direction in degrees\",\n",
    "]\n",
    "\n",
    "feature_keys = [\n",
    "    \"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"Tpot (K)\",\n",
    "    \"Tdew (degC)\",\n",
    "    \"rh (%)\",\n",
    "    \"VPmax (mbar)\",\n",
    "    \"VPact (mbar)\",\n",
    "    \"VPdef (mbar)\",\n",
    "    \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"wv (m/s)\",\n",
    "    \"max. wv (m/s)\",\n",
    "    \"wd (deg)\",\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"pink\",\n",
    "    \"gray\",\n",
    "    \"olive\",\n",
    "    \"cyan\",\n",
    "]\n",
    "\n",
    "date_time_key = \"Date Time\"\n",
    "\n",
    "\n",
    "def show_raw_visualization(data):\n",
    "    time_data = data[date_time_key]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i in range(len(feature_keys)):\n",
    "        key = feature_keys[i]\n",
    "        c = colors[i % (len(colors))]\n",
    "        t_data = data[key]\n",
    "        t_data.index = time_data\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            color=c,\n",
    "            title=\"{} - {}\".format(titles[i], key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "show_raw_visualization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heat map shows the correlation between different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmap(data):\n",
    "    plt.matshow(data.corr())\n",
    "    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_title = ['Pressure', 'Temperature', 'Saturation vapor pressure',\n",
    "                 'Vapor pressure deficit', 'Specific' 'humidity', 'Airtight', 'Wind speed']\n",
    "selected_features = ['p (mbar)',\n",
    " 'T (degC)',\n",
    " 'VPmax (mbar)',\n",
    " 'VPdef (mbar)',\n",
    " 'sh (g/kg)',\n",
    " 'rho (g/m**3)',\n",
    " 'wv (m/s)',\n",
    "\"max. wv (m/s)\",\"wd (deg)\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Here we are picking ~300,000 data points for training. Observation is recorded every\n",
    "10 mins, that means 6 times per hour. We will resample one point per hour since no\n",
    "drastic change is expected within 60 minutes. We do this via the `sampling_rate`\n",
    "argument in `timeseries_dataset_from_array` utility.\n",
    "\n",
    "We are tracking data from past 720 timestamps (720/6=120 hours). This data will be\n",
    "used to predict the temperature after 72 timestamps (76/6=12 hours).\n",
    "\n",
    "Since every feature has values with\n",
    "varying ranges, we do normalization to confine feature values to a range of `[0, 1]` before\n",
    "training a neural network.\n",
    "We do this by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "71.5 % of the data will be used to train the model, i.e. 300,693 rows. `split_fraction` can\n",
    "be changed to alter this percentage.\n",
    "\n",
    "The model is shown data for first 5 days i.e. 720 observations, that are sampled every\n",
    "hour. The temperature after 72 (12 hours * 6 observation per hour) observation will be\n",
    "used as a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.set_index('Date Time')\n",
    "## resample by the mean of hour values\n",
    "df =df.resample('1h').mean().fillna(method='ffill') # \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only important features\n",
    "df_0 = df.copy()\n",
    "df = df[selected_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the correlation heatmap, few parameters like Relative Humidity and\n",
    "Specific Humidity are redundant. Hence we will be using select features, not all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - test\n",
    "split_fraction = 0.7\n",
    "train_split = int(split_fraction * int(len(df)))\n",
    "\n",
    "train_data = df.iloc[0 : train_split]\n",
    "val_data = df.iloc[train_split:]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## previous hours to consider\n",
    "window_size = 6 \n",
    "## Number of hours later to predict\n",
    "horizon = 12\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "columns = df.columns\n",
    "# Get the target column index\n",
    "target_col_idx = columns.get_loc('T (degC)')\n",
    "\n",
    "dataset_train = create_windows_multivariate_tf(\n",
    "    train_data, window_size, horizon, target_col_idx=target_col_idx, shuffle=True)\n",
    "\n",
    "dataset_val = create_windows_multivariate_tf(\n",
    "    val_data, window_size, horizon, target_col_idx=target_col_idx, shuffle=False)\n",
    "\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_val = dataset_val.batch(500)\n",
    "\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "    break\n",
    "\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)\n",
    "inputs_shape = (inputs.shape[1], inputs.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    "`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "\n",
    "`Dataset.prefetch()` overlaps data preprocessing and model execution while training. \n",
    "\n",
    "Interested readers can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance#prefetching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "dataset_train = dataset_train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "dataset_val = dataset_val.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization layer\n",
    "We can normalize the features with [Normalization layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization).\n",
    "\n",
    "```python\n",
    "tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1, dtype=None, mean=None, variance=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "norm = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "`adapt` computes mean and std of the train data and store them as the layer's weights. `adapt`\n",
    " should be called before fit, evaluate, or predict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1, dtype=None, mean=None, variance=None\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "print('Unnormalized row: ', df.iloc[:1].values)\n",
    "print('Normalized row: ', norm(df.iloc[:1]))\n",
    "print('Normalized df, mean row: ',norm(df.values).numpy().mean(1))\n",
    "print('Normalized df, std row: ',np.std(norm(df.values).numpy(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features =  9\n",
    "inputs_shape = (window_size, num_features)\n",
    "\n",
    "inputs = keras.layers.Input(shape=inputs_shape)\n",
    "inputs_norm = norm(inputs)\n",
    "rnn_out = keras.layers.SimpleRNN(32)(inputs_norm)\n",
    "outputs = keras.layers.Dense(1)(rnn_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `ModelCheckpoint` callback to regularly save checkpoints, and\n",
    "the `EarlyStopping` callback to interrupt training when the validation loss\n",
    "is not longer improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the loss with the function below. After one point, the loss stops\n",
    "decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        horizon,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, (x, y) in enumerate(dataset_val):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    y_pred = model.predict(x)\n",
    "    plt.plot(y.numpy(), label='Actual')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:  Change the `keras.layers.SimpleRNN` layer to `keras.layers.LSTM` and to `keras.layers.GRU` and compare the results\n",
    "You can also set the `recurrent_dropout` parameter\n",
    "\n",
    "```python\n",
    "tf.keras.layers.x(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_features = ...\n",
    "inputs_shape = (window_size, num_features)\n",
    "\n",
    "\n",
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "inputs = layers.Input(shape=inputs_shape)\n",
    "inputs_norm = norm(inputs)\n",
    "\n",
    "## complete the code\n",
    "lstm_out = layers...(...)(...)\n",
    "outputs = layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        horizon,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "for i, (x, y) in enumerate(dataset_val):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    y_pred = model.predict(x)\n",
    "    plt.plot(y.numpy(), label='Actual')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Wind velocity\n",
    "One thing that should stand out is the min value of the wind velocity, wv (m/s) and max. wv (m/s) columns. This -9999 is likely erroneous. There's a separate wind direction column, so the velocity should be >=0. Replace it with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = df['wv (m/s)']\n",
    "bad_wv = wv == -9999.0\n",
    "wv[bad_wv] = 0.0\n",
    "df['wv (m/s)'] = wv\n",
    "\n",
    "max_wv = df['max. wv (m/s)']\n",
    "bad_max_wv = max_wv == -9999.0\n",
    "max_wv[bad_max_wv] = 0.0\n",
    "df['max. wv (m/s)'] = max_wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this will be easier for the model to interpret if you convert the wind direction and velocity columns to a wind vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = df.pop('wv (m/s)')\n",
    "\n",
    "# Convert to radians.\n",
    "wd_rad = df.pop('wd (deg)')*np.pi / 180\n",
    "\n",
    "# Calculate the wind x and y components.\n",
    "df['Wx'] = wv*np.cos(wd_rad)\n",
    "df['Wy'] = wv*np.sin(wd_rad)\n",
    "\n",
    "# Calculate the max wind x and y components.\n",
    "df['max Wx'] = max_wv*np.cos(wd_rad)\n",
    "df['max Wy'] = max_wv*np.sin(wd_rad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the Date Time column is very useful, but not in this string form. Start by converting it to seconds:\n",
    "\n",
    "\n",
    "Similar to the wind direction the time in seconds is not a useful model input. Being weather data it has clear daily and yearly periodicity. There are many ways you could deal with periodicity.\n",
    "\n",
    "A simple approach to convert it to a usable signal is to use sin and cos to convert the time to clear \"Time of day\" and \"Time of year\" signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamp_s = df.index.map(datetime.timestamp)\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - test\n",
    "split_fraction = 0.7\n",
    "train_split = int(split_fraction * int(len(df)))\n",
    "\n",
    "train_data = df.iloc[0 : train_split]\n",
    "val_data = df.iloc[train_split:]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## previous hours to consider\n",
    "window_size = 6 \n",
    "## Number of hours later to predict\n",
    "horizon = 12\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "# Get the target column index\n",
    "target_col_idx = columns.get_loc('T (degC)')\n",
    "\n",
    "dataset_train = create_windows_multivariate_tf(\n",
    "    train_data, window_size, horizon, target_col_idx=target_col_idx, shuffle=True)\n",
    "\n",
    "dataset_val = create_windows_multivariate_tf(\n",
    "    val_data, window_size, horizon, target_col_idx=target_col_idx, shuffle=False)\n",
    "\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_val = dataset_val.batch(500)\n",
    "\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "    break\n",
    "\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)\n",
    "inputs_shape = (inputs.shape[1], inputs.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "dataset_train = dataset_train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "dataset_val = dataset_val.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:  Use the same model as before and compare the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_features = ...\n",
    "inputs_shape = (past, num_features)\n",
    "\n",
    "inputs = layers.Input(shape=inputs_shape)\n",
    "inputs_norm = norm(inputs)\n",
    "## complete the code\n",
    "lstm_out = layers..(...)(...)\n",
    "outputs = layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        horizon,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "for i, (x, y) in enumerate(dataset_val):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    y_pred = model.predict(x)\n",
    "    plt.plot(y.numpy(), label='Actual')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:  Create a deep model stacking two recurrent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_features = ...\n",
    "inputs_shape = (window_size, num_features)\n",
    "\n",
    "\n",
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "inputs = layers.Input(shape=inputs_shape)\n",
    "inputs_norm = norm(inputs)\n",
    "\n",
    "## complete the code\n",
    "l_1 = keras.layers.LSTM(..., return_sequences=...)(...)\n",
    "l_2 = keras.layers.LSTM(..., return_sequences=...)(...)\n",
    "\n",
    "outputs = keras.layers.Dense(1)(...)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=...)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        horizon,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "for i, (x, y) in enumerate(dataset_val):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    y_pred = model.predict(x)\n",
    "    plt.plot(y.numpy(), label='Actual')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:  Use `Bidirectional`  layer\n",
    "\n",
    "```python\n",
    "layers.Bidirectional(layers.LSTM(64, return_sequences=)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(..., ...))\n",
    "inputs_norm = norm(inputs)\n",
    "\n",
    "## complete the code\n",
    "l_1 = ...(...)\n",
    "\n",
    "outputs = keras.layers.Dense(1)(...)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=...)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        horizon,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "for i, (x, y) in enumerate(dataset_val):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    y_pred = model.predict(x)\n",
    "    plt.plot(y.numpy(), label='Actual')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5:  Obtain a good model for predicting the temperature in 24h\n",
    "\n",
    "Try different architectures and different values for `window_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## previous hours to consider\n",
    "window_size = ...\n",
    "\n",
    "batch_size = ...\n",
    "\n",
    "\n",
    "# Get the target column index\n",
    "target_col_idx = columns.get_loc('T (degC)')\n",
    "\n",
    "dataset_train = create_windows_multivariate_tf(\n",
    "    train_data, window_size, horizon, target_col_idx=target_col_idx, shuffle=True)\n",
    "\n",
    "dataset_val = create_windows_multivariate_tf(\n",
    "    val_data, window_size, horizon, target_col_idx=target_col_idx, shuffle=False)\n",
    "\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_val = dataset_val.batch(500)\n",
    "\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "    break\n",
    "\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)\n",
    "inputs_shape = (inputs.shape[1], inputs.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "dataset_train = dataset_train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "dataset_val = dataset_val.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "    axis=-1\n",
    ")\n",
    "norm.adapt(dataset_train.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(..., ...))\n",
    "inputs_norm = norm(inputs)\n",
    "\n",
    "\n",
    "l_1 = keras.layers.LSTM(..., return_sequences=...)(...)\n",
    "l_2 = keras.layers.LSTM(..., return_sequences=...)(...)\n",
    "\n",
    "outputs = keras.layers.Dense(1)(...)\n",
    "\n",
    "model = keras.Model(inputs=...)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=3)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=25,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        horizon,\n",
    "        \"Single Step Prediction\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "for i, (x, y) in enumerate(dataset_val):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print('batch: ', i)\n",
    "    y_pred = model.predict(x)\n",
    "    plt.plot(y.numpy(), label='Actual')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Cryptocurrency Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, obtain and pre-process the data from [www.cryptodatadownload.com](www.cryptodatadownload.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.cryptodatadownload.com/cdd/gemini_{0}USD_1hr.csv\n",
    "# '../data/gemini_{0}USD_1hr.csv'\n",
    "def get_coin_df(fpath='../data/gemini_{0}USD_1hr.csv'):\n",
    "    coins = ['BTC', 'ETH', 'LTC', 'ZEC', 'DOGE']\n",
    "    file_path = fpath.format('BTC')\n",
    "    print(file_path)\n",
    "    df = pd.read_csv(file_path, parse_dates=['date'], skiprows=1, skipfooter=1)\n",
    "    df = df[['date', 'open']]\n",
    "    df = df.set_index('date')\n",
    "    df = df.sort_index()\n",
    "    df = df.rename(columns={'open': 'BTC'})\n",
    "    for coin in ['ETH', 'LTC', 'ZEC', 'DOGE']:\n",
    "        coin_path = fpath.format(coin)\n",
    "        df_coin = pd.read_csv(coin_path,\n",
    "                              parse_dates=['date'],\n",
    "                              skiprows=1,\n",
    "                              skipfooter=1)\n",
    "        df_coin = df_coin[['date', 'open']]\n",
    "        df_coin = df_coin.set_index('date')\n",
    "        df_coin = df_coin.sort_index()\n",
    "        df_coin = df_coin.rename(columns={'open': coin})\n",
    "        df[coin] = np.nan\n",
    "        df.loc[df_coin.index, coin] = df_coin.values.flatten()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = get_coin_df(\n",
    "    fpath='http://www.cryptodatadownload.com/cdd/Gemini_{0}USD_1h.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.diff()\n",
    "# df[df.columns] = np.sqrt(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the scaled values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "df_normalized = pd.DataFrame(x_scaled, columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = ['BTC', 'ETH', 'LTC', 'ZEC']\n",
    "for col in coins:\n",
    "    df_normalized[col].plot(legend=True, figsize=(15, 7))\n",
    "plt.ylabel('Dolars scaled')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Time Series\n",
    "To start with, we will use only one cryptocurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_target = 'ETH' #  Coin that we want to predict, ['BTC', 'ETH', 'LTC', 'ZEC']\n",
    "df_coin = df[coin_target].copy()\n",
    "# delete nan rows\n",
    "df_coin = df_coin.dropna()\n",
    "df_coin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coin.plot(legend=True,figsize=(10,5))\n",
    "plt.ylabel(coin_target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "test_date = pd.Timestamp(\"2023-01-10\")\n",
    "init_date = pd.Timestamp(\"2017-10-08 14:00:00\")\n",
    "\n",
    "# train_data = df_coin.loc[df_coin.index < test_date].values\n",
    "\n",
    "train_data = df_coin.loc[(df_coin.index < test_date) *\n",
    "                         (df_coin.index > init_date)].values\n",
    "\n",
    "test_data = df_coin.loc[df_coin.index >= test_date].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = train_data.mean()\n",
    "sigma = train_data.std()\n",
    "print('mu, sigma: ', mu, sigma)\n",
    "\n",
    "train_data = (train_data - mu) / sigma\n",
    "test_data = (test_data - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0Question 1: Create a model to predict the ETH value in 24h, `RMSE(test) < 50$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windows\n",
    "window_size, horizon = (6, 24)\n",
    "X_train, y_train = create_windows_np(train_data, window_size, horizon, shuffle=True)\n",
    "X_test, y_test = create_windows_np(test_data, window_size, horizon, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0, :], y_test[0], test_data[:window_size + horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "##\u00a0Modify the initial model to obtain better results\n",
    "\n",
    "inputs = keras.layers.Input(shape=(window_size, 1))\n",
    "\n",
    "l_1 = layers.SimpleRNN(1, return_sequences=False) (inputs) \n",
    "outputs = layers.Dense(1)(l_1)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=8)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).flatten() * sigma + mu\n",
    "y_target = y_test * sigma + mu\n",
    "diff = y_pred - y_target\n",
    "print('max deviation: ', np.abs(y_pred - y_target).max())\n",
    "print('RMSE: ', np.mean((y_pred - y_target)**2)**0.5)\n",
    "print('MAE: ', np.abs(y_pred - y_target).mean())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_target)\n",
    "plt.plot(y_pred)\n",
    "plt.legend(['True {0}'.format(coin_target), 'Predictions {0}'.format(coin_target)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can try feature engineering with df\n",
    "coin_target = 'ETH' # 'BTC'\n",
    "coins =  ['ETH', 'LTC', 'ZEC', 'BTC'] # ['BTC', 'ETH', 'LTC', 'ZEC']\n",
    "df_multi = df[coins].dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create new features\n",
    "\n",
    "## moving average features\n",
    "for coin in coins:\n",
    "    df_multi[coin+'_week_mu'] = df_multi[coin].rolling(window=7*24).mean()\n",
    "    df_multi[coin+'_week_sigma'] = df_multi[coin].rolling(window=7*24).std()\n",
    "df_multi = df_multi.dropna()\n",
    "\n",
    "## time features\n",
    "dt = df_multi.index.to_numpy()\n",
    "dt = (dt - dt.min()) / np.timedelta64(24 * 365, 'h')\n",
    "df_multi['time'] = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_multi.loc[(df_multi.index < test_date) * (df_multi.index > init_date ), : ].copy()\n",
    "test_data = df_multi.loc[df_multi.index >= test_date, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_multi.columns\n",
    "mu_dict = {}\n",
    "sigma_dict = {}\n",
    "for c in features:\n",
    "    mu = train_data[c].mean()\n",
    "    sigma = train_data[c].std()\n",
    "    mu_dict[c] = mu\n",
    "    sigma_dict[c] = sigma\n",
    "    print('feature: {0} , mu,sigma'.format(c), mu, sigma)\n",
    "print(mu_dict, sigma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in features:\n",
    "    mu = mu_dict[c]\n",
    "    sigma = sigma_dict[c]\n",
    "    train_data.loc[:, c] = (train_data[c].values - mu) / sigma\n",
    "    test_data.loc[:, c] = (test_data[c].values - mu) / sigma\n",
    "train_data.describe()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Create a model to predict the ETH value in 24h, `RMSE(test) < 50$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create windows\n",
    "window_size, horizon = (..., 24)\n",
    "\n",
    "columns = df_multi.columns\n",
    "# Get the target column index\n",
    "target_col_idx = columns.get_loc(coin_target)\n",
    "\n",
    "X_train, y_train = create_windows_multivariate_np(\n",
    "    train_data, window_size, horizon, target_col_idx=target_col_idx, shuffle=True)\n",
    "X_test, y_test = create_windows_multivariate_np(\n",
    "    test_data, window_size, horizon, target_col_idx=target_col_idx, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.layers.Input(shape=(window_size, len(features)))\n",
    "...\n",
    "\n",
    "outputs = layers.Dense(1)(...)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=\"mse\")\n",
    "\n",
    "model.summary()\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=8)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = sigma_dict[coin_target]\n",
    "mu = mu_dict[coin_target]\n",
    "y_pred = model.predict(X_test).flatten() * sigma + mu\n",
    "y_target = y_test * sigma + mu\n",
    "diff = y_pred - y_target\n",
    "print('max deviation: ', np.abs(y_pred - y_target).max())\n",
    "print('RMSE: ', np.mean((y_pred - y_target)**2)**0.5)\n",
    "print('MAE: ', np.abs(y_pred - y_target).mean())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_target)\n",
    "plt.plot(y_pred)\n",
    "plt.legend(['True {0}'.format(coin_target), 'Predictions {0}'.format(coin_target)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Finance time series, SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download yahoo finance library\n",
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the list of S&P 500 companies\n",
    "sp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "data = pd.read_html(sp500_url)\n",
    "sp500_table = data[0]\n",
    "sp500_tickers = sp500_table['Symbol'].tolist()\n",
    "\n",
    "# Download hourly stock data for each company\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2023-05-06'\n",
    "interval = '1h'\n",
    "\n",
    "data_dict = {}\n",
    "for ticker in tqdm(sp500_tickers):\n",
    "    try:\n",
    "        stock_data = yf.download(ticker, start=start_date, end=end_date, interval=interval, progress=False)\n",
    "        data_dict[ticker] = stock_data['Close']\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {ticker}: {e}\")\n",
    "\n",
    "# Create a DataFrame with every company in the columns\n",
    "data_df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['MSFT', 'META', 'GOOGL', 'AMZN']\n",
    "data_df[cols].plot(figsize=(15, 7), legend=False)\n",
    "plt.title(\"S&P 500 Hourly Stock Data\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/rnn\n",
    "\n",
    "https://keras.io/examples/timeseries/timeseries_weather_forecasting/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}