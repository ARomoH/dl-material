{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/RNN/IMBD_RNN.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/RNN/IMBD_RNN.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Example\n",
    " Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this example, you\u2019ll learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def show_loss_accuracy_evolution(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Sparse Categorical Crossentropy')\n",
    "    ax1.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    ax1.plot(hist['epoch'], hist['val_loss'], label='Val Error')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['val_accuracy'], label='Val Accuracy')\n",
    "    ax2.grid()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset: The IMDB dataset\n",
    "We\u2019ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They\u2019re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews. The  parameter `num_words` controls how many words different we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "num_words = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform word_id to word and reverse\n",
    "word2int = imdb.get_word_index()\n",
    "word2int = {w: i+3 for w, i in word2int.items()}\n",
    "word2int[\"<PAD>\"] = 0\n",
    "word2int[\"<START>\"] = 1\n",
    "word2int[\"<UNK>\"] = 2\n",
    "word2int[\"<UNUSED>\"] = 3\n",
    "int2word = {i: w for w, i in word2int.items()}\n",
    "num_words = num_words+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transforming an id-sequence to a phrase use get_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sentence, int2word):\n",
    "    return ' '.join([int2word.get(i, '<UNK>') for i in sentence])\n",
    "\n",
    "\n",
    "get_words(train_data[0], int2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP model\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "You need to convert your raw text to an appropriate input to a sequential model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_sentence(text, word2int):\n",
    "    tokens = text.split(' ')\n",
    "    tokens_id = [word2int.get(tk,2) for tk in tokens]\n",
    "    return tokens_id\n",
    "\n",
    "text = get_words(train_data[0], int2word)\n",
    "print(text)\n",
    "print(vectorize_text_sentence(text, word2int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a bag of words model. BoW is a simplifying representation used in natural language processing. In this model, a text (such as a sentence or a document) is represented as the Each key is the word, and each value is the frequency of occurrences of that word in the given text document.\n",
    "\n",
    "- **Input document**: `\"John likes to watch movies Mary likes movies too\"`\n",
    "- **BoW**: `{'John': 0.11, 'likes': 0.22, 'to': 0.11, 'watch': 0.11, 'movies': 0.22, 'Mary': 0.11, 'too': 0.11}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_bag_of_words(sequence, norm=True):\n",
    "    word_count = Counter(sequence)\n",
    "    if norm:\n",
    "        total = sum(word_count.values())\n",
    "        word_freq = {w: n / total for w, n in word_count.items()}\n",
    "        return word_freq\n",
    "    else:\n",
    "        return dict(word_count.items())\n",
    "\n",
    "\n",
    "text_example = \"John likes to watch movies Mary likes movies too\"\n",
    "print('text_example', text_example)\n",
    "text_sequence = text_example.split()\n",
    "print('text splitted', text_sequence)\n",
    "bag_of_words = get_bag_of_words(text_sequence)\n",
    "print('bag_of_words', bag_of_words)\n",
    "print('bag_of_words norm=False', get_bag_of_words(text_sequence, norm=False))\n",
    "print(\n",
    "    'bag_of_words with indexes', {\n",
    "        word2int[w.lower()]: p\n",
    "        for w, p in get_bag_of_words(text_sequence, norm=False).items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we convert every BoW to a vector of `dim=num_words` with `vectorize_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(sequence, num_words, norm=True):\n",
    "    vec = np.zeros(num_words)\n",
    "    bow = get_bag_of_words(sequence, norm)\n",
    "    for w, freq in bow.items():\n",
    "        if w < num_words:\n",
    "            vec[w] = freq\n",
    "    return vec\n",
    "\n",
    "\n",
    "def vectorize_sequences(sequences, num_words=num_words, norm=True):\n",
    "    \"\"\"Creates an all-zero matrix of shape (len(sequences), num_words)\"\"\"\n",
    "    results = np.zeros((len(sequences), num_words))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, :] = vectorize_sequence(sequence, num_words, norm)\n",
    "    return results\n",
    "\n",
    "\n",
    "x_train = vectorize_sequences(train_data, num_words=num_words)\n",
    "x_test = vectorize_sequences(test_data, num_words=num_words)\n",
    "y_train =np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train a model \n",
    "Define, compile and fit your sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "history = model.fit(x_train, y_train, validation_split=0.2, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "You need to obtain a Test Accuracy > 0.85. Try to get more than 0.9!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictioins with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['the film was really bad and i am very disappointed',\n",
    "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
    "           'this film was just brilliant']\n",
    "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
    "             for review in reviews]\n",
    "\n",
    "x_pred = vectorize_sequences(sequences, num_words=num_words)\n",
    "np.round(model.predict(x_pred), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model\n",
    "\n",
    "Lets use a recurrent neural network and compare results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0Simple RNN model\n",
    "\n",
    "There are three built-in RNN layers in Keras:\n",
    "\n",
    "1. [`keras.layers.SimpleRNN`](https://keras.io/api/layers/recurrent_layers/simple_rnn/), a fully-connected RNN where the output from previous\n",
    "timestep is to be fed to next timestep.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.SimpleRNN(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    ")\n",
    "````\n",
    "\n",
    "2. [`keras.layers.GRU`](https://keras.io/api/layers/recurrent_layers/gru/), first proposed in\n",
    "[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n",
    "```python\n",
    "tf.keras.layers.GRU(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    ")\n",
    "```\n",
    "\n",
    "3. [`keras.layers.LSTM`](https://keras.io/api/layers/recurrent_layers/lstm/), first proposed in\n",
    "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "```python\n",
    "tf.keras.layers.LSTM(\n",
    "    units,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    ")\n",
    "````\n",
    "For more information, see the\n",
    "[RNN API documentation](https://keras.io/api/layers/recurrent_layers/).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In sequence classification we are going to use the **many-to-one** architecture with default parameter `return_sequences=False`.\n",
    "\n",
    "The shape of the output  for this architecture  is `(batch_size, units)`.\n",
    "where `units` corresponds to the `units` argument passed to the layer's constructor.\n",
    "\n",
    "Lets see one some examples for understanding the input/output dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims of input: [batch, timesteps, features]\n",
    "inputs = tf.random.normal([32, 10, 4])\n",
    "print('input dim (batch, timesteps, feature): ', inputs.shape)\n",
    "# return_sequences=False, return_state=False\n",
    "lstm = tf.keras.layers.LSTM(units= 2)\n",
    "output = lstm(inputs)\n",
    "print('return_state=False output shape: ',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNN\n",
    "We can stack multiple layers of RNNs on top of each other. Each hidden state is continuously passed to both the next time step of the current layer and the current time step of the next layer.\n",
    "\n",
    "For stack another RNN layer to an existing one, we need to use the states with `return_sequences=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can modify the input vector before the rnn cell with TimeDistributed\n",
    "timesteps = 10\n",
    "features = 8 # dimension of the innput of every cell\n",
    "\n",
    "#Shape [batch, timesteps, features] \n",
    "inputs = tf.keras.Input(shape=(timesteps, features), name='input')\n",
    "lstm_1 = layers.LSTM(64, return_sequences=True, name='lstm_1')(inputs)\n",
    "lstm_2 = layers.LSTM(64, return_sequences=True, name='lstm_2')(lstm_1)\n",
    "# last lstm layer depends in [one to many or  many to many]\n",
    "lstm_3 = layers.LSTM(64, return_sequences=False, name='lstm_3')(lstm_2)\n",
    "model = keras.Model(inputs=inputs, outputs=lstm_3, name='rnn_example')\n",
    "#print(model.summary())\n",
    "inputs = tf.random.normal([32, timesteps, features])\n",
    "print(model(inputs).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs\n",
    "\n",
    "For sequences other than time series (e.g. text), it is often the case that a RNN model\n",
    "can perform better if it not only processes sequence from start to end, but also\n",
    "backwards. For example, to predict the next word in a sentence, it is often useful to\n",
    "have the context around the word, not only just the words that come before it.\n",
    "\n",
    "Keras provides an easy API for you to build such bidirectional RNNs: the\n",
    "`keras.layers.Bidirectional` wrapper.\n",
    "\n",
    "[link to documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# If you crete a second layer you must set return_sequences=True\n",
    "model.add(\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(timesteps, features))\n",
    ")\n",
    "# Second Bidirectional layer\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "# Output\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "num_words = 2000\n",
    "((train_data, train_labels), (test_data, test_labels)\n",
    " ) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "# \u00a0limit the data for class time\n",
    "'''size = 15000\n",
    "(train_data, train_labels), (test_data, test_labels) = (\n",
    "    (train_data[:size], train_labels[:size]), (test_data[:size], test_labels[:size]))\n",
    "'''\n",
    "# Transform word_id to word and reverse\n",
    "word2int = imdb.get_word_index()\n",
    "word2int = {w: i+3 for w, i in word2int.items()}\n",
    "word2int[\"<PAD>\"] = 0\n",
    "word2int[\"<START>\"] = 1\n",
    "word2int[\"<UNK>\"] = 2\n",
    "word2int[\"<UNUSED>\"] = 3\n",
    "int2word = {i: w for w, i in word2int.items()}\n",
    "num_words = num_words+3\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "For data preprocessing we first use [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences):\n",
    "```python\n",
    "tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=None, dtype='int32', padding='pre',\n",
    "    truncating='pre', value=0.0\n",
    ")\n",
    "```\n",
    "- **padding**:\t'pre' or 'post' (optional, defaults to 'pre'): pad either before or after each sequence.\n",
    "- **truncating**:\tString, 'pre' or 'post' (optional, defaults to 'pre'): remove values from sequences larger than maxlen, either at the beginning or at the end of the sequences.\n",
    "\n",
    "\n",
    "Our RNN will take sequences of constant length. In our case this length is the `maxlen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "input_seq = [1, 2, 3]\n",
    "max_len = 5\n",
    "print('input sequence: ', input_seq)\n",
    "pad_seq = sequence.pad_sequences([input_seq], maxlen=max_len)\n",
    "print('input sequence with padding: ', pad_seq)\n",
    "\n",
    "input_seq = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "max_len = 5\n",
    "print('input sequence: ', input_seq)\n",
    "pad_seq = sequence.pad_sequences([input_seq], maxlen=max_len)\n",
    "print('input sequence with padding: ', pad_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = [1, 2, 3]\n",
    "max_len = 5\n",
    "print('input sequence: ', input_seq)\n",
    "pad_seq = sequence.pad_sequences([input_seq], maxlen=max_len, padding='post')\n",
    "print('input sequence with padding: ', pad_seq)\n",
    "\n",
    "input_seq = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "max_len = 5\n",
    "print('input sequence: ', input_seq)\n",
    "pad_seq = sequence.pad_sequences([input_seq], maxlen=max_len, truncating='post')\n",
    "print('input sequence with padding: ', pad_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "x_train_seq = sequence.pad_sequences(train_data, maxlen=max_len, truncating='post', padding='post')\n",
    "x_test_seq = sequence.pad_sequences(test_data, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "print('train shape:', x_train_seq.shape)\n",
    "print('test shape:', x_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RNN model\n",
    "\n",
    "For the input of the first rnn layer we need a tensor of `(timesteps, features)` or `(batchsize, timesteps, features)`. We have a matrix of sentences of `(train_size, max_len)`. Every sentence is a  `max_len`, we need to convert it to a sentence of one-hot vectors of dim `(max_len, num_words)`. \n",
    "For get the one-hot encoding of every sequence we are going to use:\n",
    "\n",
    "```python\n",
    "layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
    "  input_length=max_len, embeddings_initializer='identity', trainable=False)\n",
    "```\n",
    "\n",
    "This layer converts the input tensor `(batch_size, max_len)` to one-hot encoded sequences `(batch_size, max_len, num_words)`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq = np.array([[[0, 1, 2, 2, 0]]])\n",
    "print(seq, seq.shape)\n",
    "layers.Embedding(input_dim=3, output_dim=3,\n",
    "                 input_length=5, embeddings_initializer='identity',\n",
    "                 trainable=False)(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0RNN model\n",
    "Use `keras.layers.SimpleRNN`,  `keras.layers.GRU`,  `keras.layers.LSTM` or `keras.layers.Bidirectional`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
    "## one-hot encoding\n",
    "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
    "                           input_length=max_len, embeddings_initializer='identity',\n",
    "                           trainable=False))\n",
    "## complete the model with recurrent layers\n",
    "#model.add(...)\n",
    "model.add(layers.SimpleRNN(16, return_sequences=False))\n",
    "## add binary classification output\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the loss and see the results\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "epochs = 5\n",
    "history = model.fit(x_train_seq, train_labels,\n",
    "                    validation_split=0.1, epochs=epochs,\n",
    "                    batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history)\n",
    "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_errors(x_test, model, labels, int2word, n_samples=10):\n",
    "    preds = 1.0 * (model.predict(x_test).flatten() > 0.5)\n",
    "    bad_pred_inds = np.where(preds != labels)[0]\n",
    "    n_samples = min(len(bad_pred_inds), n_samples)\n",
    "    samples_inds = np.random.choice(bad_pred_inds, n_samples)\n",
    "    for ind in samples_inds:\n",
    "        print('Predicted : {0}, real : {1}, lenght: {2}'.format(\n",
    "            int(preds[ind]), labels[ind], len(test_data[ind])))\n",
    "        print(get_words(test_data[ind], int2word))\n",
    "        print()\n",
    "    return\n",
    "\n",
    "show_errors(x_test_seq, model, test_labels, int2word, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictioins with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['the film was really bad and i am very disappointed',\n",
    "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
    "           'this film was just brilliant',\n",
    "           'the film is not good',\n",
    "           'the film is not bad',\n",
    "          'the movie is not bad I like it']\n",
    "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
    "             for review in reviews]\n",
    "\n",
    "##\u00a0Padding the sequences\n",
    "x_pred  = sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')# ...\n",
    "\n",
    "np.round(model.predict(x_pred), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0*(model.predict(x_pred) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0GRU model\n",
    "Use `keras.layers.GRU`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
    "## one-hot encoding\n",
    "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
    "                           input_length=max_len, embeddings_initializer='identity',\n",
    "                           trainable=False))\n",
    "\n",
    "## complete the model with recurrent layers\n",
    "model.add(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the loss and see the results\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=...,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "epochs = 5\n",
    "history = model.fit(x_train_seq, train_labels,\n",
    "                    validation_split=0.1, epochs=epochs,\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0LSTM model\n",
    "Use `keras.layers.LSTM` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
    "## one-hot encoding\n",
    "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
    "                           input_length=max_len, embeddings_initializer='identity',\n",
    "                           trainable=False))\n",
    "\n",
    "## complete the model with recurrent layers\n",
    "model.add(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the loss and see the results\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=...,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "epochs = 5\n",
    "history = model.fit(x_train_seq, train_labels,\n",
    "                    validation_split=0.1, epochs=epochs,\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0Deep model\n",
    "Use `keras.layers.SimpleRNN`,  `keras.layers.GRU`,  `keras.layers.LSTM` or `keras.layers.Bidirectional`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
    "## one-hot encoding\n",
    "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
    "                           input_length=max_len, embeddings_initializer='identity',\n",
    "                           trainable=False))\n",
    "\n",
    "## complete the model with recurrent layers\n",
    "model.add(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the loss and see the results\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=...,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "epochs = 5\n",
    "history = model.fit(x_train_seq, train_labels,\n",
    "                    validation_split=0.1, epochs=epochs,\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\u00a0Bidirectional model\n",
    "Use `keras.layers.SimpleRNN`,  `keras.layers.GRU`,  `keras.layers.LSTM` with `keras.layers.Bidirectional`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
    "## one-hot encoding\n",
    "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
    "                           input_length=max_len, embeddings_initializer='identity',\n",
    "                           trainable=False))\n",
    "\n",
    "## complete the model with recurrent layers\n",
    "model.add(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the loss and see the results\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=...,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "epochs = 5\n",
    "history = model.fit(x_train_seq, train_labels,\n",
    "                    validation_split=0.1, epochs=epochs,\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test_seq, test_labels, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictioins with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['the film was really bad and i am very disappointed',\n",
    "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
    "           'this film was just brilliant',\n",
    "           'the film is not good',\n",
    "           'the film is not bad',\n",
    "          'the movie is not bad I like it']\n",
    "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
    "             for review in reviews]\n",
    "\n",
    "##\u00a0Padding the sequences\n",
    "x_pred  = sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')# ...\n",
    "\n",
    "np.round(model.predict(x_pred), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_errors(x_test_seq, model, test_labels, int2word, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Use a convolutional network instead of a RNN\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Conv1D(\n",
    "    filters, kernel_size\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "tf.keras.layers.MaxPool1D(\n",
    "    pool_size=2\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Flatten()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "num_words = 2000\n",
    "((train_data, train_labels), (test_data, test_labels)\n",
    " ) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "# \u00a0limit the data for class time\n",
    "# Transform word_id to word and reverse\n",
    "word2int = imdb.get_word_index()\n",
    "word2int = {w: i+3 for w, i in word2int.items()}\n",
    "word2int[\"<PAD>\"] = 0\n",
    "word2int[\"<START>\"] = 1\n",
    "word2int[\"<UNK>\"] = 2\n",
    "word2int[\"<UNUSED>\"] = 3\n",
    "int2word = {i: w for w, i in word2int.items()}\n",
    "num_words = num_words+3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "x_train_seq = sequence.pad_sequences(train_data, maxlen=max_len, truncating='post', padding='post')\n",
    "x_test_seq = sequence.pad_sequences(test_data, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "print('train shape:', x_train_seq.shape)\n",
    "print('test shape:', x_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(max_len,), name='input'))\n",
    "## one-hot encoding\n",
    "model.add(layers.Embedding(input_dim=num_words, output_dim=num_words,\n",
    "                           input_length=max_len, embeddings_initializer='identity',\n",
    "                           trainable=False))\n",
    "\n",
    "\n",
    "model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "epochs = 5\n",
    "history = model.fit(x_train_seq, train_labels,\n",
    "                    validation_split=0.1, epochs=epochs,\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['the film was really bad and i am very disappointed',\n",
    "           'The film was very funny entertaining and good we had a great time . brilliant film',\n",
    "           'this film was just brilliant',\n",
    "           'the film is not good',\n",
    "           'the film is not bad']\n",
    "sequences = [vectorize_text_sentence(review.lower(), word2int)\n",
    "             for review in reviews]\n",
    "\n",
    "##\u00a0Padding the sequences\n",
    "x_pred  = sequence.pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')# ...\n",
    "\n",
    "np.round(model.predict(x_pred), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_errors(x_test_seq, model, test_labels, int2word, n_samples=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}