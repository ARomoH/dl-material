{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1QC-kBux1ZltvboH5-TXJML86yZT_ZVc7?usp=sharing)"
      ],
      "metadata": {
        "id": "6HjD--_WyBVv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B--qBaAlxjml"
      },
      "source": [
        "# Semantic similarity using pretrained encoders\n",
        "\n",
        "\n",
        "source: https://keras.io/examples/nlp/semantic_similarity_with_keras_hub/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl2Y3rZFxjmm"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Semantic similarity refers to the task of determining the degree of similarity between two\n",
        "sentences in terms of their meaning. Using SNLI (Stanford Natural Language Inference) corpus to predict sentence semantic similarity.\n",
        " - https://nlp.stanford.edu/projects/snli/\n",
        "\n",
        "\n",
        "Review all potential architectures that you can use in Keras:\n",
        "\n",
        "- https://keras.io/keras_hub/api/models/\n",
        "\n",
        "Check more model checkpoints in the hub:\n",
        "- https://www.kaggle.com/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTnJ3OrHxjmn"
      },
      "outputs": [],
      "source": [
        "# !pip install -q --upgrade keras-hub\n",
        "# !pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpPsMtWwxjmn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras_hub\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDZyby0Bxjmn"
      },
      "source": [
        "To load the SNLI dataset, we use the tensorflow-datasets library, which\n",
        "contains over 550,000 samples in total. However, to ensure that this example runs\n",
        "quickly, we use only 20% of the training samples.\n",
        "\n",
        "## Overview of SNLI Dataset\n",
        "\n",
        "Every sample in the dataset contains three components: `hypothesis`, `premise`,\n",
        "and `label`. epresents the original caption provided to the author of the pair,\n",
        "while the hypothesis refers to the hypothesis caption created by the author of\n",
        "the pair. The label is assigned by annotators to indicate the similarity between\n",
        "the two sentences.\n",
        "\n",
        "The dataset contains three possible similarity label values: Contradiction, Entailment,\n",
        "and Neutral. Contradiction represents completely dissimilar sentences, while Entailment\n",
        "denotes similar meaning sentences. Lastly, Neutral refers to sentences where no clear\n",
        "similarity or dissimilarity can be established between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS4dfblvxjmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b879df05-b1e9-4332-a162-39f3eb91e842"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hypothesis': <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              " array([b'A girl is entertaining on stage',\n",
              "        b'A group of people posing in front of a body of water.',\n",
              "        b\"The group of people aren't inide of the building.\",\n",
              "        b'The people are taking a carriage ride.'], dtype=object)>,\n",
              " 'label': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 0, 0, 0])>,\n",
              " 'premise': <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              " array([b'A girl in a blue leotard hula hoops on a stage with balloon shapes in the background.',\n",
              "        b'A group of people taking pictures on a walkway in front of a large body of water.',\n",
              "        b'Many people standing outside of a place talking to each other in front of a building that has a sign that says \"HI-POINTE.\"',\n",
              "        b'Three people are riding a carriage pulled by four horses.'],\n",
              "       dtype=object)>}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "snli_train = tfds.load(\"snli\", split=\"train[:20%]\")\n",
        "snli_val = tfds.load(\"snli\", split=\"validation\")\n",
        "snli_test = tfds.load(\"snli\", split=\"test\")\n",
        "\n",
        "# Here's an example of how our training samples look like, where we randomly select\n",
        "# four samples:\n",
        "sample = snli_test.batch(4).take(1).get_single_element()\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgEWMd0vxjmo"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "In our dataset, we have identified that some samples have missing or incorrectly labeled\n",
        "data, which is denoted by a value of -1. To ensure the accuracy and reliability of our model,\n",
        "we simply filter out these samples from our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n74xW6lUxjmo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def filter_labels(sample):\n",
        "    return sample[\"label\"] >= 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14NFfAX_xjmo"
      },
      "source": [
        "Here's a utility function that splits the example into an `(x, y)` tuple that is suitable\n",
        "for `model.fit()`. By default, `keras_hub.models.BertClassifier` will tokenize and pack\n",
        "together raw strings using a `\"[SEP]\"` token during training. Therefore, this label\n",
        "splitting is all the data preparation that we need to perform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkOt1_5mxjmo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split_labels(sample):\n",
        "    x = (sample[\"hypothesis\"], sample[\"premise\"])\n",
        "    y = sample[\"label\"]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_ds = (\n",
        "    snli_train.filter(filter_labels)\n",
        "    .map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(16)\n",
        ")\n",
        "val_ds = (\n",
        "    snli_val.filter(filter_labels)\n",
        "    .map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(16)\n",
        ")\n",
        "test_ds = (\n",
        "    snli_test.filter(filter_labels)\n",
        "    .map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(16)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iNy4LSGxjmo"
      },
      "source": [
        "## Establishing baseline with BERT.\n",
        "\n",
        "We use the BERT model from KerasHub to establish a baseline for our semantic similarity\n",
        "task. The `keras_hub.models.BertClassifier` class attaches a classification head to the BERT\n",
        "Backbone.\n",
        "\n",
        "KerasHub models have built-in tokenization capabilities that handle tokenization by default\n",
        "based on the selected model. However, users can also use custom preprocessing techniques\n",
        "as per their specific needs. If we pass a tuple as input, the model will tokenize all the\n",
        "strings and concatenate them with a `\"[SEP]\"` separator.\n",
        "\n",
        "We use this model with pretrained weights, and we can use the `from_preset()` method\n",
        "to use our own preprocessor. For the SNLI dataset, we set `num_classes` to 3.\n",
        "\n",
        "Pretrained model: https://www.kaggle.com/models/keras/bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_uhdc3Bxjmo"
      },
      "outputs": [],
      "source": [
        "bert_classifier = keras_hub.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\", num_classes=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5gxMe0Xxjmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52aea1c1-9c5f-4f2e-df87-2173c0daf5f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   6867/Unknown \u001b[1m189s\u001b[0m 25ms/step - accuracy: 0.6034 - loss: 0.8602"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6867/6867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 26ms/step - accuracy: 0.6034 - loss: 0.8602 - val_accuracy: 0.7650 - val_loss: 0.5798\n",
            "\u001b[1m614/614\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7673 - loss: 0.5710\n",
            "CPU times: user 1min 51s, sys: 11 s, total: 2min 3s\n",
            "Wall time: 3min 26s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.580228865146637, 0.7630293369293213]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "%%time\n",
        "bert_classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(5e-5),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "bert_classifier.fit(train_ds, validation_data=val_ds, epochs=1)\n",
        "bert_classifier.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u0TwPL-xjmp"
      },
      "source": [
        "## Save and Reload the model\n",
        "\n",
        "If you want to do several test, you need to reload original model and save the results of the current training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFqdLbKfxjmp"
      },
      "outputs": [],
      "source": [
        "# bert_classifier.save(\"bert_classifier.keras\")\n",
        "# restored_model = keras.models.load_model(\"bert_classifier.keras\")\n",
        "# restored_model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13n6pvi8xjmp"
      },
      "source": [
        "**Next steps**: improve model finetuning RoBERTa checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference example"
      ],
      "metadata": {
        "id": "PK-c9PpT29SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_similarity(model, sentence1, sentence2):\n",
        "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
        "    test_data = BertSemanticDataGenerator(\n",
        "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "    )\n",
        "\n",
        "    proba = model.predict(test_data[0])[0]\n",
        "    idx = np.argmax(proba)\n",
        "    proba = f\"{proba[idx]: .2f}%\"\n",
        "    pred = labels[idx]\n",
        "    return pred, proba"
      ],
      "metadata": {
        "id": "7iGjHl6J28qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
        "\n",
        "hypotheses = [\n",
        "    \"A child is eating ice cream near the park.\",\n",
        "    \"A woman is sleeping on the couch.\"\n",
        "]\n",
        "premises = [\n",
        "    \"The kid is enjoying a sweet outside.\",\n",
        "    \"Someone is on the couch reading.\"\n",
        "]\n",
        "\n",
        "# combine segments into one string\n",
        "inputs = [\n",
        "    f\"{h} [SEP] {p}\"\n",
        "    for (h, p) in zip(hypotheses, premises)\n",
        "]\n",
        "\n",
        "raw_logits = bert_classifier.predict(inputs, batch_size=2)\n",
        "probs = tf.nn.softmax(raw_logits, axis=-1).numpy()\n",
        "\n",
        "for i, combined in enumerate(inputs):\n",
        "    pred_idx = int(np.argmax(probs[i]))\n",
        "    print(f\"Input: {combined}\")\n",
        "    print(f\"Predicted label = {label_map[pred_idx]} (confidence {probs[i][pred_idx]:.2f})\")\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "id": "pM_4DIRo0nGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a9f8bce-2b76-4958-d06b-389b2fef3656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562ms/step\n",
            "Input: A child is eating ice cream near the park. [SEP] The kid is enjoying a sweet outside.\n",
            "Predicted label = neutral (confidence 0.49)\n",
            "------\n",
            "Input: A woman is sleeping on the couch. [SEP] Someone is on the couch reading.\n",
            "Predicted label = contradiction (confidence 0.90)\n",
            "------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}