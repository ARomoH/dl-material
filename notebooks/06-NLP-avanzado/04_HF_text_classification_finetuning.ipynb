{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1F6rZjsXF6Mn_BhVIyc6Z2ph13KjbajaY?usp=sharing)\n",
        "\n",
        "Source: https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb"
      ],
      "metadata": {
        "id": "JAa5C9e8GuBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning and feature extractor using HuggingFace models\n",
        "\n",
        "This notebook is an updated version of the notebook at the following address.\n",
        "\n",
        "https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb\n",
        "\n",
        "\n",
        "We will follow the steps of the notebook and make a possible alternative to the network-wide finetuning known as feature extractor or transfer-learning."
      ],
      "metadata": {
        "id": "zRaFCIMv67fh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install the most recent versions of ðŸ¤— Transformers and ðŸ¤— Datasets. We will also need `scipy` and `scikit-learn` for some of the metrics. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "! pip -q install transformers\n",
        "! pip -q install datasets evaluate\n",
        "! pip -q install scipy sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a text classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model to a text classification task of the [GLUE Benchmark](https://gluebenchmark.com/).\n",
        "\n",
        "\n",
        "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
        "\n",
        "- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.is a  dataset containing sentences labeled grammatically correct or not.\n",
        "- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
        "- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
        "- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
        "- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
        "- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
        "- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
        "- [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
        "- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)\n",
        "\n",
        "We will see how to easily load the dataset for each one of those tasks and use Keras to fine-tune a model on it. Each task is named by its acronym, with `mnli-mm` standing for the mismatched version of MNLI (a task with the same training set as `mnli` but different validation and test sets):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZbiBDuGIrId"
      },
      "outputs": [],
      "source": [
        "GLUE_TASKS = [\n",
        "    \"cola\",\n",
        "    \"mnli\",\n",
        "    \"mnli-mm\",\n",
        "    \"mrpc\",\n",
        "    \"qnli\",\n",
        "    \"qqp\",\n",
        "    \"rte\",\n",
        "    \"sst2\",\n",
        "    \"stsb\",\n",
        "    \"wnli\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "This notebook is built to run on any of the tasks in the list above, with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a classification head. Depending on your model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set these three parameters, then the rest of the notebook should run smoothly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "task = \"cola\"\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and the [ðŸ¤— Evaluate](https://github.com/huggingface/datasets) library to get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the `load_dataset` function from `datasets` and and the `load` function from `evaluate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from evaluate import load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "With the exception of `mnli-mm`, we can directly pass our task name to those functions. `load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_AY1ATSIrIq"
      },
      "outputs": [],
      "source": [
        "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
        "dataset = load_dataset(\"glue\", actual_task)\n",
        "metric = load(\"glue\", actual_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of `mnli`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWiVUF0jIrIv",
        "outputId": "a818d683-be9a-4f91-bd90-08babe3cec8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 8551\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 1043\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 1063\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "To access an actual element, you need to select a split first, then give an index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6HrpprwIrIz",
        "outputId": "f4889b12-8369-402b-ce2f-6779fd3aec4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
              " 'label': 1,\n",
              " 'idx': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(\n",
        "        dataset\n",
        "    ), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset) - 1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset) - 1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZy5tRB_IrI7",
        "outputId": "0ae8a52b-78ec-4e8e-c5cb-15c547883d30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jean reads never this newspaper.</td>\n",
              "      <td>unacceptable</td>\n",
              "      <td>1080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It would be inconvenience for you to leave so soon.</td>\n",
              "      <td>acceptable</td>\n",
              "      <td>5040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They believe there to be a dragon in the wood.</td>\n",
              "      <td>acceptable</td>\n",
              "      <td>4626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I ran a man who was old down.</td>\n",
              "      <td>unacceptable</td>\n",
              "      <td>1109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It tried to bother me that Chris lied.</td>\n",
              "      <td>unacceptable</td>\n",
              "      <td>4284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>To whom do you think it is the book that Mary gave?</td>\n",
              "      <td>unacceptable</td>\n",
              "      <td>5149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Why do you think he left?</td>\n",
              "      <td>acceptable</td>\n",
              "      <td>4843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The Emperor's every wish was immediately carried out.</td>\n",
              "      <td>acceptable</td>\n",
              "      <td>7895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>John rolled the ball from the tree to the bush.</td>\n",
              "      <td>acceptable</td>\n",
              "      <td>655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Him, they let him go yesterday.</td>\n",
              "      <td>acceptable</td>\n",
              "      <td>1853</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_random_elements(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnjDIuQ3IrI-"
      },
      "source": [
        "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o4rUteaIrI_",
        "outputId": "fd06d312-ae9d-44b0-a2c5-3519ca77c56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationModule(name: \"glue\", module_type: \"metric\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
              "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
              "Args:\n",
              "    predictions: list of predictions to score.\n",
              "        Each translation should be tokenized into a list of tokens.\n",
              "    references: list of lists of references for each translation.\n",
              "        Each reference should be tokenized into a list of tokens.\n",
              "Returns: depending on the GLUE subset, one or several of:\n",
              "    \"accuracy\": Accuracy\n",
              "    \"f1\": F1 score\n",
              "    \"pearson\": Pearson Correlation\n",
              "    \"spearmanr\": Spearman Correlation\n",
              "    \"matthews_correlation\": Matthew Correlation\n",
              "Examples:\n",
              "\n",
              "    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'accuracy': 1.0}\n",
              "\n",
              "    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'accuracy': 1.0, 'f1': 1.0}\n",
              "\n",
              "    >>> glue_metric = evaluate.load('glue', 'stsb')\n",
              "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
              "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
              "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
              "\n",
              "    >>> glue_metric = evaluate.load('glue', 'cola')\n",
              "    >>> references = [0, 1]\n",
              "    >>> predictions = [0, 1]\n",
              "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'matthews_correlation': 1.0}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "You can call its `compute` method with your predictions and labels directly and it will return a dictionary with the metric(s) value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XN1Rq0aIrJC",
        "outputId": "b7316552-ff2f-4967-908b-9b2d4093ce84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'matthews_correlation': -0.09416472308615167}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "fake_preds = np.random.randint(0, 2, size=(64,))\n",
        "fake_labels = np.random.randint(0, 2, size=(64,))\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOCrQwPoIrJG"
      },
      "source": [
        "Note that `load` has loaded the proper metric associated to your task, which is:\n",
        "\n",
        "- for CoLA: [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
        "- for MNLI (matched or mismatched): Accuracy\n",
        "- for MRPC: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for QNLI: Accuracy\n",
        "- for QQP: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for RTE: Accuracy\n",
        "- for SST-2: Accuracy\n",
        "- for STS-B: [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [Spearman's_Rank_Correlation_Coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
        "- for WNLI: Accuracy\n",
        "\n",
        "so the metric object only computes the one(s) needed for your task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on one sentence or a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5hBlsrHIrJL",
        "outputId": "6e106519-7851-444c-bbd6-181cc29c6efd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7592, 1010, 2023, 2003, 1037, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokenizer(\"Hello, this is a sentence!\", \"And this sentence goes with it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_0B1M2IrJM"
      },
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
        "\n",
        "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyGdtK9oIrJM"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbqtC4MrIrJO"
      },
      "source": [
        "We can double check it does work on our current dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19GG646uIrJO",
        "outputId": "08155b9a-3da3-467f-c9ae-797972c1f376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Our friends won't buy this analysis, let alone the next one we propose.\n"
          ]
        }
      ],
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "if sentence2_key is None:\n",
        "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
        "else:\n",
        "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
        "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "We can them write the function that will preprocess our samples. We just feed them to the `tokenizer` with the arguments `truncation=True` and `padding='longest`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model, and all inputs will be padded to the maximum input length to give us a single input array. A more performant method that reduces the number of padding tokens is to write a generator or `tf.data.Dataset` to only pad each *batch* to the maximum length in that batch, but most GLUE tasks are relatively quick on modern GPUs either way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    if sentence2_key is None:\n",
        "        return tokenizer(examples[sentence1_key], truncation=True)\n",
        "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b70jh26IrJS",
        "outputId": "12733aa3-c319-42a2-fa4c-a661d32489dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 2030, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 1996, 2062, 2057, 2817, 16025, 1010, 1996, 13675, 16103, 2121, 2027, 2131, 1012, 102], [101, 2154, 2011, 2154, 1996, 8866, 2024, 2893, 14163, 8024, 3771, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "preprocess_function(dataset[\"train\"][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "87fd36bd-2f4b-46dd-9096-7693008a6958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns added by tokenizer: ['attention_mask', 'input_ids']\n"
          ]
        }
      ],
      "source": [
        "pre_tokenizer_columns = set(dataset[\"train\"].features)\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "tokenizer_columns = list(set(encoded_dataset[\"train\"].features) - pre_tokenizer_columns)\n",
        "print(\"Columns added by tokenizer:\", tokenizer_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVQtegPCBxjj",
        "outputId": "6255e04b-6e96-4647-b7ab-8484325c80f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassLabel(names=['unacceptable', 'acceptable'], id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "encoded_dataset[\"train\"].features[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the `TFAutoModelForSequenceClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem and MNLI where we have 3 labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW",
        "outputId": "22517938-2828-47fe-d7ad-2207972e2c79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "num_labels = 3 if task.startswith(\"mnli\") else 1 if task == \"stsb\" else 2\n",
        "if task == \"stsb\":\n",
        "    num_labels = 1\n",
        "elif task.startswith(\"mnli\"):\n",
        "    num_labels = 3\n",
        "else:\n",
        "    num_labels = 2\n",
        "\n",
        "# This next little bit is optional, but will give us cleaner label outputs later\n",
        "# If you're using a task other than CoLA, you will probably need to change these\n",
        "# to match the label names for your task!\n",
        "id2label = {0: \"Invalid\", 1: \"Valid\"}\n",
        "label2id = {val: key for key, val in id2label.items()}\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elKHAVlYPsJD",
        "outputId": "c027490e-6a27-47fc-dc97-5c27e0e19a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMai  multiple                 66362880  \n",
            " nLayer)                                                         \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66,955,010\n",
            "Trainable params: 66,955,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ObtÃ©n los pesos de todas las capas del modelo\n",
        "bert_layer_weights = model.layers[0].get_weights()\n",
        "pc_layer_weights = model.layers[0].get_weights()\n",
        "\n",
        "print(\"Pesos de las primeras capas BERT:\")\n",
        "for layer_idx in range(10):\n",
        "    print(f\"Capa {layer_idx + 1}: {bert_layer_weights[layer_idx].shape}\")\n",
        "\n",
        "print(\"\\nPesos de las capas del pre-classifer (capas ocultas):\")\n",
        "for layer_idx in range(2):\n",
        "    print(f\"Capa {layer_idx + 1}: {pc_layer_weights[layer_idx].shape}\")\n",
        "\n",
        "# Puedes acceder a pesos especÃ­ficos de capas individuales si lo deseas\n",
        "# Por ejemplo, los pesos de la primera capa\n",
        "print()\n",
        "first_layer_weights = bert_layer_weights[0]\n",
        "first_layer_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAwWaTXdCqLP",
        "outputId": "38eee442-63ab-4eb8-9e45-4df44ff8f088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos de las primeras capas BERT:\n",
            "Capa 1: (30522, 768)\n",
            "Capa 2: (512, 768)\n",
            "Capa 3: (768,)\n",
            "Capa 4: (768,)\n",
            "Capa 5: (768, 768)\n",
            "Capa 6: (768,)\n",
            "Capa 7: (768, 768)\n",
            "Capa 8: (768,)\n",
            "Capa 9: (768, 768)\n",
            "Capa 10: (768,)\n",
            "\n",
            "Pesos de las capas del pre-classifer (capas ocultas):\n",
            "Capa 1: (30522, 768)\n",
            "Capa 2: (512, 768)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01664949, -0.06661227, -0.01632868, ..., -0.01999032,\n",
              "        -0.05139988, -0.0263568 ],\n",
              "       [-0.01319846, -0.06733431, -0.01605646, ..., -0.0226614 ,\n",
              "        -0.05537301, -0.02600443],\n",
              "       [-0.01759106, -0.07094341, -0.01443494, ..., -0.02457913,\n",
              "        -0.05956192, -0.0231829 ],\n",
              "       ...,\n",
              "       [-0.0231029 , -0.05878259, -0.01048967, ..., -0.01945743,\n",
              "        -0.02615411, -0.02118432],\n",
              "       [-0.0490171 , -0.05614787, -0.00465348, ..., -0.01065376,\n",
              "        -0.01797333, -0.02187675],\n",
              "       [-0.00646111, -0.0914881 , -0.00254872, ..., -0.01505679,\n",
              "        -0.05040044,  0.04597744]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU9oqLVRBxjk",
        "outputId": "f38a0449-549c-4aa8-a9fb-7b9a26bd53e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "validation_key = (\n",
        "    \"validation_mismatched\"\n",
        "    if task == \"mnli-mm\"\n",
        "    else \"validation_matched\"\n",
        "    if task == \"mnli\"\n",
        "    else \"validation\"\n",
        ")\n",
        "\n",
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    encoded_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "tf_validation_dataset = model.prepare_tf_dataset(\n",
        "    encoded_dataset[validation_key],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKGjwTwTBxjl"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "num_epochs = 1\n",
        "batches_per_epoch = len(encoded_dataset[\"train\"]) // batch_size\n",
        "total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps\n",
        ")\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsQVYnqGBxjm"
      },
      "source": [
        "With that out of the way, how do we actually use `KerasMetricCallback`? It's straightfoward: We simply define a function that computes metrics given a tuple of numpy arrays of predictions and labels, then we pass that, along with the validation set to compute metrics on, to the callback:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "\n",
        "def compute_metrics(eval_predictions):\n",
        "    predictions, labels = eval_predictions\n",
        "    if task != \"stsb\":\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "    else:\n",
        "        predictions = predictions[:, 0]\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn=compute_metrics, eval_dataset=tf_validation_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72130c2e-9359-40e7-f070-3708f8adabee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "534/534 [==============================] - 85s 109ms/step - loss: 0.5179 - val_loss: 0.4737 - matthews_correlation: 0.4587\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x781d0aaf8f40>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "callbacks = [metric_callback]\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_validation_dataset,\n",
        "    epochs=num_epochs,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobamos que efectivamente tanto las capas del clasificador com olos pesos del transformer han cambiado\n",
        "bert_layer_weights_ft = model.layers[0].get_weights()\n",
        "pc_layer_weights_ft = model.layers[1].get_weights()\n",
        "\n",
        "print(\"Pesos de las primeras capas BERT:\")\n",
        "for layer_idx in range(10):\n",
        "    print(f\"Capa {layer_idx + 1}: {bert_layer_weights_ft[layer_idx].shape}\")\n",
        "    same_weights = np.array_equal(bert_layer_weights_ft[layer_idx], bert_layer_weights[layer_idx])\n",
        "    print(f\"Capas iguales al checkpoint inicial: {same_weights}\")\n",
        "\n",
        "print(\"\\nPesos de las capas del pre-classifer (capas ocultas):\")\n",
        "for layer_idx in range(2):\n",
        "    print(f\"Capa {layer_idx + 1}: {pc_layer_weights_ft[layer_idx].shape}\")\n",
        "    same_weights = np.array_equal(pc_layer_weights_ft[layer_idx], pc_layer_weights[layer_idx])\n",
        "    print(f\"Capas iguales al checkpoint inicial: {same_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU2HzKstDnEk",
        "outputId": "7567e02f-31b8-4d84-fd02-7b3afae10df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos de las primeras capas BERT:\n",
            "Capa 1: (30522, 768)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 2: (512, 768)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 3: (768,)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 4: (768,)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 5: (768, 768)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 6: (768,)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 7: (768, 768)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 8: (768,)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 9: (768, 768)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 10: (768,)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "\n",
            "Pesos de las capas del pre-classifer (capas ocultas):\n",
            "Capa 1: (768, 768)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 2: (768,)\n",
            "Capas iguales al checkpoint inicial: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning only head layer\n",
        "\n",
        "In this case we just want to use the pre-trained embedding as an extractor feature and apply dense layers at the end of the network to suit our problem. Let's see how it would change our problem."
      ],
      "metadata": {
        "id": "o7ourHcvLWWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0DgVGWLLWIv",
        "outputId": "0e884fc5-f51f-41a6-99a9-f3dd216946c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Congelar pesos\n",
        "\n",
        "# en tensorflow\n",
        "model.distilbert.trainable = False\n",
        "model.summary()\n",
        "\n",
        "# # en pytorch\n",
        "# for param in model.distilbert.parameters():\n",
        "#     param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmTknkFVL85O",
        "outputId": "094e9498-51dc-4c42-fc3a-fa2ca821fb04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMai  multiple                 66362880  \n",
            " nLayer)                                                         \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66,955,010\n",
            "Trainable params: 592,130\n",
            "Non-trainable params: 66,362,880\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "num_epochs = 1\n",
        "batches_per_epoch = len(encoded_dataset[\"train\"]) // batch_size\n",
        "total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps\n",
        ")\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "mckXZfrmLpKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "callbacks = [metric_callback]\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_validation_dataset,\n",
        "    epochs=num_epochs,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6Gh3fNzL0Z_",
        "outputId": "cd75743f-7fb3-4a01-b9da-b9faac4fc2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "534/534 [==============================] - 30s 43ms/step - loss: 0.6059 - val_loss: 0.6112 - matthews_correlation: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x781cf56e3d60>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_layer_weights_ft = model.layers[0].get_weights()\n",
        "pc_layer_weights_ft = model.layers[1].get_weights()\n",
        "\n",
        "print(\"Pesos de las primeras capas BERT:\")\n",
        "for layer_idx in range(10):\n",
        "    print(f\"Capa {layer_idx + 1}: {bert_layer_weights_ft[layer_idx].shape}\")\n",
        "    same_weights = np.array_equal(bert_layer_weights_ft[layer_idx], bert_layer_weights[layer_idx])\n",
        "    print(f\"Capas iguales al checkpoint inicial: {same_weights}\")\n",
        "\n",
        "print(\"\\nPesos de las capas del pre-classifer (capas ocultas):\")\n",
        "for layer_idx in range(2):\n",
        "    print(f\"Capa {layer_idx + 1}: {pc_layer_weights_ft[layer_idx].shape}\")\n",
        "    same_weights = np.array_equal(pc_layer_weights_ft[layer_idx], pc_layer_weights[layer_idx])\n",
        "    print(f\"Capas iguales al checkpoint inicial: {same_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwsR3u1CL2FZ",
        "outputId": "9d2abae8-464a-4e25-da34-ba6d54dc81bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos de las primeras capas BERT:\n",
            "Capa 1: (30522, 768)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "Capa 2: (512, 768)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "Capa 3: (768,)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "Capa 4: (768,)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "Capa 5: (768, 768)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "Capa 6: (768,)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "Capa 7: (768, 768)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "Capa 8: (768,)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "Capa 9: (768, 768)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "Capa 10: (768,)\n",
            "Capas iguales al checkpoint inicial: True\n",
            "\n",
            "Pesos de las capas del pre-classifer (capas ocultas):\n",
            "Capa 1: (768, 768)\n",
            "Capas iguales al checkpoint inicial: False\n",
            "Capa 2: (768,)\n",
            "Capas iguales al checkpoint inicial: False\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}