{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "540c8652",
      "metadata": {
        "id": "540c8652"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1QFQQNPt83KujAKd1EmDmlfWE48hWWq11?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e220629",
      "metadata": {
        "id": "7e220629"
      },
      "source": [
        "# Semantic search\n",
        "\n",
        "In this notebook, we'll introduce semantic search and question-answering using [`sentence-transformers`](https://www.sbert.net/), a Python library for state-of-the-art sentence, text and image embeddings. These embeddings are useful for semantic similarity tasks, such as information retrieval and question-answering systems.\n",
        "\n",
        "<br>\n",
        "\n",
        "If you want to check all models availabe (in Models section):\n",
        "\n",
        "https://huggingface.co/sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85446b7",
      "metadata": {
        "id": "c85446b7"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fef53ec",
      "metadata": {
        "id": "8fef53ec"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import gzip\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ee4a0a8",
      "metadata": {
        "id": "9ee4a0a8"
      },
      "source": [
        "We'll use a pre-trained Sentence Transformer model to generate sentence embeddings. Many pre-trained models are available [here](https://www.sbert.net/docs/pretrained_models.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56081ef1",
      "metadata": {
        "id": "56081ef1"
      },
      "outputs": [],
      "source": [
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ab530d",
      "metadata": {
        "id": "f7ab530d"
      },
      "source": [
        "For our semantic search and question-answering task, we need a list of documents or paragraphs to search through for relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b473c53a",
      "metadata": {
        "id": "b473c53a"
      },
      "outputs": [],
      "source": [
        "# Sample paragraphs\n",
        "paragraphs = [\n",
        "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
        "    \"The Statue of Liberty is a colossal neoclassical sculpture on Liberty Island in New York Harbor within New York City, in the United States.\",\n",
        "    \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China.\",\n",
        "    \"The Colosseum, also known as the Flavian Amphitheatre, is an oval amphitheatre in the centre of the city of Rome, Italy.\",\n",
        "    \"The Taj Mahal is an ivory-white marble mausoleum on the southern bank of the river Yamuna in the Indian city of Agra.\"\n",
        "]\n",
        "\n",
        "paragraphs = np.array(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad2ca34",
      "metadata": {
        "id": "2ad2ca34"
      },
      "outputs": [],
      "source": [
        "# Generate embeddings for paragraphs\n",
        "corpus_embeddings = model.encode(paragraphs)\n",
        "print(corpus_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a5756f",
      "metadata": {
        "id": "87a5756f"
      },
      "source": [
        "Now, let's define a function to perform semantic search, given a query and a list of paragraph embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad6a9592",
      "metadata": {
        "id": "ad6a9592"
      },
      "outputs": [],
      "source": [
        "def semantic_search(query, model, corpus_embeddings, paragraphs, top_k=2):\n",
        "    query_embedding = model.encode([query])[0]\n",
        "    similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]\n",
        "    indexes = np.argpartition(similarities, -top_k)[-top_k:]\n",
        "    indexes = indexes[np.argsort(-similarities[indexes])]\n",
        "    print(f\"Input query: {query}\")\n",
        "    print()\n",
        "    for text, sim in zip(list(paragraphs[indexes]), similarities[indexes].tolist()):\n",
        "        print(f\"{sim:.3f}\\t{text}\")\n",
        "\n",
        "model_name = 'sentence-transformers/xlm-r-distilroberta-base-paraphrase-v1'\n",
        "model_2 = SentenceTransformer(model_name)\n",
        "semantic_search('Where is the Colosseum', model, corpus_embeddings, paragraphs, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6df5645",
      "metadata": {
        "id": "b6df5645"
      },
      "source": [
        "## Multilingual models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b0cded",
      "metadata": {
        "id": "93b0cded"
      },
      "outputs": [],
      "source": [
        "# lets try in other languages\n",
        "model_name = 'sentence-transformers/xlm-r-distilroberta-base-paraphrase-v1'\n",
        "# Generate embeddings for paragraphs\n",
        "corpus_embeddings = model_2.encode(paragraphs)\n",
        "print(corpus_embeddings.shape)\n",
        "model_2 = SentenceTransformer(model_name)\n",
        "\n",
        "semantic_search('¿Dónde está el Coliseo?', model_2, corpus_embeddings, paragraphs, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d55f723",
      "metadata": {
        "id": "5d55f723"
      },
      "source": [
        "We have multilinguals models available [here](https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea38223",
      "metadata": {
        "id": "5ea38223"
      },
      "outputs": [],
      "source": [
        "# we can use multilingual models\n",
        "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "multi_model = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7e03d2",
      "metadata": {
        "id": "6d7e03d2"
      },
      "outputs": [],
      "source": [
        "multi_corpus_embeddings = multi_model.encode(paragraphs)\n",
        "print(multi_corpus_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d89f20",
      "metadata": {
        "id": "a6d89f20"
      },
      "outputs": [],
      "source": [
        "semantic_search('¿Dónde está el Coliseo?', multi_model, multi_corpus_embeddings, paragraphs, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08858a4b",
      "metadata": {
        "id": "08858a4b"
      },
      "source": [
        "## Wikipedia semantic search\n",
        "\n",
        "As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
        "about 170k articles. We split these articles into paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15aef3ce",
      "metadata": {
        "id": "15aef3ce"
      },
      "outputs": [],
      "source": [
        "wikipedia_filepath = 'data/simplewiki-2020-11-01.jsonl.gz'\n",
        "\n",
        "if not os.path.exists(wikipedia_filepath):\n",
        "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
        "\n",
        "passages = []\n",
        "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        data = json.loads(line.strip())\n",
        "        for paragraph in data['paragraphs']:\n",
        "            # We encode the passages as [title, text]\n",
        "            passages.append(data['title']+':  '+ paragraph)\n",
        "\n",
        "# If you like, you can also limit the number of passages you want to use\n",
        "print(\"Passages:\", len(passages))\n",
        "print(passages[0])\n",
        "print(passages[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa111507",
      "metadata": {
        "id": "fa111507"
      },
      "outputs": [],
      "source": [
        "reduced_passages = np.array(passages[:5000])\n",
        "reduced_passages.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4cc331",
      "metadata": {
        "id": "5a4cc331"
      },
      "outputs": [],
      "source": [
        "corpus_embeddings = model.encode(reduced_passages, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d1260d",
      "metadata": {
        "id": "c7d1260d"
      },
      "outputs": [],
      "source": [
        "semantic_search('Best american actor', model, corpus_embeddings, reduced_passages, top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbad4f4a",
      "metadata": {
        "id": "fbad4f4a"
      },
      "outputs": [],
      "source": [
        "semantic_search('Number countries Europe', model, corpus_embeddings, reduced_passages, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb3ca16",
      "metadata": {
        "id": "8eb3ca16"
      },
      "source": [
        "## Find text duplicates\n",
        "\n",
        "Try to find duplicate or near-duplicate texts in a given corpus based on their semantic similarity using sentence-transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e63124d",
      "metadata": {
        "id": "2e63124d"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "    \"The weather today is sunny and warm.\",\n",
        "    \"Today's forecast calls for clear skies and pleasant temperatures.\",\n",
        "    \"I like pizza with extra cheese and pepperoni.\",\n",
        "    \"She enjoys salads with fresh vegetables and a light vinaigrette.\",\n",
        "    \"Cats are known for their independent and aloof nature.\",\n",
        "    \"Dogs are typically seen as loyal and affectionate companions.\",\n",
        "    \"Mountains are majestic and serene in the early morning light.\",\n",
        "    \"Cities are bustling and noisy at all hours of the day.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b500687",
      "metadata": {
        "id": "3b500687"
      },
      "outputs": [],
      "source": [
        "# Step 1: Initialize the SentenceTransformer model\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe93930",
      "metadata": {
        "id": "5fe93930"
      },
      "outputs": [],
      "source": [
        "# Step 2: Obtain corpus embeddings\n",
        "embeddings = model.encode(texts, convert_to_tensor=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce3fd65",
      "metadata": {
        "id": "fce3fd65"
      },
      "outputs": [],
      "source": [
        "# Step 3: Calculate similarity and find duplicates\n",
        "\n",
        "# TODO: Define similarity\n",
        "similarities = cosine_similarity(embeddings, embeddings)\n",
        "\n",
        "# TODO: Define a similarity threshold\n",
        "similarity_threshold = 0.9\n",
        "\n",
        "# TODO: Iterate over each pair of embeddings in the corpus\n",
        "# Calculate the cosine similarity between the embeddings\n",
        "# If the similarity is above the threshold, add the sentences to the duplicates list\n",
        "duplicates = []\n",
        "\n",
        "for i, emb1 in enumerate(embeddings):\n",
        "    for j, emb2 in enumerate(embeddings[i + 1:]):\n",
        "        if similarities[i][j] >= similarity_threshold:\n",
        "            duplicates.append((texts[i], texts[i + j + 1], similarities[i][j]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b17148d",
      "metadata": {
        "id": "8b17148d"
      },
      "outputs": [],
      "source": [
        "# Sort duplicates by similarity score in descending order\n",
        "duplicates.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "# Show the top 5 duplicates\n",
        "top_5_duplicates = duplicates[:5]\n",
        "\n",
        "# Print the top 5 duplicates\n",
        "for i, (text1, text2, similarity) in enumerate(top_5_duplicates, start=1):\n",
        "    print(f\"Top {i} Similarity Score: {similarity:.2f}\")\n",
        "    print(f\"Text 1: '{text1}'\")\n",
        "    print(f\"Text 2: '{text2}'\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results are no so good so we can finetune. We do not have so much data but we can see following notebook example to get an idea of how we could do it.\n",
        "\n",
        "https://huggingface.co/blog/how-to-train-sentence-transformers\n"
      ],
      "metadata": {
        "id": "x9DZjn_MMbdm"
      },
      "id": "x9DZjn_MMbdm"
    },
    {
      "cell_type": "markdown",
      "id": "9aadaa3a",
      "metadata": {
        "id": "9aadaa3a"
      },
      "source": [
        "# Clustering\n",
        "\n",
        "We can use BERTopic a clustering algorithm library that use sentence transformer model as baseline to create topics/clusters.\n",
        "\n",
        "BERTopic is a topic modeling technique that leverages 🤗 transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.\n",
        "\n",
        "- Documentation: https://maartengr.github.io/BERTopic/index.html\n",
        "- Notebook example: https://colab.research.google.com/#fileId=https%3A//huggingface.co/spaces/davanstrien/blog_notebooks/blob/main/BERTopic_hub_starter.ipynb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brSxVVPpHMMp"
      },
      "id": "brSxVVPpHMMp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}