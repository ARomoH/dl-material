{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1G5PJEFHEiYaJQ29pk0lfnTKk1yLxFBWT?usp=sharing)"
      ],
      "metadata": {
        "id": "zv31Oq7suQxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Language Models\n",
        "\n",
        "Causal Language Models (also called autoregressive or decorder models) are transformers designed to generate text sequentially from left to right, where each token can only attend to previous tokens.\n",
        "\n",
        "Here are some causal models that we can use from the TF/Keras Hub (2025/10 version)\n",
        "\n",
        "| Model | Size | Description |\n",
        "|-------|------|-------------|\n",
        "| bloom_560m_multi | 559.21M | 24-layer Bloom model with hidden dimension of 1024. Trained on 45 natural languages and 12 programming languages. |\n",
        "| bloomz_560m_multi | 559.21M | 24-layer Bloom model with hidden dimension of 1024. Finetuned on crosslingual task mixture (xP3) dataset. |\n",
        "| bloom_1.1b_multi | 1.07B | 24-layer Bloom model with hidden dimension of 1536. Trained on 45 natural languages and 12 programming languages. |\n",
        "| bloomz_1.1b_multi | 1.07B | 24-layer Bloom model with hidden dimension of 1536. Finetuned on crosslingual task mixture (xP3) dataset. |\n",
        "| bloom_1.7b_multi | 1.72B | 24-layer Bloom model with hidden dimension of 2048. Trained on 45 natural languages and 12 programming languages. |\n",
        "| bloomz_1.7b_multi | 1.72B | 24-layer Bloom model with hidden dimension of 2048. Finetuned on crosslingual task mixture (xP3) dataset. |\n",
        "| bloom_3b_multi | 3.00B | 30-layer Bloom model with hidden dimension of 2560. Trained on 45 natural languages and 12 programming languages. |\n",
        "| bloomz_3b_multi | 3.00B | 30-layer Bloom model with hidden dimension of 2560. Finetuned on crosslingual task mixture (xP3) dataset. |\n",
        "| falcon_refinedweb_1b_en | 1.31B | 24-layer Falcon model (Falcon with 1B parameters), trained on 350B tokens of RefinedWeb dataset. |\n",
        "| vault_gemma_1b_en | 1.04B | 1 billion parameter, 26-layer, VaultGemma model. |\n",
        "| gemma_2b_en | 2.51B | 2 billion parameter, 18-layer, base Gemma model. |\n",
        "| gemma_instruct_2b_en | 2.51B | 2 billion parameter, 18-layer, instruction tuned Gemma model. |\n",
        "| gemma_1.1_instruct_2b_en | 2.51B | 2 billion parameter, 18-layer, instruction tuned Gemma model. The 1.1 update improves model quality. |\n",
        "| code_gemma_1.1_2b_en | 2.51B | 2 billion parameter, 18-layer, CodeGemma model. This model has been trained on a fill-in-the-middle (FIM) task for code completion. The 1.1 update improves model quality. |\n",
        "| code_gemma_2b_en | 2.51B | 2 billion parameter, 18-layer, CodeGemma model. This model has been trained on a fill-in-the-middle (FIM) task for code completion. |\n",
        "| gemma2_2b_en | 2.61B | 2 billion parameter, 26-layer, base Gemma model. |\n",
        "| gemma2_instruct_2b_en | 2.61B | 2 billion parameter, 26-layer, instruction tuned Gemma model. |\n",
        "| shieldgemma_2b_en | 2.61B | 2 billion parameter, 26-layer, ShieldGemma model. |\n",
        "| gemma_7b_en | 8.54B | 7 billion parameter, 28-layer, base Gemma model. |\n",
        "| gemma_instruct_7b_en | 8.54B | 7 billion parameter, 28-layer, instruction tuned Gemma model. |\n",
        "| gemma_1.1_instruct_7b_en | 8.54B | 7 billion parameter, 28-layer, instruction tuned Gemma model. The 1.1 update improves model quality. |\n",
        "| code_gemma_7b_en | 8.54B | 7 billion parameter, 28-layer, CodeGemma model. This model has been trained on a fill-in-the-middle (FIM) task for code completion. |\n",
        "| code_gemma_instruct_7b_en | 8.54B | 7 billion parameter, 28-layer, instruction tuned CodeGemma model. This model has been trained for chat use cases related to code. |\n",
        "| code_gemma_1.1_instruct_7b_en | 8.54B | 7 billion parameter, 28-layer, instruction tuned CodeGemma model. This model has been trained for chat use cases related to code. The 1.1 update improves model quality. |\n",
        "| gemma2_9b_en | 9.24B | 9 billion parameter, 42-layer, base Gemma model. |\n",
        "| gemma2_instruct_9b_en | 9.24B | 9 billion parameter, 42-layer, instruction tuned Gemma model. |\n",
        "| shieldgemma_9b_en | 9.24B | 9 billion parameter, 42-layer, ShieldGemma model. |\n",
        "| gemma2_27b_en | 27.23B | 27 billion parameter, 42-layer, base Gemma model. |\n",
        "| gemma2_instruct_27b_en | 27.23B | 27 billion parameter, 42-layer, instruction tuned Gemma model. |\n",
        "| shieldgemma_27b_en | 27.23B | 27 billion parameter, 42-layer, ShieldGemma model. |\n",
        "| gemma3_270m | 268.10M | 270-million parameter (170m embedding, 100m transformer params) model, 18-layer, text-only designed for hyper-efficient AI, particularly for task-specific fine-tuning. |\n",
        "| gemma3_instruct_270m | 268.10M | 270-million parameter (170m embedding, 100m transformer params) model, 18-layer, text-only, instruction-tuned model designed for hyper-efficient AI, particularly for task-specific fine-tuning. |\n",
        "| gemma3_1b | 999.89M | 1 billion parameter, 26-layer, text-only pretrained Gemma3 model. |\n",
        "| gemma3_instruct_1b | 999.89M | 1 billion parameter, 26-layer, text-only instruction-tuned Gemma3 model. |\n",
        "| gemma3_4b_text | 3.88B | 4 billion parameter, 34-layer, text-only pretrained Gemma3 model. |\n",
        "| gemma3_instruct_4b_text | 3.88B | 4 billion parameter, 34-layer, text-only instruction-tuned Gemma3 model. |\n",
        "| gemma3_4b | 4.30B | 4 billion parameter, 34-layer, vision+text pretrained Gemma3 model. |\n",
        "| gemma3_instruct_4b | 4.30B | 4 billion parameter, 34-layer, vision+text instruction-tuned Gemma3 model. |\n",
        "| gemma3_12b_text | 11.77B | 12 billion parameter, 48-layer, text-only pretrained Gemma3 model. |\n",
        "| gemma3_instruct_12b_text | 11.77B | 12 billion parameter, 48-layer, text-only instruction-tuned Gemma3 model. |\n",
        "| gemma3_12b | 12.19B | 12 billion parameter, 48-layer, vision+text pretrained Gemma3 model. |\n",
        "| gemma3_instruct_12b | 12.19B | 12 billion parameter, 48-layer, vision+text instruction-tuned Gemma3 model. |\n",
        "| gemma3_27b_text | 27.01B | 27 billion parameter, 62-layer, text-only pretrained Gemma3 model. |\n",
        "| gemma3_instruct_27b_text | 27.01B | 27 billion parameter, 62-layer, text-only instruction-tuned Gemma3 model. |\n",
        "| gemma3_27b | 27.43B | 27 billion parameter, 62-layer, vision+text pretrained Gemma3 model. |\n",
        "| gemma3_instruct_27b | 27.43B | 27 billion parameter, 62-layer, vision+text instruction-tuned Gemma3 model. |\n",
        "| gpt2_base_en | 124.44M | 12-layer GPT-2 model where case is maintained. Trained on WebText. |\n",
        "| gpt2_base_en_cnn_dailymail | 124.44M | 12-layer GPT-2 model where case is maintained. Finetuned on the CNN/DailyMail summarization dataset. |\n",
        "| gpt2_medium_en | 354.82M | 24-layer GPT-2 model where case is maintained. Trained on WebText. |\n",
        "| gpt2_large_en | 774.03M | 36-layer GPT-2 model where case is maintained. Trained on WebText. |\n",
        "| gpt2_extra_large_en | 1.56B | 48-layer GPT-2 model where case is maintained. Trained on WebText. |\n",
        "| llama2_7b_en | 6.74B | 7 billion parameter, 32-layer, base LLaMA 2 model. |\n",
        "| llama2_instruct_7b_en | 6.74B | 7 billion parameter, 32-layer, instruction tuned LLaMA 2 model. |\n",
        "| vicuna_1.5_7b_en | 6.74B | 7 billion parameter, 32-layer, instruction tuned Vicuna v1.5 model. |\n",
        "| llama2_7b_en_int8 | 6.74B | 7 billion parameter, 32-layer, base LLaMA 2 model with activation and weights quantized to int8. |\n",
        "| llama2_instruct_7b_en_int8 | 6.74B | 7 billion parameter, 32-layer, instruction tuned LLaMA 2 model with activation and weights quantized to int8. |\n",
        "| llama3.2_1b | 1.50B | 1 billion parameter, 16-layer, based LLaMA 3.2 model. |\n",
        "| llama3.2_instruct_1b | 1.50B | 1 billion parameter, 16-layer, instruction tuned LLaMA 3.2. |\n",
        "| llama3.2_guard_1b | 1.50B | 1 billion parameter, 16-layer, based LLaMA 3.2 model fine-tuned for consent safety classification. |\n",
        "| llama3.2_3b | 3.61B | 3 billion parameter, 26-layer, based LLaMA 3.2 model. |\n",
        "| llama3.2_instruct_28 b_en | 3.61B | 3 billion parameter, 28-layer, instruction tuned LLaMA 3.2. |\n",
        "| llama3_8b_en | 8.03B | 8 billion parameter, 32-layer, base LLaMA 3 model. |\n",
        "| llama3_instruct_8b_en | 8.03B | 8 billion parameter, 32-layer, instruction tuned LLaMA 3 model. |\n",
        "| llama3.1_8b | 8.03B | 8 billion parameter, 32-layer, based LLaMA 3.1 model. |\n",
        "| llama3.1_instruct_8b | 8.03B | 8 billion parameter, 32-layer, instruction tuned LLaMA 3.1. |\n",
        "| llama3.1_guard_8b | 8.03B | 8 billion parameter, 32-layer, LLaMA 3.1 fine-tuned for consent safety classification. |\n",
        "| llama3_8b_en_int8 | 8.03B | 8 billion parameter, 32-layer, base LLaMA 3 model with activation and weights quantized to int8. |\n",
        "| mistral_7b_en | 7.24B | Mistral 7B base model. |\n",
        "| mistral_instruct_7b_en | 7.24B | Mistral 7B instruct model. |\n",
        "| mistral_0.2_instruct_7b_en | 7.24B | Mistral 7B instruct version 0.2 model. |\n",
        "| mistral_0.3_7b_en | 7.25B | Mistral 7B base version 0.3 model. |\n",
        "| mistral_0.3_instruct_7b_en | 7.25B | Mistral 7B instruct version 0.3 model. |\n",
        "| mixtral_8_7b_en | 46.70B | 32-layer Mixtral MoE model with 7 billion active parameters and 8 experts per MoE layer. |\n",
        "| mixtral_8_instruct_7b_en | 46.70B | Instruction fine-tuned 32-layer Mixtral MoE model with 7 billion active parameters and 8 experts per MoE layer. |\n",
        "| moonshine_tiny_en | 27.09M | Moonshine tiny model for English speech recognition. Developed by Useful Sensors for real-time transcription. |\n",
        "| moonshine_base_en | 61.51M | Moonshine base model for English speech recognition. Developed by Useful Sensors for real-time transcription. |\n",
        "| opt_125m_en | 125.24M | 12-layer OPT model where case is maintained. Trained on BookCorpus, CommonCrawl, Pile, and PushShift.io corpora. |\n",
        "| opt_1.3b_en | 1.32B | 24-layer OPT model where case is maintained. Trained on BookCorpus, CommonCrawl, Pile, and PushShift.io corpora. |\n",
        "| opt_2.7b_en | 2.70B | 32-layer OPT model where case is maintained. Trained on BookCorpus, CommonCrawl, Pile, and PushShift.io corpora. |\n",
        "| opt_6.7b_en | 6.70B | 32-layer OPT model where case is maintained. Trained on BookCorpus, CommonCrawl, Pile, and PushShift.io corpora. |\n",
        "| pali_gemma_3b_mix_224 | 2.92B | image size 224, mix fine tuned, text sequence length is 256. |\n",
        "| pali_gemma_3b_224 | 2.92B | image size 224, pre-trained, text sequence length is 128. |\n",
        "| pali_gemma_3b_mix_448 | 2.92B | image size 448, mix fine tuned, text sequence length is 512. |\n",
        "| pali_gemma_3b_448 | 2.92B | image size 448, pre-trained, text sequence length is 512. |\n",
        "| pali_gemma_3b_896 | 2.93B | image size 896, pre-trained, text sequence length is 512. |\n",
        "| pali_gemma2_mix_3b_224 | 3.03B | 3 billion parameter, image size 224, 27-layer for SigLIP-So400m vision encoder and 26-layer Gemma2 2B language model. This model has been fine-tuned on a wide range of vision-language tasks and domains. |\n",
        "| pali_gemma2_pt_3b_224 | 3.03B | 3 billion parameter, image size 224, 27-layer for SigLIP-So400m vision encoder and 26-layer Gemma2 2B language model. This model has been pre-trained on a mixture of datasets. |\n",
        "| pali_gemma_2_ft_docci_3b_448 | 3.03B | 3 billion parameter, image size 448, 27-layer for SigLIP-So400m vision encoder and 26-layer Gemma2 2B language model. This model has been fine-tuned on the DOCCI dataset for improved descriptions with fine-grained details. |\n",
        "| pali_gemma2_mix_3b_448 | 3.03B | 3 billion parameter, image size 448, 27-layer for SigLIP-So400m vision encoder and 26-layer Gemma2 2B language model. This model has been fine-tuned on a wide range of vision-language tasks and domains. |\n",
        "| pali_gemma2_pt_3b_448 | 3.03B | 3 billion parameter, image size 448, 27-layer for SigLIP-So400m vision encoder and 26-layer Gemma2 2B language model. This model has been pre-trained on a mixture of datasets. |\n",
        "| pali_gemma2_pt_3b_896 | 3.04B | 3 billion parameter, image size 896, 27-layer for SigLIP-So400m vision encoder and 26-layer Gemma2 2B language model. This model has been pre-trained on a mixture of datasets. |\n",
        "| pali_gemma2_mix_10b_224 | 9.66B | 10 billion parameter, image size 224, 27-layer for SigLIP-So400m vision encoder and 42-layer Gemma2 9B language model. This model has been fine-tuned on a wide range of vision-language tasks and domains. |\n",
        "| pali_gemma2_pt_10b_224 | 9.66B | 10 billion parameter, image size 224, 27-layer for SigLIP-So400m vision encoder and 42-layer Gemma2 9B language model. This model has been pre-trained on a mixture of datasets. |\n",
        "| pali_gemma2_ft_docci_10b_448 | 9.66B | 10 billion parameter, 27-layer for SigLIP-So400m vision encoder and 42-layer Gemma2 9B language model. This model has been fine-tuned on the DOCCI dataset for improved descriptions with fine-grained details. |\n",
        "| pali_gemma2_mix_10b_448 | 9.66B | 10 billion parameter, image size 448, 27-layer for SigLIP-So400m vision encoder and 42-layer Gemma2 9B language model. This model has been fine-tuned on a wide range of vision-language tasks and domains. |\n",
        "| pali_gemma2_pt_10b_448 | 9.66B | 10 billion parameter, image size 448, 27-layer for SigLIP-So400m vision encoder and 42-layer Gemma2 9B language model. This model has been pre-trained on a mixture of datasets. |\n",
        "| pali_gemma2_pt_10b_896 | 9.67B | 10 billion parameter, image size 896, 27-layer for SigLIP-So400m vision encoder and 42-layer Gemma2 9B language model. This model has been pre-trained on a mixture of datasets. |\n",
        "| phi3_mini_4k_instruct_en | 3.82B | 3.8 billion parameters, 32 layers, 4k context length, Phi-3 model. The model was trained using the Phi-3 datasets which include both synthetic data and filtered publicly available website data, with an emphasis on high-quality and reasoning-dense properties. |\n",
        "| phi3_mini_128k_instruct_en | 3.82B | 3.8 billion parameters, 32 layers, 128k context length, Phi-3 model. The model was trained using the Phi-3 datasets. |\n",
        "| qwen2.5_0.5b_en | 494.03M | 24-layer Qwen model with 0.5 billion parameters. |\n",
        "| qwen2.5_instruct_0.5b_en | 494.03M | Instruction fine-tuned 24-layer Qwen model with 0.5 billion parameters. |\n",
        "| qwen2.5_3b_en | 3.09B | 36-layer Qwen model with 3.1 billion parameters. |\n",
        "| qwen2.5_7b_en | 6.99B | 48-layer Qwen model with 7 billion parameters. |\n",
        "| qwen2.5_instruct_32b_en | 32.76B | Instruction fine-tuned 64-layer Qwen model with 32 billion parameters. |\n",
        "| qwen2.5_instruct_72b_en | 72.71B | Instruction fine-tuned 80-layer Qwen model with 72 billion parameters. |\n",
        "| qwen3_0.6b_en | 596.05M | 28-layer Qwen3 model with 596 M parameters, optimized for efficiency and fast inference on resource-constrained devices. |\n",
        "| qwen3_1.7b_en | 1.72B | 28-layer Qwen3 model with 1.72 billion parameters, offering a good balance between performance and resource usage. |\n",
        "| qwen3_4b_en | 4.02B | 36-layer Qwen3 model with 4.02 billion parameters, offering improved reasoning capabilities and better performance than smaller variants. |\n",
        "| qwen3_8b_en | 8.19B | 36-layer Qwen3 model with 8.19 billion parameters, featuring enhanced reasoning, coding, and instruction-following capabilities. |\n",
        "| qwen3_14b_en | 14.77B | 40-layer Qwen3 model with 14.77 billion parameters, featuring advanced reasoning, coding, and multilingual capabilities. |\n",
        "| qwen3_32b_en | 32.76B | 64-layer Qwen3 model with 32.76 billion parameters, featuring state-of-the-art performance across reasoning, coding, and general language tasks. |\n",
        "| qwen1.5_moe_2.7b_en | 14.32B | 24-layer Qwen MoE model with 2.7 billion active parameters and 8 experts per MoE layer. |\n"
      ],
      "metadata": {
        "id": "rJxCPFO1nFNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GBYflaF3m-Qy"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_hub\n",
        "from keras_hub import samplers\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_name = \"gpt2_base_en\"\n",
        "causal_lm = keras_hub.models.CausalLM.from_preset(pretrained_model_name)\n",
        "causal_lm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "zWFZ2LceoHRc",
        "outputId": "991ae3a7-62d6-4f70-fafc-89d67beb41ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/3/download/config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 431/431 [00:00<00:00, 851kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/3/download/model.weights.h5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 475M/475M [00:05<00:00, 90.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/3/download/tokenizer.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 618/618 [00:00<00:00, 1.27MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/3/download/assets/tokenizer/vocabulary.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 0.99M/0.99M [00:00<00:00, 5.05MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/3/download/assets/tokenizer/merges.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 446k/446k [00:00<00:00, 2.98MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                                │                       Vocab size: \u001b[38;5;34m50,257\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                                │                       Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │     \u001b[38;5;34m124,439,808\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)       │      \u001b[38;5;34m38,597,376\u001b[0m │ gpt2_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │ gpt2_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text\n",
        "\n",
        "Even when our model is already trained, we have several techniques to generate text that could affect to the quality of the generations."
      ],
      "metadata": {
        "id": "eeel3nAjwjKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time in a distant galaxy, the explorers discovered\"\n",
        "\n",
        "def generate_text(causal_lm, prompt, max_length=50, temperature=1.0, top_k=None, top_p=None):\n",
        "    # Decide which sampler to use\n",
        "    if top_p is not None:\n",
        "        sampler = samplers.TopPSampler(p=top_p, temperature=temperature)\n",
        "    elif top_k is not None:\n",
        "        sampler = samplers.TopKSampler(k=top_k, temperature=temperature)\n",
        "    else:\n",
        "        sampler = samplers.GreedySampler(temperature=temperature)\n",
        "\n",
        "    # Compile model with the sampler\n",
        "    causal_lm.compile(sampler=sampler)\n",
        "\n",
        "    # Generate text\n",
        "    output = causal_lm.generate(prompt, max_length=max_length, strip_prompt=True)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "CPChfYfuoP1M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature experiment\n",
        "\n",
        "Controls the randomness of predictions by scaling the probability distribution before sampling."
      ],
      "metadata": {
        "id": "ZhiuemPmo7Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Temperature sweep ===\")\n",
        "for temp in [0.2, 0.5, 1.0, 1.5]:\n",
        "    text = generate_text(causal_lm, prompt, temperature=temp, top_k=50, top_p=None)\n",
        "    print(f\"Temp={temp} → {text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF_SbaNEo5a1",
        "outputId": "f7756ba1-eaec-47b3-de20-ff381760d6dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Temperature sweep ===\n",
            "Temp=0.2 →  a planet called Teth. The planet was inhabited by a race of sentient beings called the Tethians. The Tethians were the descendants of the Tethians who had been\n",
            "\n",
            "Temp=0.5 →  a strange planet. The planet was a planet in the midst of an interstellar war. The invaders had discovered a planet with a unique technology, and sought to destroy it. The explorers were\n",
            "\n",
            "Temp=1.0 →  a group of alien races, of unknown origin. But how did they get here? The story of this enigmatic civilization turns to the face of the unknown, a tale of betrayal, deception\n",
            "\n",
            "Temp=1.5 →  hundreds of small islands out-world. Today they're looking for what life on these distant islands could look like.\n",
            "\n",
            "\n",
            "Inhabited by space colony types known as HOD,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top k experiment\n",
        "\n",
        "Only consider the top k most likely tokens for sampling, then redistribute probability among them."
      ],
      "metadata": {
        "id": "ObSqJAclxPwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Top-k sweep ===\")\n",
        "for k in [1, 10, 50, 200]:\n",
        "    text = generate_text(causal_lm, prompt, top_k=k, top_p=None)\n",
        "    print(f\"Top_k={k} → {text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObbR1ZTFpBQR",
        "outputId": "9f922982-8713-4b9a-8346-4637dca66cfe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Top-k sweep ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <bound method GPT2CausalLM.generate_step of <GPT2CausalLM name=gpt2_causal_lm, built=True>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top_k=1 →  a planet called the \"Planet of the Gods.\" The planet was a planet of great power and great wealth. The planet was a planet of great wealth and great power. The planet was\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <bound method GPT2CausalLM.generate_step of <GPT2CausalLM name=gpt2_causal_lm, built=True>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top_k=10 →  the planet of the future. The young, beautiful world of Kain and its inhabitants are a paradise to be seen by the young explorers. The explorers have come to realize they have a\n",
            "\n",
            "Top_k=50 →  hundreds of billions of years ago that Mars wasn't built in 20,000 years.\n",
            "\n",
            "Today, even if our planet, our home planet, is 5,000 times more huge\n",
            "\n",
            "Top_k=200 →  a civilization inhabited on a race of moons. Now the mighty civilizations upon the moon are all peaceful. To face off against what could be considered conquest from this alien race, the Orion System\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top p experiment\n",
        "\n",
        "Keep the smallest set of tokens whose cumulative probability ≥ p, then sample from that set."
      ],
      "metadata": {
        "id": "q-QeSr9wxVcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Top-p sweep ===\")\n",
        "for p in [0.3, 0.6, 0.9, 0.99]:\n",
        "    text = generate_text(causal_lm, prompt, temperature=1.0, top_k=None, top_p=p)\n",
        "    print(f\"Top_p={p} → {text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8jc3eWup8Sq",
        "outputId": "a1c2121a-47f4-4c15-fc36-f1f106fdd5db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Top-p sweep ===\n",
            "Top_p=0.3 →  a strange planet. The inhabitants of the planet were determined to discover the secrets of the planet. The planet was called Zilith, and it was a very beautiful planet. It was\n",
            "\n",
            "Top_p=0.6 →  a world of extraordinary life, and its people were a force to be reckoned with. But a small group of brave and clever men led by a mysterious figure came into contact with the larger\n",
            "\n",
            "Top_p=0.9 →  a fascinating and absolutely sublime location on Mars where the time has come to prevent humanity from stopping their civilization from having a nuclear meltdown that will ultimately destroy mankind. Even though this disaster, the\n",
            "\n",
            "Top_p=0.99 →  a world that was traveling through time. And yet – dark for a race with no clear understanding of time – it went awry.\n",
            "\n",
            "The halves of the largest of the stars\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next steps**\n",
        "\n",
        "To avoid toxic language, control certain topics that we should no answer, include certain words, we can improve the `generate_text` function"
      ],
      "metadata": {
        "id": "1Xtcw7nAsfDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune causal language model\n",
        "\n",
        "source: https://keras.io/examples/generative/gpt2_text_generation_with_keras_hub/\n",
        "\n",
        "loss function: https://keras.io/api/optimizers/learning_rate_schedules/polynomial_decay/"
      ],
      "metadata": {
        "id": "hgbc47ugtEgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!# Load chinese poetry dataset.\n",
        "!git clone https://github.com/chinese-poetry/chinese-poetry.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBkon9fqp9qQ",
        "outputId": "e0d7b3fe-293e-47ca-df97-ed9f9dc223d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chinese-poetry'...\n",
            "remote: Enumerating objects: 7341, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 7341 (delta 7), reused 3 (delta 3), pack-reused 7330 (from 2)\u001b[K\n",
            "Receiving objects: 100% (7341/7341), 236.99 MiB | 35.06 MiB/s, done.\n",
            "Resolving deltas: 100% (5012/5012), done.\n",
            "Updating files: 100% (2285/2285), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "poem_collection = []\n",
        "for file in os.listdir(\"chinese-poetry/全唐诗\"):\n",
        "    if \".json\" not in file or \"poet\" not in file:\n",
        "        continue\n",
        "    full_filename = \"%s/%s\" % (\"chinese-poetry/全唐诗\", file)\n",
        "    with open(full_filename, \"r\") as f:\n",
        "        content = json.load(f)\n",
        "        poem_collection.extend(content)\n",
        "\n",
        "paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]"
      ],
      "metadata": {
        "id": "SzL9bkl6tJSQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paragraphs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOEtteVUtNEd",
        "outputId": "4fcc50ee-ab3a-4f4a-d3f8-2ec7f54d51ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "樂全老子如星日，真一仙人似鳳鸞。早歲光明均照耀，異時文彩避高寒。低回氣類追千劫，邂逅風流得二難。環堵蕭然清徹骨，却疑深雪卧袁安。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(paragraphs)\n",
        "    .batch(16)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Running through the whole dataset takes long, only take `500` and run 1\n",
        "# epochs for demo purposes.\n",
        "train_ds = train_ds.take(500)\n",
        "num_epochs = 1\n",
        "\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-4,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "d3KHIdgQtOOE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "causal_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "causal_lm.fit(train_ds, epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UleVlgtguH9z",
        "outputId": "58011ce8-7ea1-4be3-ddde-87c027fc99f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 356ms/step - accuracy: 0.2486 - loss: 0.4986\n",
            "CPU times: user 3min 38s, sys: 6.81 s, total: 3min 45s\n",
            "Wall time: 4min 27s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fe5270d5970>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate_text(causal_lm, \"昨夜雨疏风骤\", top_k=None, top_p=0.3)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovLGiuxNtRTG",
        "outputId": "93314c02-a9e4-471e-cf57-01500b29db6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "，暮推林林書爲。江南曾翁\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DaGDdz65uKkA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}