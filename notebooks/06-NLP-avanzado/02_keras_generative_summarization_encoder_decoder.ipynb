{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ApzOccU5rfx4zvh_irK8k82cxP_EurjH?usp=sharing)"
      ],
      "metadata": {
        "id": "AT6rWHj-99Pz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6FayKvE6KeB"
      },
      "source": [
        "# Generative summarization with enconder-decoder model\n",
        "\n",
        "reference: https://keras.io/examples/nlp/abstractive_summarization_with_bart/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq_i21I06KeD"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "[Bidirectional Autoregressive Transformer (BART)](https://arxiv.org/abs/1910.13461)\n",
        "is a Transformer-based encoder-decoder model, often used for\n",
        "sequence-to-sequence tasks like summarization and neural machine translation.\n",
        "BART is pre-trained in a self-supervised fashion on a large text corpus.\n",
        "\n",
        "During\n",
        "pre-training, the text is corrupted and BART is trained to reconstruct the\n",
        "original text (hence called a \"denoising autoencoder\"). Some pre-training tasks\n",
        "include token masking, token deletion, sentence permutation (shuffle sentences\n",
        "and train BART to fix the order), etc.\n",
        "\n",
        "Review all potential architectures that you can use in Keras:\n",
        "- https://keras.io/keras_hub/api/models/\n",
        "\n",
        "Check more model checkpoints in the hub:\n",
        "- https://www.kaggle.com/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSYl0H1b6KeE"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before we start implementing the pipeline, let's install and import all the\n",
        "libraries we need. We'll be using the KerasHub library. We will also need a\n",
        "couple of utility libraries.\n",
        "\n",
        "- **py7zr** is a library and utility to support 7zip archive compression, decompression, encryption and decryption written by Python programming language.\n",
        "- **keras-hub**: download pretrained models from hub (https://www.kaggle.com/models/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dge556N56KeE"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/keras-team/keras-hub.git py7zr -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QapmQhP6KeF"
      },
      "source": [
        "Import all necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6rhTfBz6KeF"
      },
      "outputs": [],
      "source": [
        "import py7zr\n",
        "import time\n",
        "\n",
        "import keras_hub\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYc6ApRR6KeG"
      },
      "source": [
        "Let's also define our hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP7dUCkX6KeG"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_BATCHES = 600\n",
        "EPOCHS = 1\n",
        "MAX_ENCODER_SEQUENCE_LENGTH = 512\n",
        "MAX_DECODER_SEQUENCE_LENGTH = 128\n",
        "MAX_GENERATION_LENGTH = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfb1wc336KeG"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Let's load the [SAMSum dataset](https://arxiv.org/abs/1911.12237). This dataset\n",
        "contains around 15,000 pairs of conversations/dialogues and summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EJfIYrx6KeG"
      },
      "outputs": [],
      "source": [
        "# Download the dataset manually (only once)\n",
        "filename = keras.utils.get_file(\n",
        "    \"corpus.7z\",\n",
        "    origin=\"https://arxiv.org/src/1911.12237v2/anc/corpus.7z\",\n",
        ")\n",
        "\n",
        "# Extract it\n",
        "with py7zr.SevenZipFile(filename, mode=\"r\") as z:\n",
        "    z.extractall(path=\"/root/tensorflow_datasets/downloads/manual\")\n",
        "\n",
        "# Now load via TFDS\n",
        "samsum_ds = tfds.load(\"samsum\", split=\"train\", as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDOgtp8P6KeG"
      },
      "source": [
        "The dataset has two fields: `dialogue` and `summary`. Let's see a sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OjQvg5j6KeG"
      },
      "outputs": [],
      "source": [
        "for dialogue, summary in samsum_ds:\n",
        "    print(dialogue.numpy())\n",
        "    print(summary.numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwa3cfhP6KeG"
      },
      "source": [
        " We will, therefore, change\n",
        "the format of the dataset to a dictionary having two keys: `\"encoder_text\"` and\n",
        "`\"decoder_text\"`.\n",
        "\n",
        "This is how [`keras_hub.models.BartSeq2SeqLMPreprocessor`](https://keras.io/keras_hub/api/models/bart/bart_seq_2_seq_lm_preprocessor/)\n",
        "expects the input format to be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59MwFnp06KeH"
      },
      "outputs": [],
      "source": [
        "train_ds = (\n",
        "    samsum_ds.map(\n",
        "        lambda dialogue, summary: {\"encoder_text\": dialogue, \"decoder_text\": summary}\n",
        "    )\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        ")\n",
        "train_ds = train_ds.take(NUM_BATCHES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVMWN9F56KeH"
      },
      "source": [
        "## Fine-tune BART\n",
        "\n",
        "Let's load the model and preprocessor first. We use sequence lengths of 512\n",
        "and 128 for the encoder and decoder, respectively, instead of 1024 (which is the\n",
        "default sequence length). This will allow us to run this example quickly\n",
        "on Colab.\n",
        "\n",
        "- the preprocessor is attached to the model.\n",
        "- the preprocessor tokenizes the encoder text\n",
        "and the decoder text, adds special tokens and pads them. To generate labels\n",
        "- for auto-regressive training, the preprocessor shifts the decoder text one\n",
        "position to the right to predict the next token.\n",
        "\n",
        "https://www.kaggle.com/models/keras/bart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zjl4Pg2f6KeH"
      },
      "outputs": [],
      "source": [
        "preprocessor = keras_hub.models.BartSeq2SeqLMPreprocessor.from_preset(\n",
        "    \"bart_base_en\",\n",
        "    encoder_sequence_length=MAX_ENCODER_SEQUENCE_LENGTH,\n",
        "    decoder_sequence_length=MAX_DECODER_SEQUENCE_LENGTH,\n",
        ")\n",
        "bart_lm = keras_hub.models.BartSeq2SeqLM.from_preset(\n",
        "    \"bart_base_en\", preprocessor=preprocessor\n",
        ")\n",
        "\n",
        "bart_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h479W2K16KeH"
      },
      "source": [
        "Define the optimizer and loss. We use the Adam optimizer with a linearly\n",
        "decaying learning rate. Compile the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUIiZXQt6KeH"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    epsilon=1e-6,         # added for numerical stability (paper)\n",
        "    global_clipnorm=1.0,  # gradient clipping.\n",
        ")\n",
        "\n",
        "# Exclude layernorm and bias terms from weight decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
        "\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "bart_lm.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88zrczM-6KeH"
      },
      "source": [
        "Let's train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjzDGvBp6KeH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "bart_lm.fit(train_ds, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9xhCXBs6KeH"
      },
      "source": [
        "## Generate summaries and evaluate them!\n",
        "\n",
        "Now that the model has been trained, let's get to the fun part - actually\n",
        "generating summaries! Let's pick the first 100 samples from the validation set\n",
        "and generate summaries for them. We will use the default decoding strategy, i.e.,\n",
        "greedy search.\n",
        "\n",
        "Generation in KerasHub is highly optimized. It is backed by the power of XLA.\n",
        "Secondly, key/value tensors in the self-attention layer and cross-attention layer\n",
        "in the decoder are cached to avoid recomputation at every timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkslYBCH6KeH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_text(model, input_text, max_length=200, print_time_taken=False):\n",
        "    start = time.time()\n",
        "    output = model.generate(input_text, max_length=max_length)\n",
        "    end = time.time()\n",
        "    print(f\"Total Time Elapsed: {end - start:.2f}s\")\n",
        "    return output\n",
        "\n",
        "\n",
        "# Load the dataset.\n",
        "val_ds = tfds.load(\"samsum\", split=\"validation\", as_supervised=True)\n",
        "val_ds = val_ds.take(100)\n",
        "\n",
        "dialogues = []\n",
        "ground_truth_summaries = []\n",
        "for dialogue, summary in val_ds:\n",
        "    dialogues.append(dialogue.numpy())\n",
        "    ground_truth_summaries.append(summary.numpy())\n",
        "\n",
        "# Let's make a dummy call - the first call to XLA generally takes a bit longer.\n",
        "_ = generate_text(bart_lm, \"sample text\", max_length=MAX_GENERATION_LENGTH)\n",
        "\n",
        "# Generate summaries.\n",
        "generated_summaries = generate_text(\n",
        "    bart_lm,\n",
        "    val_ds.map(lambda dialogue, _: dialogue).batch(8),\n",
        "    max_length=MAX_GENERATION_LENGTH,\n",
        "    print_time_taken=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iyLBhup6KeH"
      },
      "source": [
        "Let's see some of the summaries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dialogues)"
      ],
      "metadata": {
        "id": "T7rBd22YJoNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFSy3F9Y6KeH"
      },
      "outputs": [],
      "source": [
        "for dialogue, generated_summary, ground_truth_summary in zip(\n",
        "    dialogues[3:8], generated_summaries[3:8], ground_truth_summaries[3:8]\n",
        "):\n",
        "    print(\"Dialogue:\", dialogue)\n",
        "    print(\"Generated Summary:\", generated_summary)\n",
        "    print(\"Ground Truth Summary:\", ground_truth_summary)\n",
        "    print(\"=============================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlYdNu-e6KeH"
      },
      "source": [
        "The generated summaries look awesome! Not bad for a model trained only for 1\n",
        "epoch and on 5000 examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NAjUx-sCK-uI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}