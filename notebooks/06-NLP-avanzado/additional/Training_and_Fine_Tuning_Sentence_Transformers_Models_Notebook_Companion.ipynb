{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Fine-Tune Sentence Transformers Models - Notebook Companion"
      ],
      "metadata": {
        "id": "I729oEbmU1rv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWjnEb3_XT0Y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Sentence Transformers models work\n"
      ],
      "metadata": {
        "id": "P2CPLypZUwTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "## Step 1: use an existing language model\n",
        "word_embedding_model = models.Transformer('distilroberta-base')\n",
        "\n",
        "## Step 2: use a pool function over the token embeddings\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "\n",
        "## Join steps 1 and 2 using the modules argument\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "metadata": {
        "id": "pixb3XSjXaqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to prepare your dataset for training a Sentence Transformers model\n"
      ],
      "metadata": {
        "id": "HYB2Vtvga1Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "1iMXl5_ka3Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"embedding-data/QQP_triplets\"\n",
        "# dataset_id = \"embedding-data/sentence-compression\"\n",
        "\n",
        "dataset = load_dataset(dataset_id)"
      ],
      "metadata": {
        "id": "max24luaa5OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"- The {dataset_id} dataset has {dataset['train'].num_rows} examples.\")\n",
        "print(f\"- Each example is a {type(dataset['train'][0])} with a {type(dataset['train'][0]['set'])} as value.\")\n",
        "print(f\"- Examples look like this: {dataset['train'][0]}\")"
      ],
      "metadata": {
        "id": "2VUd11y8bP_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the examples into `InputExample`s. It might around 10 seconds in Google Colab."
      ],
      "metadata": {
        "id": "-O2o_92kuzCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "\n",
        "train_examples = []\n",
        "train_data = dataset['train']['set']\n",
        "# For agility we only 1/2 of our available data\n",
        "n_examples = dataset['train'].num_rows // 2\n",
        "\n",
        "for i in range(n_examples):\n",
        "  example = train_data[i]\n",
        "  train_examples.append(InputExample(texts=[example['query'], example['pos'][0], example['neg'][0]]))"
      ],
      "metadata": {
        "id": "yQ9v7x2J1C75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"We have a {type(train_examples)} of length {len(train_examples)} containing {type(train_examples[0])}'s.\")"
      ],
      "metadata": {
        "id": "j1XgafnCFHbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wrap our training dataset into a Pytorch `Dataloader` to shuffle examples and get batch sizes."
      ],
      "metadata": {
        "id": "6qVYjIIOyppB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
      ],
      "metadata": {
        "id": "gbrXjod4dhJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss functions for training a Sentence Transformers model\n"
      ],
      "metadata": {
        "id": "0HeVHzC-8bbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "train_loss = losses.TripletLoss(model=model)"
      ],
      "metadata": {
        "id": "t1gFBA6EFK10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to train a Sentence Transformer model\n"
      ],
      "metadata": {
        "id": "QTSVaysgVHZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data"
      ],
      "metadata": {
        "id": "rBBR6OseFW0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training takes around 45 minutes with a Google Colab Pro account. Decrease the number of epochs and examples if you are using a free account or no GPU."
      ],
      "metadata": {
        "id": "Y17niC39k9S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          epochs=num_epochs,\n",
        "          warmup_steps=warmup_steps)"
      ],
      "metadata": {
        "id": "7Ana59pTFjKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to share a Sentence Transformers to the Hugging Face Hub"
      ],
      "metadata": {
        "id": "uQEnJz5MVMPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "2iwjWb7HyTEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_to_hub(\n",
        "    \"distilroberta-base-sentence-transformer\",\n",
        "    organization=\"embedding-data\",\n",
        "    train_datasets=[\"embedding-data/QQP_triplets\"],\n",
        "    exist_ok=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "sM6pu_adyUw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra: How to fine-tune a Sentence Transformer model\n"
      ],
      "metadata": {
        "id": "Rq2ROpPUVSS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will fine-tune our Sentence Transformer model."
      ],
      "metadata": {
        "id": "iErisVnE5sCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelB = SentenceTransformer('embedding-data/distilroberta-base-sentence-transformer')"
      ],
      "metadata": {
        "id": "JPEqCxxr0REd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = \"embedding-data/sentence-compression\"\n",
        "datasetB = load_dataset(dataset_id)"
      ],
      "metadata": {
        "id": "akDLR6IS0sdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Examples look like this: {datasetB['train']['set'][0]}\")"
      ],
      "metadata": {
        "id": "fwYA76vY2YbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examplesB = []\n",
        "train_dataB = dataset['train']['set']\n",
        "n_examples = dataset['train'].num_rows\n",
        "\n",
        "for i in range(n_examples):\n",
        "  example = train_dataB[i]\n",
        "  train_examplesB.append(InputExample(texts=[example[0], example[1]]))"
      ],
      "metadata": {
        "id": "2s1c05zF2g7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloaderB = DataLoader(train_examplesB, shuffle=True, batch_size=64)\n",
        "train_lossB = losses.MultipleNegativesRankingLoss(model=modelB)\n",
        "num_epochsB = 10\n",
        "warmup_stepsB = int(len(train_dataloaderB) * num_epochsB * 0.1) #10% of train data"
      ],
      "metadata": {
        "id": "cMYfDAbd53xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelB.fit(train_objectives=[(train_dataloaderB, train_lossB)],\n",
        "          epochs=num_epochsB,\n",
        "          warmup_steps=warmup_stepsB)"
      ],
      "metadata": {
        "id": "lfRC496k6hS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelB.save_to_hub(\n",
        "    \"distilroberta-base-sentence-transformer\",\n",
        "    organization=\"embedding-data\",\n",
        "    train_datasets=[\"embedding-data/sentence-compression\"],\n",
        "    exist_ok=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "Ilq1kA6Rbboh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}