{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1Fgv6mY_VdoX2Y8evlBdq4bI2bSqUg774?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnzMD40DdVFO"
      },
      "source": [
        "source\n",
        "\n",
        "- https://github.com/huggingface/datasets/blob/main/notebooks/Overview.ipynb\n",
        "- https://github.com/huggingface/notebooks/blob/main/datasets_doc/en/tensorflow/quickstart.ipynb\n",
        "- https://huggingface.co/docs/datasets/process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNp6kK7OvSUg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# HuggingFace ðŸ¤— Datasets library - Quick overview\n",
        "\n",
        "Models come and go (linear models, LSTM, Transformers, ...) but two core elements have consistently been the beating heart of Natural Language Processing: Datasets & Metrics\n",
        "\n",
        "ðŸ¤— Datasets is a fast and efficient library to easily share and load datasets, already providing access to the public datasets in the [Hugging Face Hub](https://huggingface.co/datasets).\n",
        "\n",
        "The library has several interesting features (besides easy access to datasets):\n",
        "\n",
        "- Build-in interoperability with PyTorch, Tensorflow 2, Pandas and Numpy\n",
        "- Lighweight and fast library with a transparent and pythonic API\n",
        "- Strive on large datasets: frees you from RAM memory limits, all datasets are memory-mapped on drive by default.\n",
        "- Smart caching with an intelligent `tf.data`-like cache: never wait for your data to process several times\n",
        "\n",
        "ðŸ¤— Datasets originated from a fork of the awesome Tensorflow-Datasets and the HuggingFace team want to deeply thank the team behind this amazing library and user API. We have tried to keep a layer of compatibility with `tfds` and can provide conversion from one format to the other.\n",
        "To learn more about how to use metrics, take a look at the library ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)! In addition to metrics, you can find more tools for evaluating models and datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzk9aEtIvSUh",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Main datasets API\n",
        "\n",
        "This notebook is a quick dive in the main user API for loading datasets in `datasets`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my95uHbLyjwR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# install datasets\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVjXLiYxvSUl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Let's import the library. We typically only need at most two methods:\n",
        "from datasets import list_datasets, load_dataset\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNloBBx-vSUo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Listing the currently available datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3RJisGLvSUp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Currently available datasets\n",
        "datasets = list_datasets()\n",
        "\n",
        "print(f\"ðŸ¤© Currently {len(datasets)} datasets are available on the hub:\")\n",
        "pprint(datasets[:100] + [f\"{len(datasets) - 100} more...\"], compact=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uqSkkSovSUt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## An example with SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOXl6afcvSUu",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Downloading and loading a dataset\n",
        "dataset = load_dataset('squad', split='validation[:10%]')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ0G-eK3vSUw",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "This call to `datasets.load_dataset()` does the following steps under the hood:\n",
        "\n",
        "1. Download and import in the library the **SQuAD python processing script** from HuggingFace AWS bucket if it's not already stored in the library. You can find the SQuAD processing script [here](https://github.com/huggingface/datasets/tree/master/datasets/squad/squad.py) for instance.\n",
        "\n",
        "   Processing scripts are small python scripts which define the info (citation, description) and format of the dataset and contain the URL to the original SQuAD JSON files and the code to load examples from the original SQuAD JSON files.\n",
        "\n",
        "\n",
        "2. Run the SQuAD python processing script which will:\n",
        "    - **Download the SQuAD dataset** from the original URL (see the script) if it's not already downloaded and cached.\n",
        "    - **Process and cache** all SQuAD in a structured Arrow table for each standard splits stored on the drive.\n",
        "\n",
        "      Arrow table are arbitrarily long tables, typed with types that can be mapped to numpy/pandas/python standard types and can store nested objects. They can be directly access from drive, loaded in RAM or even streamed over the web.\n",
        "    \n",
        "\n",
        "3. Return a **dataset built from the splits** asked by the user (default: all); in the above example we create a dataset with the first 10% of the validation split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fercoFwLvSUx",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Informations on the dataset (description, citation, size, splits, format...)\n",
        "# are provided in `dataset.info` (a simple python dataclass) and also as direct attributes in the dataset object\n",
        "pprint(dataset.info.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE0E87zsvSUz",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Inspecting and using the dataset: elements, slices and columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKf4YFnevSU0",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The returned `Dataset` object is a memory mapped dataset that behaves similarly to a normal map-style dataset. It is backed by an Apache Arrow table which allows many interesting features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP1xPqSyvSU0",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiO3rC8yvSU2",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "You can query it's length and get items or slices like you would do normally with a python mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxLcdj2yvSU3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(f\"ðŸ‘‰ Dataset len(dataset): {len(dataset)}\")\n",
        "print(\"\\nðŸ‘‰ First item 'dataset[0]':\")\n",
        "pprint(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk1WQ_cczP5w",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Or get slices with several examples:\n",
        "print(\"\\nðŸ‘‰Slice of the two items 'dataset[10:12]':\")\n",
        "pprint(dataset[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXj2Qr5KvSU5",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# You can get a full column of the dataset by indexing with its name as a string:\n",
        "print(dataset['question'][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Au7rqPMvSU7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The `__getitem__` method will return different format depending on the type of query:\n",
        "\n",
        "- Items like `dataset[0]` are returned as dict of elements.\n",
        "- Slices like `dataset[10:20]` are returned as dict of lists of elements.\n",
        "- Columns like `dataset['question']` are returned as a list of elements.\n",
        "\n",
        "This may seems surprising at first but in our experiments it's actually a lot easier to use for data processing than returning the same format for each of these views on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DB_y79cvSU8",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In particular, you can easily iterate along columns in slices, and also naturally permute consecutive indexings with identical results as showed here by permuting column indexing with elements and slices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjGocqArvSU9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(dataset[0]['question'] == dataset['question'][0])\n",
        "print(dataset[10:20]['context'] == dataset['context'][10:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1-Kj1xQvSU_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Dataset are internally typed and structured\n",
        "\n",
        "The dataset is backed by one (or several) Apache Arrow tables which are typed and allows for fast retrieval and access as well as arbitrary-size memory mapping.\n",
        "\n",
        "This means respectively that the format for the dataset is clearly defined and that you can load datasets of arbitrary size without worrying about RAM memory limitation (basically the dataset take no space in RAM, it's directly read from drive when needed with fast IO access)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAnp_RyPvSVA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# You can inspect the dataset column names and types\n",
        "print(\"Column names:\")\n",
        "pprint(dataset.column_names)\n",
        "print(\"Features:\")\n",
        "pprint(dataset.features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au4v3mOQvSVC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Additional misc properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efFhDWhlvSVC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Datasets also have shapes informations\n",
        "print(\"The number of rows\", dataset.num_rows, \"also available as len(dataset)\", len(dataset))\n",
        "print(\"The number of columns\", dataset.num_columns)\n",
        "print(\"The shape (rows, columns)\", dataset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ox7ppKDvSVN",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Modifying the dataset with `dataset.map`\n",
        "\n",
        "Now that we know how to inspect our dataset we also want to update it. For that there is a powerful method `.map()` which is inspired by `tf.data` map method and that you can use to apply a function to each examples, independently or in batch.\n",
        "\n",
        "`.map()` takes a callable accepting a dict as argument (same dict as the one returned by `dataset[i]`) and iterate over the dataset by calling the function on each example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz2-27HevSVN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Let's print the length of each `context` string in our subset of the dataset\n",
        "# (10% of the validation i.e. 1057 examples)\n",
        "\n",
        "dataset.map(lambda example: print(len(example['question']), end=','))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta3celHnvSVP",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "This is basically the same as doing\n",
        "\n",
        "```python\n",
        "for example in dataset:\n",
        "    function(example)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Fjr0DJawuS",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The above examples was a bit verbose. We can control the logging level of ðŸ¤— Datasets with it's logging module:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAgptXFYaquI",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from datasets import logging\n",
        "logging.set_verbosity_warning()\n",
        "\n",
        "dataset.map(lambda example: print(len(example['context']), end=','))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfED6CEHa8J_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Let's keep it verbose for our tutorial though\n",
        "from datasets import logging\n",
        "logging.set_verbosity_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Ouw5gDvSVP",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The above example had no effect on the dataset because the method we supplied to `.map()` didn't return a `dict` or a `abc.Mapping` that could be used to update the examples in the dataset.\n",
        "\n",
        "In such a case, `.map()` will return the same dataset (`self`).\n",
        "\n",
        "Now let's see how we can use a method that actually modify the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEnCi9DFvSVQ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Modifying the dataset example by example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA37VgZhvSVQ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The main interest of `.map()` is to update and modify the content of the table and leverage smart caching and fast backend.\n",
        "\n",
        "To use `.map()` to update elements in the table you need to provide a function with the following signature: `function(example: dict) -> dict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUr65K-4vSVQ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Let's add a prefix 'My cute title: ' to each of our titles\n",
        "\n",
        "def add_prefix_to_title(example):\n",
        "    example['title'] = 'My cute title: ' + example['title']\n",
        "    return example\n",
        "\n",
        "prefixed_dataset = dataset.map(add_prefix_to_title)\n",
        "\n",
        "print(prefixed_dataset.unique('title'))  # `.unique()` is a super fast way to print the unique elemnts in a column (see the doc for all the methods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcZ_amDAvSVS",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "This call to `.map()` compute and return the updated table. It will also store the updated table in a cache file indexed by the current state and the mapped function.\n",
        "\n",
        "A subsequent call to `.map()` (even in another python session) will reuse the cached file instead of recomputing the operation.\n",
        "\n",
        "You can test this by running again the previous cell, you will see that the result are directly loaded from the cache and not re-computed again.\n",
        "\n",
        "The updated dataset returned by `.map()` is (again) directly memory mapped from drive and not allocated in RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skbf8LUEvSVT",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The function you provide to `.map()` should accept an input with the format of an item of the dataset: `function(dataset[0])` and return a python dict.\n",
        "\n",
        "The columns and type of the outputs can be different than the input dict. In this case the new keys will be added as additional columns in the dataset.\n",
        "\n",
        "Bascially each dataset example dict is updated with the dictionary returned by the function like this: `example.update(function(example))`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5De0CfTvSVT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Since the input example dict is updated with our function output dict,\n",
        "# we can actually just return the updated 'title' field\n",
        "titled_dataset = dataset.map(lambda example: {'title': 'My cutest title: ' + example['title']})\n",
        "\n",
        "print(titled_dataset.unique('title'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5vny56-vSVV",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Removing columns\n",
        "You can also remove columns when running map with the `remove_columns=List[str]` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sPWnsz-vSVW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# This will remove the 'title' column while doing the update (after having send it the the mapped function so you can use it in your function!)\n",
        "less_columns_dataset = dataset.map(lambda example: {'new_title': 'Wouhahh: ' + example['title']}, remove_columns=['title'])\n",
        "\n",
        "print(less_columns_dataset.column_names)\n",
        "print(less_columns_dataset.unique('new_title'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G459HzD-vSVY",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Using examples indices\n",
        "With `with_indices=True`, dataset indices (from `0` to `len(dataset)`) will be supplied to the function which must thus have the following signature: `function(example: dict, indice: int) -> dict`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kFL37R2vSVY",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# This will add the index in the dataset to the 'question' field\n",
        "with_indices_dataset = dataset.map(lambda example, idx: {'question': f'{idx}: ' + example['question']},\n",
        "                                   with_indices=True)\n",
        "\n",
        "pprint(with_indices_dataset['question'][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xckhVEWFvSVb",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Modifying the dataset with batched updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzmicbSnvSVb",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "`.map()` can also work with batch of examples (slices of the dataset).\n",
        "\n",
        "This is particularly interesting if you have a function that can handle batch of inputs like the tokenizers of HuggingFace `tokenizers`.\n",
        "\n",
        "To work on batched inputs set `batched=True` when calling `.map()` and supply a function with the following signature: `function(examples: Dict[List]) -> Dict[List]` or, if you use indices, `function(examples: Dict[List], indices: List[int]) -> Dict[List]`).\n",
        "\n",
        "Bascially, your function should accept an input with the format of a slice of the dataset: `function(dataset[:10])`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxHbgSTL0itj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1utN8K4muDW"
      },
      "source": [
        "### Image datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdYUjP60m-Ie"
      },
      "source": [
        "Images are loaded using Pillow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAbviPxPm4Ce"
      },
      "outputs": [],
      "source": [
        "image_dataset = load_dataset(\"cats_vs_dogs\", split=\"train\")\n",
        "image_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0q3Do11npXd"
      },
      "outputs": [],
      "source": [
        "image_dataset[0][\"image\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzOXxNzQvSVo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Formatting outputs for Tensorflow\n",
        "\n",
        "Now that we have tokenized our inputs, we probably want to use this dataset in a `torch.Dataloader` or a `tf.data.Dataset`. There are various ways to approach this.\n",
        "\n",
        "There is also a convenience method, `to_tf_dataset()`, for the creation of `tf.data.Dataset` objects directly from a HuggingFace `Dataset`. An example will be shown below - when using this method, it is sufficient to pass the `columns` argument and your `DataCollator` - make sure you set the `return_tensors` argument of your `DataCollator` to `tf` or `np`, though, because TensorFlow won't be happy if you start passing it PyTorch Tensors!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj1ukGIuvSVq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"beans\", split=\"train[:1%]\")\n",
        "print(len(dataset))\n",
        "print(dataset.column_names)\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "\n",
        "# define the transformation function\n",
        "def preprocess_image(example):\n",
        "    image = tf.image.convert_image_dtype(example[\"image\"], tf.float32)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
        "    image = tf.image.random_hue(image, max_delta=0.5)\n",
        "    return {\"pixel_values\": image, \"labels\": example[\"labels\"]}\n",
        "\n",
        "# apply the transformation to the dataset\n",
        "dataset = dataset.map(preprocess_image)\n",
        "\n",
        "# convert to TensorFlow dataset\n",
        "tf_dataset = dataset.to_tf_dataset(\n",
        "    columns=[\"pixel_values\"],\n",
        "    label_cols=\"labels\",\n",
        "    batch_size=5,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: {\"pixel_values\": tf.stack([i[\"pixel_values\"] for i in x]), \"labels\": tf.stack([i[\"labels\"] for i in x])},\n",
        ")\n",
        "\n",
        "# usage example\n",
        "for batch in tf_dataset.take(1):\n",
        "    pixel_values, labels = batch\n",
        "    print(pixel_values.shape, labels.shape)"
      ],
      "metadata": {
        "id": "FYZOJ9uGflld"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}