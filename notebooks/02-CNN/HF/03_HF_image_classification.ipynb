{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1U3Pb8Bft24vqCrBbkujJNQkTLfInn3GB?usp=sharing)\n",
        "\n",
        "Sources:\n",
        "- https://github.com/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb\n",
        "- https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb"
      ],
      "metadata": {
        "id": "6E93PlqV80gC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdcMxVGEA9Cd"
      },
      "source": [
        "# **Fine-tuning for Image Classification with ðŸ¤— Transformers**\n",
        "\n",
        "This notebook shows how to fine-tune any pretrained Vision model for Image Classification on a custom dataset. The idea is to add a randomly initialized classification head on top of a pre-trained encoder, and fine-tune the model altogether on a labeled dataset.\n",
        "\n",
        "## ImageFolder\n",
        "\n",
        "This notebook leverages the [ImageFolder](https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder) feature to easily run the notebook on a custom dataset (namely, [EuroSAT](https://github.com/phelber/EuroSAT) in this tutorial). You can either load a `Dataset` from local folders or from local/remote files, like zip or tar.\n",
        "\n",
        "## Any model\n",
        "\n",
        "This notebook is built to run on any image classification dataset with any vision model checkpoint from the [Model Hub](https://huggingface.co/) as long as that model has a TensorFlow version with a Image Classification head, such as:\n",
        "* [ViT](https://huggingface.co/docs/transformers/model_doc/vit#transformers.TFViTForImageClassification)\n",
        "* [Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin#transformers.TFSwinForImageClassification)\n",
        "* [ConvNeXT](https://huggingface.co/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextForImageClassification)\n",
        "* [RegNet](https://huggingface.co/docs/transformers/master/en/model_doc/regnet#transformers.TFRegNetForImageClassification)\n",
        "* [ResNet](https://huggingface.co/docs/transformers/master/en/model_doc/resnet#transformers.TFResNetForImageClassification)\n",
        "\n",
        "- in short, any model supported by [TFAutoModelForImageClassification](https://huggingface.co/docs/transformers/model_doc/auto#transformers.TFAutoModelForImageClassification).\n",
        "\n",
        "## Data augmentation\n",
        "\n",
        "This notebook leverages TensorFlow's [image](https://www.tensorflow.org/api_docs/python/tf/image) module for applying data augmentation. Alternative notebooks which leverage other libraries such as [Albumentations](https://albumentations.ai/) to come!\n",
        "\n",
        "---\n",
        "\n",
        "Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those two parameters, then the rest of the notebook should run smoothly.\n",
        "\n",
        "In this notebook, we'll fine-tune from the https://huggingface.co/microsoft/swin-tiny-patch4-window7-224 checkpoint, but note that there are many, many more available on the [hub](https://huggingface.co/models?other=vision)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WMEawzyCEyG"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlArTG8KChJf"
      },
      "source": [
        "Before we start, let's install the `datasets` and `transformers` libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1532RVbJgQV"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U datasets transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XalxdrirGkLl"
      },
      "source": [
        "## Fine-tuning a model on an image classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnRWZSq0GRRh"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) vision models on an Image Classification dataset.\n",
        "\n",
        "Given an image, the goal is to predict an appropriate class for it, like \"tiger\". The screenshot below is taken from a [ViT fine-tuned on ImageNet-1k](https://huggingface.co/google/vit-base-patch16-224) - try out the inference widget!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu8Od7EgFLwe"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tiger_image.png\" alt=\"drawing\" width=\"600\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcE455KaG687"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD_G2KJgG_bU"
      },
      "source": [
        "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library's [ImageFolder](https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder) feature to download our custom dataset into a DatasetDict.\n",
        "\n",
        "In this case, the EuroSAT dataset is hosted remotely, so we provide the `data_files` argument. Alternatively, if you have local folders with images, you can load them using the `data_dir` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp9xJcHP2TTP"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_dataset(\"cifar100\", num_proc=8)\n",
        "\n",
        "# rename coarse_label and remove fine_label that we are not use for this problem\n",
        "for split in dataset.keys():\n",
        "  dataset[split] = dataset[split].remove_columns(\"fine_label\")\n",
        "  dataset[split] = dataset[split].rename_column(\"coarse_label\", \"label\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq8mwsZU2j6t"
      },
      "source": [
        "Let us also load the Accuracy metric, which we'll use to evaluate our model both during and after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UGse36eLeeb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8mTmFdlHOmN"
      },
      "source": [
        "The `dataset` object itself is a [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key per split (in this case, only \"train\" for a training split)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tjOWPQYLq4u"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfPPNjthI3u2"
      },
      "source": [
        "To access an actual element, you need to select a split first, then give an index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BujWoSgyMQlw"
      },
      "outputs": [],
      "source": [
        "example = dataset[\"train\"][10]\n",
        "example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g0APa21I_Rx"
      },
      "source": [
        "Each example consists of an image and a corresponding label. We can also verify this by checking the features of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnnL3yHBI7Z3"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"].features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ7rLOsAkJ8F"
      },
      "source": [
        "The cool thing is that we can directly view the image (as the 'image' field is an [Image feature](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Image)), as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32iolZyTkNlI"
      },
      "outputs": [],
      "source": [
        "example['img']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1TqooRukQf3"
      },
      "source": [
        "Let's make it a little bigger as the images in the EuroSAT dataset are of low resolution (64x64 pixels):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdO2VFHRkYfT"
      },
      "outputs": [],
      "source": [
        "example['img'].resize((200, 200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMH8dh9w7I86"
      },
      "source": [
        "Let's print the corresponding label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFYtvw4I7KS2"
      },
      "outputs": [],
      "source": [
        "example['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8GpxwfnJCkF"
      },
      "source": [
        "As you can see, the `label` field is not an actual string label. By default the `ClassLabel` fields are encoded into integers for convenience:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n33LZz_ZMU3o"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"].features[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LdpDtScLgeD"
      },
      "source": [
        "Let's create an `id2label` dictionary to decode them back to strings and see what they are. The inverse `label2id` will be useful too, when we load the model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuyXDtQqNUZW"
      },
      "outputs": [],
      "source": [
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "id2label[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxoikSOjs0K"
      },
      "source": [
        "### Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTupOU88p1lK"
      },
      "source": [
        "Before we can feed these images to our model, we need to preprocess them.\n",
        "\n",
        "Preprocessing images typically comes down to (1) resizing them to a particular size (2) normalizing the color channels (R,G,B) using a mean and standard deviation. These are referred to as **image transformations**.\n",
        "\n",
        "In addition, one typically performs what is called **data augmentation** during training (like random cropping and flipping) to make the model more robust and achieve higher accuracy. Data augmentation is also a great technique to increase the size of the training data.\n",
        "\n",
        "We will use `tf.image` for the image transformations/data augmentation in this tutorial, but note that one can use any other package (like [albumentations](https://albumentations.ai/), [imgaug](https://github.com/aleju/imgaug), etc.).\n",
        "\n",
        "To make sure we (1) resize to the appropriate size (2) use the appropriate image mean and standard deviation for the model architecture we are going to use, we instantiate what is called a feature extractor with the `AutoFeatureExtractor.from_pretrained` method.\n",
        "\n",
        "This feature extractor is a minimal preprocessor that can be used to prepare images for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7VDh9JaYh5n"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "feature_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FjaKyIrYh5o"
      },
      "source": [
        "The Datasets library is made for processing data very easily. We can write custom functions, which can then be applied on an entire dataset (either using `.map()` or `.set_transform()`).\n",
        "\n",
        "Here we define 2 separate functions, one for training (which includes data augmentation) and one for validation (which only includes resizing, center cropping and normalizing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mcktmC6Sqzo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "# Create data augmentation with keras as we already see in previous notebooks\n",
        "data_augmentation = tf.keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.25),\n",
        "    layers.RandomZoom(0.25),\n",
        "    layers.RandomContrast(0.1),\n",
        "    layers.RandomBrightness(factor=0.2),\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing import image as keras_image\n",
        "\n",
        "# normalize image function using Keras backend for epsilon\n",
        "def normalize_img(img, mean, std):\n",
        "    mean = tf.constant(mean, dtype=img.dtype)\n",
        "    std = tf.constant(std, dtype=img.dtype)\n",
        "    return (img - mean) / tf.maximum(std, K.epsilon())\n",
        "\n",
        "# function to resize image to a target size\n",
        "def resize_image(img, target_size):\n",
        "    return tf.image.resize(img, target_size)\n",
        "\n",
        "# function to apply data augmentation\n",
        "def data_augmentation(img):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.random_brightness(img, max_delta=0.1)\n",
        "    img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
        "    return img\n",
        "\n",
        "# transform function for training images\n",
        "def train_transforms(image):\n",
        "    image = keras_image.img_to_array(image)\n",
        "    image = resize_image(image, (224, 224))\n",
        "    image = data_augmentation(image)\n",
        "    # image /= 255.0  # Scale pixel values to [0, 1]\n",
        "    image = normalize_img(image,\n",
        "                          mean=feature_extractor.image_mean,\n",
        "                          std=feature_extractor.image_std)\n",
        "    return tf.transpose(image, (2, 0, 1))\n",
        "\n",
        "# preprocess function for training batch\n",
        "def preprocess_train(example_batch):\n",
        "    example_batch['pixel_values'] = [train_transforms(img.convert(\"RGB\")) for img in example_batch[\"img\"]]\n",
        "    return example_batch\n",
        "\n",
        "# preprocess function for validation batch (assuming same as train for simplicity)\n",
        "def preprocess_val(example_batch):\n",
        "    return preprocess_train(example_batch)\n"
      ],
      "metadata": {
        "id": "EsDFZbIubO6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h20kKvlkYh5s"
      },
      "source": [
        "Next, we can preprocess our dataset by applying these functions. We will use the set_transform functionality, which allows to apply the functions above on-the-fly (meaning that they will only be applied when the images are loaded in RAM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qISRPFuvYh5s"
      },
      "outputs": [],
      "source": [
        "# split up training into training + validation\n",
        "splits = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "train_ds = splits['train']\n",
        "val_ds = splits['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_iy7mdYh5t"
      },
      "outputs": [],
      "source": [
        "train_ds.set_transform(preprocess_train)\n",
        "val_ds.set_transform(preprocess_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5_G8EyDYh5t",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSxY43eRYh5u"
      },
      "source": [
        "Let's access an element to see that we've added a \"pixel_values\" feature:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOXmyPQ76Qv9"
      },
      "source": [
        "### Training with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a-2YT7O6ayC"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. For classification we use the `TFAutoModelForImageClassification` class. Calling the `from_pretrained` method on it will download and cache the weights for us. As the label ids and the number of labels are dataset dependent, we pass `label2id`, and `id2label` alongside the `model_checkpoint` here. This will make sure a custom classification head will be created (with a custom number of output neurons).\n",
        "\n",
        "NOTE: in case you're planning to fine-tune an already fine-tuned checkpoint, like [facebook/convnext-tiny-224](https://huggingface.co/facebook/convnext-tiny-224) (which has already been fine-tuned on ImageNet-1k), then you need to provide the additional argument `ignore_mismatched_sizes=True` to the `from_pretrained` method. This will make sure the output head (with 1000 output neurons) is thrown away and replaced by a new, randomly initialized classification head that includes a custom number of output neurons. You don't need to specify this argument in case the pre-trained model doesn't include a head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhfN1KMQYh5v"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForImageClassification, AutoModelForImageClassification\n",
        "\n",
        "model = TFAutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes=True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "rkDmM3tUXnxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check model architecture and dimensions\n",
        "_model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "_model"
      ],
      "metadata": {
        "id": "Q-CBh040XcgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8EmET_f6458"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the weights and bias of the `classifier` layer) and randomly initializing some other (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzx3uCNFYh5w"
      },
      "outputs": [],
      "source": [
        "learning_rate = 5e-5\n",
        "weight_decay = 0.01\n",
        "epochs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-Malwq3Yh5x"
      },
      "source": [
        "Now we initialize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZylsAB5fYh5x"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamWeightDecay\n",
        "\n",
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqGkgnt4Yh5x"
      },
      "source": [
        "Note that most models on the Hub compute loss internally, so we actually don't have to specify anything there! Leaving the loss field blank will cause the model to read the loss head as its loss value.\n",
        "\n",
        "This is an unusual quirk of TensorFlow models in ðŸ¤— Transformers, so it's worth elaborating on in a little more detail. All ðŸ¤— Transformers models are capable of computing an appropriate loss for their task internally (for example, a CausalLM model will use a cross-entropy loss). To do this, the labels must be provided in the input dict (or equivalently, in the `columns` argument to `to_tf_dataset()`), so that they are visible to the model during the forward pass.\n",
        "\n",
        "This is quite different from the standard Keras way of handling losses, where labels are passed separately and not visible to the main body of the model, and loss is handled by a function that the user passes to `compile()`, which uses the model outputs and the label to compute a loss value.\n",
        "\n",
        "The approach we take is that if the user does not pass a loss to `compile()`, the model will assume you want the **internal** loss. If you are doing this, you should make sure that the labels column(s) are included in the **input dict** or in the `columns` argument to `to_tf_dataset`.\n",
        "\n",
        "If you want to use your own loss, that is of course possible too! If you do this, you should make sure your labels column(s) are passed like normal labels, either as the **second argument** to model.fit(), or in the `label_cols` argument to `to_tf_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jur15JL8Yh5y"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxi3X0CnYh5y"
      },
      "source": [
        "We need to convert our datasets to a format Keras understands. The easiest way to do this is with the `to_tf_dataset()` method. Note that our data collators are designed to work for multiple frameworks, so ensure you set the `return_tensors='np'` argument to get NumPy arrays out - you don't want to accidentally get a load of `torch.Tensor` objects in the middle of your nice TF code! You could also use `return_tensors='tf'` to get TensorFlow tensors, but our `to_tf_dataset` pipeline actually uses a NumPy loader internally, which is wrapped at the end with a `tf.data.Dataset`. As a result, `np` is usually more reliable and performant when you're using it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ1eelzhYh5z"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vHhNBRKYh5z"
      },
      "outputs": [],
      "source": [
        "data_collator = DefaultDataCollator(return_tensors=\"np\")\n",
        "\n",
        "train_set = train_ds.to_tf_dataset(\n",
        "    columns=[\"pixel_values\", \"label\"],\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "val_set = val_ds.to_tf_dataset(\n",
        "    columns=[\"pixel_values\", \"label\"],\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX2UYhsNYh5z"
      },
      "source": [
        "`train_set` is now a `tf.data.Dataset` type object. We see that it contains two elements - `labels` and `pixel_values` (but not `image`) as a result of the preprocessing done in `preprocess_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9VhSTtSYh50"
      },
      "outputs": [],
      "source": [
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bfWcfG9Yh50",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_set))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buuuNy96Yh51"
      },
      "source": [
        "The last thing to define is how to compute the metrics from the predictions. We need to define a function for this, which will just use the metric we loaded earlier. The only preprocessing we have to do is to take the argmax of our predicted logits.\n",
        "\n",
        "In addition, let's wrap this metric computation function in a `KerasMetricCallback`. This callback will compute the metric on the validation set each epoch, including printing it and logging it for other callbacks like TensorBoard and EarlyStopping.\n",
        "\n",
        "Why do it this way, though, and not just use a straightforward Keras Metric object? This is a good question - on this task, metrics such as Accuracy are very straightforward, and it would probably make more sense to just use a Keras metric for those instead. However, we want to demonstrate the use of `KerasMetricCallback` here, because it can handle any arbitrary Python function for the metric computation.\n",
        "\n",
        "Wow do we actually use `KerasMetricCallback`? We simply define a function that computes metrics given a tuple of numpy arrays of predictions and labels, then we pass that, along with the validation set to compute metrics on, to the callback:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Efu9lwSWYh51"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "# the compute_metrics function takes a Tuple as input:\n",
        "# first element is the logits of the model as Numpy arrays,\n",
        "# second element is the ground-truth labels as Numpy arrays.\n",
        "def compute_metrics(eval_predictions):\n",
        "    predictions = np.argmax(eval_predictions[0], axis=1)\n",
        "    metric_val = metric.compute(predictions=predictions, references=eval_predictions[1])\n",
        "    return {\"val_\" + k: v for k, v in metric_val.items()}\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn=compute_metrics, eval_dataset=val_set, batch_size=batch_size, label_cols=['labels']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EXrtwiYYh52"
      },
      "source": [
        "Now we can train our model. We can also add a callback to sync up our model with the Hub - this allows us to resume training from other machines and even test the model's inference quality midway through training! Make sure to change the `username` if you do. If you don't want to do this, simply remove the callbacks argument in the call to `fit()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aek8sKloYh53"
      },
      "outputs": [],
      "source": [
        "callbacks = [metric_callback]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO3HNgIIYh53"
      },
      "outputs": [],
      "source": [
        " model.fit(\n",
        "    train_set,\n",
        "    validation_data=val_set,\n",
        "    callbacks=callbacks,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "049gH1wt-Akp"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's say you have a new image, on which you'd like to make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX6dwmT7GP91"
      },
      "outputs": [],
      "source": [
        "image = dataset['test'][2]['img']\n",
        "label_id = dataset['test'][2]['label']\n",
        "print(f'Label: {model.config.id2label[label_id]}')\n",
        "image.resize((200, 200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "298o50gr-Rwt"
      },
      "outputs": [],
      "source": [
        "# prepare image for the model\n",
        "image_preprocessed = train_transforms(image)\n",
        "model_input = {'pixel_values': tf.expand_dims(image_preprocessed, axis=0)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtslLjT_Yh58"
      },
      "outputs": [],
      "source": [
        "outputs = model(model_input)\n",
        "logits = outputs.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ctUvqfs_Yyn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predicted_class_idx = tf.math.argmax(logits, -1).numpy()[0]\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "668fb96a716f4e6c0ace6609c578b3593a4af0cc3bba8b3739e6b5cb74dc056a"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "668fb96a716f4e6c0ace6609c578b3593a4af0cc3bba8b3739e6b5cb74dc056a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}