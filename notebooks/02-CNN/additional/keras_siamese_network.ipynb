{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWUyfDiV6Q2-"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_aTc_O0oIB3FMX_zGYIiu-HLWSYJuR2d?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke8z2tlIK4iX"
      },
      "source": [
        "https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Siamese_net.ipynb\n",
        "\n",
        "We are going to create a siamese net for learning a metric to calculate similarities and dissimilarities between different types of images.\n",
        "\n",
        "<img src=\"https://i.ibb.co/FV4pj65/siamese.png\" alt=\"Alex-Net-architecture\" border=\"0\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hge3dmmA0Rug"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQAPn9sb0Rup"
      },
      "source": [
        "## Simple Siamese-Net with Fashion MNIST\n",
        "\n",
        "Fashion MNIST dataset contains 70,000 grayscale images with 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWas9HgU0Ruq"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# prepare train and test sets\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "\n",
        "# normalize values\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "\n",
        "class_names = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt',\n",
        "    'Sneaker', 'Bag', 'Ankle boot'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-myJBGF0Rus"
      },
      "outputs": [],
      "source": [
        "# We create triplets where the anchor and the positive have the same class.\n",
        "# The negative has different class\n",
        "def create_triplets(images, labels, n_triplets_per_class=7500):\n",
        "    triplets = [] # (anchor, positive, negative)\n",
        "    n_labels = len(np.unique(labels))\n",
        "    for class_ind in range(n_labels):\n",
        "        # Iterate over the classes\n",
        "        pos_class_inds = np.argwhere(labels == class_ind).flatten()\n",
        "        neg_class_inds = np.argwhere(labels != class_ind).flatten()\n",
        "        for _ in range(n_triplets_per_class):\n",
        "            # anchor\n",
        "            anchor_ind = np.random.choice(pos_class_inds)\n",
        "            anchor = images[anchor_ind]\n",
        "            # positive\n",
        "            pos_ind = np.random.choice(pos_class_inds)\n",
        "            positive = images[pos_ind]\n",
        "            # negative\n",
        "            neg_ind = np.random.choice(neg_class_inds)\n",
        "            negative = images[neg_ind]\n",
        "            # add triplet\n",
        "            triplets.append((anchor, positive, negative))\n",
        "    triplets = np.array(triplets)\n",
        "    return triplets\n",
        "\n",
        "train_triplets = create_triplets(train_images, train_labels, 7500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lvg2yKHD0Rus"
      },
      "outputs": [],
      "source": [
        "train_triplets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs7XfvHQ0Rux"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "ind = np.random.randint(len(train_triplets))\n",
        "labels = ['Anchor', 'Positive', 'Negative']\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_triplets[ind,i,:,:], cmap='gray')\n",
        "    plt.xlabel(labels[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c1e2kwK6Q3D"
      },
      "source": [
        "### Triplet Loss\n",
        "\n",
        "- A is  \"Anchor\"\n",
        "- P is  \"Positive\"\n",
        "- N is  \"Negative\"\n",
        "\n",
        " $(A^{(i)}, P^{(i)}, N^{(i)})$   is the $i$-th training triplet.\n",
        "\n",
        "You'd like to make sure that an image $A^{(i)}$  is closer to the Positive $P^{(i)}$ than to the Negative image $N^{(i)}$) by at least a margin $\\alpha$:\n",
        "\n",
        "$$\\mid \\mid \\mathrm{vec}(A^{(i)}) - \\mathrm{vec}(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid \\mathrm{vec}(A^{(i)}) - \\mathrm{vec}(N^{(i)}) \\mid \\mid_2^2$$\n",
        "\n",
        "We want to minimize the  \"triplet cost\":\n",
        "\n",
        "$$\\mathcal{J}(A,P,N) = \\sum^{m}_{i=1} \\max \\left(  \\small \\underbrace{\\mid \\mid \\mathrm{vec}(A^{(i)}) - \\mathrm{vec}(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid \\mathrm{vec}(A^{(i)}) - \\mathrm{vec}(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha  , 0 \\right) $$\n",
        "\n",
        "- The term (1) is the squared distance between the anchor \"A\" and the positive \"P\" for a given triplet; you want this to be small.\n",
        "```python\n",
        "pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "```\n",
        "\n",
        "- The term (2) is the squared distance between the anchor \"A\" and the negative \"N\" for a given triplet, you want this to be relatively large. It has a minus sign preceding it because minimizing the negative of the term is the same as maximizing that term.\n",
        "```python\n",
        "neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "```\n",
        "\n",
        "- $\\alpha$ is  the margin.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHkeGRXb0Ruz"
      },
      "outputs": [],
      "source": [
        "def triplet_loss(y_true, y_pred, margin = 0.2):\n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "    loss = tf.maximum(pos_dist - neg_dist + margin, 0.0)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "\n",
        "def distance(vec1, vec2):\n",
        "    return tf.reduce_sum(tf.square(vec1 - vec2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8kjjqfL0Ru0"
      },
      "source": [
        "### Create Model\n",
        "\n",
        "First of all, we create the shared `base_model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPbQZPfq0Ru1"
      },
      "outputs": [],
      "source": [
        "vec_dim = 64\n",
        "\n",
        "inputs = tf.keras.Input(shape=(28,28,))\n",
        "x = layers.Flatten(name=\"flatten_input\")(inputs)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.Dropout(0.25)(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.Dropout(0.25, name=\"second_dropout\")(x)\n",
        "\n",
        "x = layers.Dense(vec_dim, activation='linear')(x)\n",
        "# L2 normalize\n",
        "x = layers.Lambda(lambda z: tf.math.l2_normalize(z,axis=1))(x)\n",
        "\n",
        "base_model = tf.keras.Model(inputs=inputs, outputs=x, name='base_model')\n",
        "\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAcI5U_j0Ru2"
      },
      "outputs": [],
      "source": [
        "# Input triplets (anchor, positive, negative)\n",
        "inputs = tf.keras.Input(shape=(3, 28, 28), name='inputs')\n",
        "\n",
        "## Anchor vector\n",
        "anchor_input = inputs[:, 0, :, :]\n",
        "anchor_vec = base_model(anchor_input)\n",
        "\n",
        "## Positive vector\n",
        "positive_input = inputs[:, 1, :, :]\n",
        "positive_vec = base_model(positive_input)\n",
        "\n",
        "## Negative vector\n",
        "negative_input = inputs[:, 2, :, :]\n",
        "negative_vec = base_model(negative_input)\n",
        "\n",
        "## stack all the vectors dim: (3, none, vec_dim)\n",
        "outputs = tf.stack([anchor_vec, positive_vec, negative_vec])\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV_Yh8gK0Ru2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM-LBQTo0Ru2"
      },
      "outputs": [],
      "source": [
        "y = np.zeros(len(train_triplets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsudWqWL0Ru3"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=triplet_loss,optimizer='adam')\n",
        "history = model.fit(train_triplets, y, epochs=3, validation_split=0.15, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDI4Husz0Ru3"
      },
      "source": [
        "### Visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFigwBZj0Ru3"
      },
      "outputs": [],
      "source": [
        "#(test_images, test_labels)\n",
        "image = test_images[5]\n",
        "image_vec = base_model(np.expand_dims(image, 0))\n",
        "image.shape, image_vec.shape, image_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkWi8BeY0Ru4"
      },
      "outputs": [],
      "source": [
        "def plot_images_dis(base_model, image_vec, images, labels, class_names, margin=0.2):\n",
        "    plt.figure(figsize=(30, 30))\n",
        "    for class_ind in range(len(class_names)):\n",
        "        class_name = class_names[class_ind]\n",
        "        class_inds = np.argwhere(labels == class_ind).flatten()\n",
        "        image_class = np.squeeze(images[np.random.choice(class_inds)])\n",
        "        image_class_vec = base_model(np.expand_dims(image_class, 0))\n",
        "        dis = np.round(distance(image_vec, image_class_vec), 3)\n",
        "        dis_str = str(dis)\n",
        "        if dis < margin:\n",
        "            color = 'green'\n",
        "        else:\n",
        "            color = 'red'\n",
        "        plt.subplot(1, len(class_names), class_ind + 1)\n",
        "        plt.title(class_name, color=color)\n",
        "        plt.xlabel(dis_str, color=color)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(image_class)\n",
        "    plt.show()\n",
        "\n",
        "for class_ind in range(10):\n",
        "    print('-'*80)\n",
        "    class_name = class_names[class_ind]\n",
        "    print(class_name, class_ind)\n",
        "    class_inds = np.argwhere(test_labels == class_ind).flatten()\n",
        "    image = np.squeeze(test_images[np.random.choice(class_inds)])\n",
        "    image_vec = base_model(np.expand_dims(image, 0))\n",
        "\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "    print('distances:')\n",
        "    plot_images_dis(base_model, image_vec, test_images, test_labels, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBdo8iZ-6Q3F"
      },
      "source": [
        "### Similarity and Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXY-LEe76Q3G"
      },
      "outputs": [],
      "source": [
        "def calculate_nearest_class(image_vec, image_class, vecs, labels):\n",
        "    '''Find the neares class of a single image vector'''\n",
        "    # vectors are normalized: argmin_i |v -v_i| = argmax_i v_i.dot(v)\n",
        "    most_similar_ind = np.argsort(-vecs.dot(image_vec))[1]\n",
        "    # index 0 is for the same vector\n",
        "    nearest_class = labels[most_similar_ind]\n",
        "    return nearest_class\n",
        "\n",
        "\n",
        "def calculate_confusion_matrix(vecs, labels, class_names, samples=20):\n",
        "    num_classes = len(class_names)\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
        "    for class_ind in range(num_classes):\n",
        "        pos_class_inds = np.argwhere(labels == class_ind).flatten()\n",
        "        class_samples = np.random.choice(pos_class_inds, samples)\n",
        "        for sample_ind in class_samples:\n",
        "            image_vec = vecs[sample_ind, :]\n",
        "            nn_class = calculate_nearest_class(image_vec, class_ind, vecs,\n",
        "                                               labels)\n",
        "            confusion_matrix[class_ind, nn_class] += 1\n",
        "    return confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHnRAVEx6Q3G"
      },
      "outputs": [],
      "source": [
        "test_vecs = base_model(test_images).numpy()\n",
        "confusion_matrix = calculate_confusion_matrix(test_vecs,\n",
        "                                              test_labels,\n",
        "                                              class_names,\n",
        "                                              samples=99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq7qSdTb6Q3G"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cmp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix,\n",
        "                             display_labels=class_names)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8));\n",
        "cmp.plot(ax=ax, xticks_rotation=\"vertical\", include_values=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDpsNgkd6Q3G"
      },
      "outputs": [],
      "source": [
        "def calculate_diff_nearest_class(image_vec, image_class, vecs, labels):\n",
        "    '''Find the nearest different class of a single image vector'''\n",
        "    # Different class indexes\n",
        "    neg_class_inds = np.argwhere(labels != image_class).flatten()\n",
        "    # Vectors of different class\n",
        "    neg_vecs = vecs[neg_class_inds, :]\n",
        "    # vectors are normalized: argmin_i |v -v_i| = argmax_i v_i.dot(v)\n",
        "    most_similar_ind = np.argmax(neg_vecs.dot(image_vec))\n",
        "    nearest_class = labels[neg_class_inds[most_similar_ind]]\n",
        "    return nearest_class\n",
        "\n",
        "def calculate_nearest_classes(vecs, labels, class_names, samples=20):\n",
        "    num_classes = len(class_names)\n",
        "    nearest_classes = np.zeros((num_classes, num_classes))\n",
        "    for class_ind in range(num_classes):\n",
        "        pos_class_inds = np.argwhere(labels == class_ind).flatten()\n",
        "        class_samples = np.random.choice(pos_class_inds, samples)\n",
        "        for sample_ind in class_samples:\n",
        "            image_vec = vecs[sample_ind, :]\n",
        "            nn_class = calculate_diff_nearest_class(image_vec, class_ind, vecs,\n",
        "                                                    labels)\n",
        "            nearest_classes[class_ind, nn_class] += 1\n",
        "    return nearest_classes\n",
        "\n",
        "\n",
        "test_vecs = base_model(test_images).numpy()\n",
        "nearest_classes = calculate_nearest_classes(test_vecs,\n",
        "                                              test_labels,\n",
        "                                              class_names,\n",
        "                                              samples=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvFHT0wN6Q3G"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cmp = ConfusionMatrixDisplay(confusion_matrix=nearest_classes, display_labels=class_names)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "cmp.plot(ax=ax, xticks_rotation=\"vertical\", include_values=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBnyx5_B0Ru6"
      },
      "source": [
        "### Question 1: Create a convolutional base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v35f_4ou0Ru7"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "(train_images,\n",
        " train_labels), (test_images,\n",
        "                 test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# prepare train and test sets\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "\n",
        "# normalize values\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "train_images = np.expand_dims(train_images, -1)\n",
        "test_images = np.expand_dims(test_images, -1)\n",
        "\n",
        "class_names = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt',\n",
        "    'Sneaker', 'Bag', 'Ankle boot'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH_3h_580Ru7"
      },
      "outputs": [],
      "source": [
        "train_triplets = create_triplets(train_images, train_labels, 10000)\n",
        "train_triplets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IElwTSBH0RvD"
      },
      "outputs": [],
      "source": [
        "vec_dim = 64\n",
        "\n",
        "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
        "\n",
        "## convolutional layers\n",
        "# Conv Layer 1\n",
        "... = layers.Conv2D(...)(inputs)\n",
        "... = layers.MaxPooling2D(...)\n",
        "...\n",
        "\n",
        "## Flatten layer\n",
        "flat = ...\n",
        "\n",
        "\n",
        "## Output vector\n",
        "x = layers.Dense(...)\n",
        "# L2 normalize\n",
        "outputs = layers.Lambda(lambda z: tf.math.l2_normalize(z,axis=1))(x)\n",
        "\n",
        "\n",
        "base_model = tf.keras.Model(inputs=inputs, outputs=outputs, name='base_model')\n",
        "\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzNefw5H0Rvf"
      },
      "outputs": [],
      "source": [
        "# Input triplets (anchor, positive, negative)\n",
        "inputs = tf.keras.Input(shape=(3, 28, 28, 1), name='inputs')\n",
        "\n",
        "## Anchor vector\n",
        "anchor_input = inputs[:, 0, :, :, :]\n",
        "anchor_vec = base_model(anchor_input)\n",
        "\n",
        "## Positive vector\n",
        "positive_input = inputs[:, 1, :, :, :]\n",
        "positive_vec = base_model(positive_input)\n",
        "\n",
        "## Negative vector\n",
        "negative_input = inputs[:, 2, :, :, :]\n",
        "negative_vec = base_model(negative_input)\n",
        "\n",
        "## stack all the vectors dim: (3, none, vec_dim)\n",
        "outputs = tf.stack([anchor_vec, positive_vec, negative_vec])\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THBXPAQw0Rvf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8oN2WaJ0Rvg"
      },
      "outputs": [],
      "source": [
        "y = np.zeros(len(train_triplets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUjlBAnV0Rvg"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=triplet_loss,optimizer='adam')\n",
        "history = model.fit(train_triplets, y, epochs=8, validation_split=0.15, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqnU_AVi0Rvg"
      },
      "source": [
        "### Visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6z3GjFL0Rvg"
      },
      "outputs": [],
      "source": [
        "for class_ind in range(10):\n",
        "    print('-'*80)\n",
        "    class_name = class_names[class_ind]\n",
        "    print(class_name, class_ind)\n",
        "    class_inds = np.argwhere(test_labels == class_ind).flatten()\n",
        "    image = np.squeeze(test_images[np.random.choice(class_inds)])\n",
        "    image_vec = base_model(np.expand_dims(image, 0))\n",
        "\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "    print('distances:')\n",
        "    plot_images_dis(base_model, image_vec, test_images, test_labels, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR73XENk6Q3I"
      },
      "source": [
        "### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvTYMVMO6Q3I"
      },
      "outputs": [],
      "source": [
        "test_vecs = base_model(test_images).numpy()\n",
        "confusion_matrix = calculate_confusion_matrix(test_vecs,\n",
        "                                              test_labels,\n",
        "                                              class_names,\n",
        "                                              samples=99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHz9XkCT6Q3I"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cmp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix,\n",
        "                             display_labels=class_names)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8));\n",
        "cmp.plot(ax=ax, xticks_rotation=\"vertical\", include_values=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO29xD7x2YJA"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "del train_triplets, train_images, test_images\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKPZwGtC0Rvh"
      },
      "source": [
        "## Practice\n",
        "\n",
        "Repeat the process with the flower dataset, a set of **~ 3700 photographs** of flowers from **5 different classes**. Same dataset as [Introduction_to_CNN.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Introduction_to_CNN.ipynb).\n",
        "\n",
        "Use a pretrained model for the base-model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUDKgd2a0Rvh"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "dataset_url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
        "data_dir = pathlib.Path(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Swd6kr0Rvh"
      },
      "outputs": [],
      "source": [
        "image_size = (80, 80)\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,  # 80%  train, 20% validation\n",
        "    subset=\n",
        "    'training',  # 'training' o 'validation', only  with 'validation_split'\n",
        "    seed=1,\n",
        "    image_size=image_size,# Dimension (img_height, img_width) for rescaling\n",
        "    batch_size=1\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=1,\n",
        "    image_size=image_size,\n",
        "    batch_size=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqu82Mbq6Q3P"
      },
      "outputs": [],
      "source": [
        "class_names = train_ds.class_names\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "WDixBr1h6Q3P"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i, (images, labels) in enumerate(train_ds.take(9)):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[0].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[0]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZcM4pBb8WRI"
      },
      "outputs": [],
      "source": [
        "train_images, train_labels = zip(*[\n",
        "    (items[0][0], items[1][0]) for items in train_ds.as_numpy_iterator()\n",
        "])\n",
        "train_images = np.array(train_images)\n",
        "train_images = train_images / 255.0\n",
        "train_labels = np.array(train_labels)\n",
        "train_images.shape, train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8fGuAGC7hWN"
      },
      "outputs": [],
      "source": [
        "test_images, test_labels = zip(*[\n",
        "    (items[0][0], items[1][0]) for items in val_ds.as_numpy_iterator()\n",
        "])\n",
        "test_images = np.array(test_images)\n",
        "test_images = test_images / 255.0\n",
        "test_labels = np.array(test_labels)\n",
        "test_images.shape, test_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBpIo2_d0Rvi"
      },
      "outputs": [],
      "source": [
        "train_triplets = create_triplets(train_images, train_labels, 5000)\n",
        "train_triplets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW1cQT2H6Q3Q"
      },
      "source": [
        "### Create the base model\n",
        "Use a pre-trained model like `MobileNetV2`: [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vcp01iw40Rvi"
      },
      "outputs": [],
      "source": [
        "vec_dim = 128\n",
        "\n",
        "inputs = tf.keras.Input(shape=image_size+(3,))\n",
        "\n",
        "## Pre-trained model\n",
        "pretrained_model = ...\n",
        "pretrained_model.trainable...\n",
        "### preprocess inputs\n",
        "\n",
        "## Flatten layer\n",
        "flat = ...\n",
        "\n",
        "## Output vector\n",
        "...\n",
        "# L2 normalize\n",
        "outputs = layers.Lambda(lambda z: tf.math.l2_normalize(z, axis=1))(...)\n",
        "\n",
        "base_model = tf.keras.Model(inputs=inputs, outputs=outputs, name='base_model')\n",
        "\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjIYXyYT0Rvj"
      },
      "outputs": [],
      "source": [
        "# Input triplets (anchor, positive, negative)\n",
        "inputs = tf.keras.Input(shape=(3, 80, 80, 3), name='inputs')\n",
        "\n",
        "## Anchor vector\n",
        "anchor_input = inputs[:, 0, :, :, : ]\n",
        "anchor_vec = base_model(anchor_input)\n",
        "\n",
        "## Positive vector\n",
        "positive_input = inputs[:, 1, :, :,:]\n",
        "positive_vec = base_model(positive_input)\n",
        "\n",
        "## Negative vector\n",
        "negative_input = inputs[:, 2, :, :,:]\n",
        "negative_vec = base_model(negative_input)\n",
        "\n",
        "## stack all the vectors dim: (3, none, vec_dim)\n",
        "outputs = tf.stack([anchor_vec, positive_vec, negative_vec])\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT428hV90Rvj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcM6K8f-0Rvj"
      },
      "outputs": [],
      "source": [
        "y = np.zeros(len(train_triplets))\n",
        "model.compile(loss=triplet_loss,optimizer='adam')\n",
        "history = model.fit(train_triplets, y, epochs=1, validation_split=0.15, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWOiIhcQ6Q3R"
      },
      "source": [
        "### EXTRA (Optional)\n",
        "\n",
        "Every one or two epochs, recreate the training triplets by choosing the negative images closest to the anchor to accelerate convergence.\n",
        "You can use [KDTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNTy0kB-6Q3S"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KDTree\n",
        "#train_vecs = base_model(train_images).numpy()\n",
        "#class_ind = 0\n",
        "#pos_class_inds = np.argwhere(train_labels == class_ind).flatten()\n",
        "#neg_class_inds = np.argwhere(train_labels != class_ind).flatten()\n",
        "def create_triplets_ordered(...\n",
        "new_triplets = create_triplets_ordered(..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq1Tuxt-6Q3S"
      },
      "outputs": [],
      "source": [
        "for _ in range(...):\n",
        "    new_triplets = create_triplets_ordered(...\n",
        "    y = np.zeros(len(new_triplets))\n",
        "    history = model.fit(new_triplets, y, epochs=1, validation_split=0.15, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ8J9N9z0Rvk"
      },
      "source": [
        "### Visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfPPp5Lh0Rvk"
      },
      "outputs": [],
      "source": [
        "for class_ind in range(len(class_names)):\n",
        "    print('-'*80)\n",
        "    class_name = class_names[class_ind]\n",
        "    print(class_name, class_ind)\n",
        "    class_inds = np.argwhere(test_labels == class_ind).flatten()\n",
        "    image = test_images[np.random.choice(class_inds)]\n",
        "    image_vec = base_model(np.expand_dims(image, 0))\n",
        "\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "    print('distances:')\n",
        "    plot_images_dis(base_model, image_vec, test_images, test_labels, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_xxbghpDqC-"
      },
      "outputs": [],
      "source": [
        "## find nearest image\n",
        "def find_k_nearest(image_vec, train_vecs, k=1):\n",
        "    dis_list = []\n",
        "    for i in range(len(train_vecs)):\n",
        "        dis = distance(image_vec, train_vecs[i])\n",
        "        dis_list.append((i, dis))\n",
        "    dis_list = sorted(dis_list, key=lambda z: z[1])\n",
        "    return dis_list[:k]\n",
        "\n",
        "train_vecs = base_model(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skfs1XGJ4hBR"
      },
      "outputs": [],
      "source": [
        "ind = np.random.randint(len(test_images))\n",
        "image = np.expand_dims(test_images[ind], 0)\n",
        "image_vec = base_model(image)\n",
        "nearest = find_k_nearest(image_vec, train_vecs, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe2kRoxbErVC"
      },
      "outputs": [],
      "source": [
        "print('Random image')\n",
        "plt.imshow(image[0])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "print()\n",
        "print('-' * 50)\n",
        "print('Nearest')\n",
        "for ind, dist in nearest:\n",
        "    img = train_images[ind]\n",
        "    label = class_names[train_labels[ind]]\n",
        "    plt.imshow(img)\n",
        "    plt.xlabel('distance:{0:1.2f}'.format(dist))\n",
        "    plt.title(label)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhHr1m073fhl"
      },
      "outputs": [],
      "source": [
        "## load new image\n",
        "# clavel\n",
        "def read_image(image_path, target_size=None, grayscale=False):\n",
        "    image = tf.keras.preprocessing.image.load_img(image_path,\n",
        "                                                  target_size=target_size,\n",
        "                                                  grayscale=grayscale)\n",
        "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    image = image.astype(np.uint8)\n",
        "    return image\n",
        "\n",
        "\n",
        "url = 'https://hips.hearstapps.com/es.h-cdn.co/mcres/images/mi-casa/terraza-jardines-porche/cuidados-del-clavel/1106673-1-esl-ES/los-cuidados-basicos-del-clavel.jpg?crop=1.00xw:0.889xh;0,0.0886xh&resize=480:*'\n",
        "image_path = tf.keras.utils.get_file(\"clavel2.jpg\", url)\n",
        "image = read_image(image_path, target_size=image_size)\n",
        "\n",
        "image = np.expand_dims(image, 0)\n",
        "image_vec = base_model(image)\n",
        "nearest = find_k_nearest(image_vec, train_vecs, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dir_Xjqm0Rvk"
      },
      "outputs": [],
      "source": [
        "print('Image')\n",
        "plt.imshow(image[0])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "print()\n",
        "print('-' * 50)\n",
        "print('Nearest')\n",
        "for ind, dist in nearest:\n",
        "    img = train_images[ind]\n",
        "    label = class_names[train_labels[ind]]\n",
        "    plt.imshow(img)\n",
        "    plt.xlabel('distance:{0:1.2f}'.format(dist))\n",
        "    plt.title(label)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke4xFfNk0Rvk"
      },
      "outputs": [],
      "source": [
        "\n",
        "url = 'https://i.blogs.es/a5bd8a/amapolas-1/450_1000.jpg'\n",
        "image_path = tf.keras.utils.get_file(\"amapola.jpg\", url)\n",
        "image = read_image(image_path, target_size=image_size)\n",
        "\n",
        "image = np.expand_dims(image, 0)\n",
        "image_vec = base_model(image)\n",
        "nearest = find_k_nearest(image_vec, train_vecs, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZM1p3VG6P2M"
      },
      "outputs": [],
      "source": [
        "print('Image')\n",
        "plt.imshow(image[0])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "print()\n",
        "print('-' * 50)\n",
        "print('Nearest')\n",
        "for ind, dist in nearest:\n",
        "    img = train_images[ind]\n",
        "    label = class_names[train_labels[ind]]\n",
        "    plt.imshow(img)\n",
        "    plt.xlabel('distance:{0:1.2f}'.format(dist))\n",
        "    plt.title(label)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}